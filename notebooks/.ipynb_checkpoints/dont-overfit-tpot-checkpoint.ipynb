{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tpot in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (0.10.2)\r\n",
      "Requirement already satisfied: tqdm>=4.26.0 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from tpot) (4.31.0)\r\n",
      "Requirement already satisfied: numpy>=1.12.1 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from tpot) (1.17.2)\r\n",
      "Requirement already satisfied: scipy>=0.19.0 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from tpot) (1.2.0)\r\n",
      "Requirement already satisfied: scikit-learn>=0.18.1 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from tpot) (0.20.2)\r\n",
      "Requirement already satisfied: stopit>=1.1.1 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from tpot) (1.1.2)\r\n",
      "Requirement already satisfied: joblib>=0.10.3 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from tpot) (0.13.2)\r\n",
      "Requirement already satisfied: update-checker>=0.16 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from tpot) (0.16)\r\n",
      "Requirement already satisfied: deap>=1.0 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from tpot) (1.3.0)\r\n",
      "Requirement already satisfied: pandas>=0.20.2 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from tpot) (0.25.1)\r\n",
      "Requirement already satisfied: requests>=2.3.0 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from update-checker>=0.16->tpot) (2.21.0)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from pandas>=0.20.2->tpot) (2019.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from pandas>=0.20.2->tpot) (2.8.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2019.9.11)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (3.0.4)\r\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (1.24.3)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2.8)\r\n",
      "Requirement already satisfied: six>=1.5 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas>=0.20.2->tpot) (1.12.0)\r\n"
     ]
    }
   ],
   "source": [
    "#!pip install tpot\n",
    "import pandas as pd\n",
    "from tpot import TPOTClassifier\n",
    "from benchmark_utils import timer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"../datasets/dont-overfit/train.csv\")\n",
    "X_test = pd.read_csv(\"../datasets/dont-overfit/test.csv\")\n",
    "\n",
    "## Change target dtype \n",
    "y_train = pd.DataFrame(X_train.target,dtype='int')\n",
    "# Drop id and target from train frame\n",
    "X_train.drop(columns=['target','id'],inplace=True)\n",
    "## Copy test id's and drop id from test frame\n",
    "id_test = X_test.id\n",
    "X_test.drop(columns=['id'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>2.165</td>\n",
       "      <td>0.681</td>\n",
       "      <td>-0.614</td>\n",
       "      <td>1.309</td>\n",
       "      <td>-0.455</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>0.276</td>\n",
       "      <td>-2.246</td>\n",
       "      <td>1.825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867</td>\n",
       "      <td>1.347</td>\n",
       "      <td>0.504</td>\n",
       "      <td>-0.649</td>\n",
       "      <td>0.672</td>\n",
       "      <td>-2.097</td>\n",
       "      <td>1.051</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>1.038</td>\n",
       "      <td>-1.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.081</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>0.326</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>0.317</td>\n",
       "      <td>1.172</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-1.695</td>\n",
       "      <td>-1.257</td>\n",
       "      <td>1.359</td>\n",
       "      <td>-0.808</td>\n",
       "      <td>-1.624</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>-1.099</td>\n",
       "      <td>-0.936</td>\n",
       "      <td>0.973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.404</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-1.222</td>\n",
       "      <td>0.726</td>\n",
       "      <td>1.444</td>\n",
       "      <td>-1.165</td>\n",
       "      <td>-1.544</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.392</td>\n",
       "      <td>-1.637</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>-0.725</td>\n",
       "      <td>-1.035</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.274</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>0.640</td>\n",
       "      <td>-0.595</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.467</td>\n",
       "      <td>-0.562</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.533</td>\n",
       "      <td>0.238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.347</td>\n",
       "      <td>-0.831</td>\n",
       "      <td>0.511</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>1.225</td>\n",
       "      <td>1.594</td>\n",
       "      <td>0.585</td>\n",
       "      <td>1.509</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>2.198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.134</td>\n",
       "      <td>2.415</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-1.006</td>\n",
       "      <td>1.378</td>\n",
       "      <td>1.246</td>\n",
       "      <td>1.478</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3      4      5      6      7      8      9  ...  \\\n",
       "0 -0.098  2.165  0.681 -0.614  1.309 -0.455 -0.236  0.276 -2.246  1.825  ...   \n",
       "1  1.081 -0.973 -0.383  0.326 -0.428  0.317  1.172  0.352  0.004 -0.291  ...   \n",
       "2 -0.523 -0.089 -0.348  0.148 -0.022  0.404 -0.023 -0.172  0.137  0.183  ...   \n",
       "3  0.067 -0.021  0.392 -1.637 -0.446 -0.725 -1.035  0.834  0.503  0.274  ...   \n",
       "4  2.347 -0.831  0.511 -0.021  1.225  1.594  0.585  1.509 -0.012  2.198  ...   \n",
       "\n",
       "     290    291    292    293    294    295    296    297    298    299  \n",
       "0  0.867  1.347  0.504 -0.649  0.672 -2.097  1.051 -0.414  1.038 -1.065  \n",
       "1 -0.165 -1.695 -1.257  1.359 -0.808 -1.624 -0.458 -1.099 -0.936  0.973  \n",
       "2  0.013  0.263 -1.222  0.726  1.444 -1.165 -1.544  0.004  0.800 -1.211  \n",
       "3 -0.404  0.640 -0.595 -0.966  0.900  0.467 -0.562 -0.254 -0.533  0.238  \n",
       "4  0.898  0.134  2.415 -0.996 -1.006  1.378  1.246  1.478  0.428  0.253  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=1275, style=ProgressStyle(descripâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.736\n",
      "Generation 2 - Current best internal CV score: 0.736\n",
      "Generation 3 - Current best internal CV score: 0.7600000000000001\n",
      "Generation 4 - Current best internal CV score: 0.7600000000000001\n",
      "Generation 5 - Current best internal CV score: 0.7600000000000001\n",
      "Generation 6 - Current best internal CV score: 0.7600000000000001\n",
      "Generation 7 - Current best internal CV score: 0.7600000000000001\n",
      "Generation 8 - Current best internal CV score: 0.7600000000000001\n",
      "Generation 9 - Current best internal CV score: 0.7600000000000001\n",
      "Generation 10 - Current best internal CV score: 0.7600000000000001\n",
      "Generation 11 - Current best internal CV score: 0.7600000000000001\n",
      "Generation 12 - Current best internal CV score: 0.7600000000000001\n",
      "Generation 13 - Current best internal CV score: 0.764\n",
      "Generation 14 - Current best internal CV score: 0.764\n",
      "Generation 15 - Current best internal CV score: 0.764\n",
      "Generation 16 - Current best internal CV score: 0.764\n",
      "Generation 17 - Current best internal CV score: 0.764\n",
      "Generation 18 - Current best internal CV score: 0.764\n",
      "Generation 19 - Current best internal CV score: 0.764\n",
      "Generation 20 - Current best internal CV score: 0.784\n",
      "Generation 21 - Current best internal CV score: 0.784\n",
      "Generation 22 - Current best internal CV score: 0.784\n",
      "Generation 23 - Current best internal CV score: 0.784\n",
      "Generation 24 - Current best internal CV score: 0.792\n",
      "Generation 25 - Current best internal CV score: 0.792\n",
      "Generation 26 - Current best internal CV score: 0.792\n",
      "Generation 27 - Current best internal CV score: 0.792\n",
      "Generation 28 - Current best internal CV score: 0.792\n",
      "Generation 29 - Current best internal CV score: 0.792\n",
      "Generation 30 - Current best internal CV score: 0.792\n",
      "Generation 31 - Current best internal CV score: 0.792\n",
      "Generation 32 - Current best internal CV score: 0.792\n",
      "Generation 33 - Current best internal CV score: 0.792\n",
      "Generation 34 - Current best internal CV score: 0.792\n",
      "Generation 35 - Current best internal CV score: 0.792\n",
      "Generation 36 - Current best internal CV score: 0.792\n",
      "Generation 37 - Current best internal CV score: 0.792\n",
      "Generation 38 - Current best internal CV score: 0.792\n",
      "Generation 39 - Current best internal CV score: 0.792\n",
      "Generation 40 - Current best internal CV score: 0.792\n",
      "Generation 41 - Current best internal CV score: 0.792\n",
      "Generation 42 - Current best internal CV score: 0.792\n",
      "Generation 43 - Current best internal CV score: 0.792\n",
      "Generation 44 - Current best internal CV score: 0.792\n",
      "Generation 45 - Current best internal CV score: 0.792\n",
      "Generation 46 - Current best internal CV score: 0.792\n",
      "Generation 47 - Current best internal CV score: 0.792\n",
      "Generation 48 - Current best internal CV score: 0.792\n",
      "Generation 49 - Current best internal CV score: 0.792\n",
      "Generation 50 - Current best internal CV score: 0.792\n",
      "\n",
      "Best pipeline: LogisticRegression(ZeroCount(RFE(input_matrix, criterion=entropy, max_features=0.8500000000000001, n_estimators=100, step=0.15000000000000002)), C=1.0, dual=False, penalty=l1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTClassifier(config_dict=None, crossover_rate=0.1, cv=5,\n",
       "        disable_update_check=False, early_stop=None, generations=50,\n",
       "        max_eval_time_mins=5, max_time_mins=None, memory=None,\n",
       "        mutation_rate=0.9, n_jobs=1, offspring_size=None,\n",
       "        periodic_checkpoint_folder=None, population_size=25,\n",
       "        random_state=42, scoring=None, subsample=1.0, template=None,\n",
       "        use_dask=False, verbosity=2, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = timer(None)\n",
    "tp = TPOTClassifier(generations=50, population_size=25,\n",
    "                                    random_state=42, verbosity=2)\n",
    "\n",
    "tp.fit(X_train, y_train)\n",
    "time = timer(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "tp.export('tpot_pipeline_dont_overfit.py')\n",
    "preds = tp.predict(X_test)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on TPOTClassifier in module tpot.tpot object:\n",
      "\n",
      "class TPOTClassifier(tpot.base.TPOTBase)\n",
      " |  TPOTClassifier(generations=100, population_size=100, offspring_size=None, mutation_rate=0.9, crossover_rate=0.1, scoring=None, cv=5, subsample=1.0, n_jobs=1, max_time_mins=None, max_eval_time_mins=5, random_state=None, config_dict=None, template=None, warm_start=False, memory=None, use_dask=False, periodic_checkpoint_folder=None, early_stop=None, verbosity=0, disable_update_check=False)\n",
      " |  \n",
      " |  TPOT estimator for classification problems.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TPOTClassifier\n",
      " |      tpot.base.TPOTBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  classification = True\n",
      " |  \n",
      " |  default_config_dict = {'sklearn.cluster.FeatureAgglomeration': {'affin...\n",
      " |  \n",
      " |  regression = False\n",
      " |  \n",
      " |  scoring_function = 'accuracy'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tpot.base.TPOTBase:\n",
      " |  \n",
      " |  __init__(self, generations=100, population_size=100, offspring_size=None, mutation_rate=0.9, crossover_rate=0.1, scoring=None, cv=5, subsample=1.0, n_jobs=1, max_time_mins=None, max_eval_time_mins=5, random_state=None, config_dict=None, template=None, warm_start=False, memory=None, use_dask=False, periodic_checkpoint_folder=None, early_stop=None, verbosity=0, disable_update_check=False)\n",
      " |      Set up the genetic programming algorithm for pipeline optimization.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      generations: int, optional (default: 100)\n",
      " |          Number of iterations to the run pipeline optimization process.\n",
      " |          Generally, TPOT will work better when you give it more generations (and\n",
      " |          therefore time) to optimize the pipeline. TPOT will evaluate\n",
      " |          POPULATION_SIZE + GENERATIONS x OFFSPRING_SIZE pipelines in total.\n",
      " |      population_size: int, optional (default: 100)\n",
      " |          Number of individuals to retain in the GP population every generation.\n",
      " |          Generally, TPOT will work better when you give it more individuals\n",
      " |          (and therefore time) to optimize the pipeline. TPOT will evaluate\n",
      " |          POPULATION_SIZE + GENERATIONS x OFFSPRING_SIZE pipelines in total.\n",
      " |      offspring_size: int, optional (default: None)\n",
      " |          Number of offspring to produce in each GP generation.\n",
      " |          By default, offspring_size = population_size.\n",
      " |      mutation_rate: float, optional (default: 0.9)\n",
      " |          Mutation rate for the genetic programming algorithm in the range [0.0, 1.0].\n",
      " |          This parameter tells the GP algorithm how many pipelines to apply random\n",
      " |          changes to every generation. We recommend using the default parameter unless\n",
      " |          you understand how the mutation rate affects GP algorithms.\n",
      " |      crossover_rate: float, optional (default: 0.1)\n",
      " |          Crossover rate for the genetic programming algorithm in the range [0.0, 1.0].\n",
      " |          This parameter tells the genetic programming algorithm how many pipelines to\n",
      " |          \"breed\" every generation. We recommend using the default parameter unless you\n",
      " |          understand how the mutation rate affects GP algorithms.\n",
      " |      scoring: string or callable, optional\n",
      " |          Function used to evaluate the quality of a given pipeline for the\n",
      " |          problem. By default, accuracy is used for classification problems and\n",
      " |          mean squared error (MSE) for regression problems.\n",
      " |      \n",
      " |          Offers the same options as sklearn.model_selection.cross_val_score as well as\n",
      " |          a built-in score 'balanced_accuracy'. Classification metrics:\n",
      " |      \n",
      " |          ['accuracy', 'adjusted_rand_score', 'average_precision', 'balanced_accuracy',\n",
      " |          'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted',\n",
      " |          'precision', 'precision_macro', 'precision_micro', 'precision_samples',\n",
      " |          'precision_weighted', 'recall', 'recall_macro', 'recall_micro',\n",
      " |          'recall_samples', 'recall_weighted', 'roc_auc']\n",
      " |      \n",
      " |          Regression metrics:\n",
      " |      \n",
      " |          ['neg_median_absolute_error', 'neg_mean_absolute_error',\n",
      " |          'neg_mean_squared_error', 'r2']\n",
      " |      \n",
      " |          If you would like to use a custom scoring function, you can pass a callable\n",
      " |          function to this parameter with the signature scorer(y_true, y_pred).\n",
      " |          See the section on scoring functions in the documentation for more details.\n",
      " |      \n",
      " |          TPOT assumes that any custom scoring function with \"error\" or \"loss\" in the\n",
      " |          name is meant to be minimized, whereas any other functions will be maximized.\n",
      " |      cv: int or cross-validation generator, optional (default: 5)\n",
      " |          If CV is a number, then it is the number of folds to evaluate each\n",
      " |          pipeline over in k-fold cross-validation during the TPOT optimization\n",
      " |           process. If it is an object then it is an object to be used as a\n",
      " |           cross-validation generator.\n",
      " |      subsample: float, optional (default: 1.0)\n",
      " |          Subsample ratio of the training instance. Setting it to 0.5 means that TPOT\n",
      " |          randomly collects half of training samples for pipeline optimization process.\n",
      " |      n_jobs: int, optional (default: 1)\n",
      " |          Number of CPUs for evaluating pipelines in parallel during the TPOT\n",
      " |          optimization process. Assigning this to -1 will use as many cores as available\n",
      " |          on the computer. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used.\n",
      " |          Thus for n_jobs = -2, all CPUs but one are used.\n",
      " |      max_time_mins: int, optional (default: None)\n",
      " |          How many minutes TPOT has to optimize the pipeline.\n",
      " |          If provided, this setting will override the \"generations\" parameter and allow\n",
      " |          TPOT to run until it runs out of time.\n",
      " |      max_eval_time_mins: float, optional (default: 5)\n",
      " |          How many minutes TPOT has to optimize a single pipeline.\n",
      " |          Setting this parameter to higher values will allow TPOT to explore more\n",
      " |          complex pipelines, but will also allow TPOT to run longer.\n",
      " |      random_state: int, optional (default: None)\n",
      " |          Random number generator seed for TPOT. Use this parameter to make sure\n",
      " |          that TPOT will give you the same results each time you run it against the\n",
      " |          same data set with that seed.\n",
      " |      config_dict: a Python dictionary or string, optional (default: None)\n",
      " |          Python dictionary:\n",
      " |              A dictionary customizing the operators and parameters that\n",
      " |              TPOT uses in the optimization process.\n",
      " |              For examples, see config_regressor.py and config_classifier.py\n",
      " |          Path for configuration file:\n",
      " |              A path to a configuration file for customizing the operators and parameters that\n",
      " |              TPOT uses in the optimization process.\n",
      " |              For examples, see config_regressor.py and config_classifier.py\n",
      " |          String 'TPOT light':\n",
      " |              TPOT uses a light version of operator configuration dictionary instead of\n",
      " |              the default one.\n",
      " |          String 'TPOT MDR':\n",
      " |              TPOT uses a list of TPOT-MDR operator configuration dictionary instead of\n",
      " |              the default one.\n",
      " |          String 'TPOT sparse':\n",
      " |              TPOT uses a configuration dictionary with a one-hot-encoder and the\n",
      " |              operators normally included in TPOT that also support sparse matrices.\n",
      " |      template: string (default: None)\n",
      " |          Template of predefined pipeline structure. The option is for specifying a desired structure\n",
      " |          for the machine learning pipeline evaluated in TPOT. So far this option only supports\n",
      " |          linear pipeline structure. Each step in the pipeline should be a main class of operators\n",
      " |          (Selector, Transformer, Classifier or Regressor) or a specific operator\n",
      " |          (e.g. SelectPercentile) defined in TPOT operator configuration. If one step is a main class,\n",
      " |          TPOT will randomly assign all subclass operators (subclasses of SelectorMixin,\n",
      " |          TransformerMixin, ClassifierMixin or RegressorMixin in scikit-learn) to that step.\n",
      " |          Steps in the template are delimited by \"-\", e.g. \"SelectPercentile-Transformer-Classifier\".\n",
      " |          By default value of template is None, TPOT generates tree-based pipeline randomly.\n",
      " |      warm_start: bool, optional (default: False)\n",
      " |          Flag indicating whether the TPOT instance will reuse the population from\n",
      " |          previous calls to fit().\n",
      " |      memory: a Memory object or string, optional (default: None)\n",
      " |          If supplied, pipeline will cache each transformer after calling fit. This feature\n",
      " |          is used to avoid computing the fit transformers within a pipeline if the parameters\n",
      " |          and input data are identical with another fitted pipeline during optimization process.\n",
      " |          String 'auto':\n",
      " |              TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n",
      " |          String path of a caching directory\n",
      " |              TPOT uses memory caching with the provided directory and TPOT does NOT clean\n",
      " |              the caching directory up upon shutdown. If the directory does not exist, TPOT will\n",
      " |              create it.\n",
      " |          Memory object:\n",
      " |              TPOT uses the instance of joblib.Memory for memory caching,\n",
      " |              and TPOT does NOT clean the caching directory up upon shutdown.\n",
      " |          None:\n",
      " |              TPOT does not use memory caching.\n",
      " |      use_dask: boolean, default False\n",
      " |          Whether to use Dask-ML's pipeline optimiziations. This avoid re-fitting\n",
      " |          the same estimator on the same split of data multiple times. It\n",
      " |          will also provide more detailed diagnostics when using Dask's\n",
      " |          distributed scheduler.\n",
      " |      \n",
      " |          See `avoid repeated work <https://dask-ml.readthedocs.io/en/latest/hyper-parameter-search.html#avoid-repeated-work>`__\n",
      " |          for more details.\n",
      " |      periodic_checkpoint_folder: path string, optional (default: None)\n",
      " |          If supplied, a folder in which tpot will periodically save pipelines in pareto front so far while optimizing.\n",
      " |          Currently once per generation but not more often than once per 30 seconds.\n",
      " |          Useful in multiple cases:\n",
      " |              Sudden death before tpot could save optimized pipeline\n",
      " |              Track its progress\n",
      " |              Grab pipelines while it's still optimizing\n",
      " |      early_stop: int or None (default: None)\n",
      " |          How many generations TPOT checks whether there is no improvement in optimization process.\n",
      " |          End optimization process if there is no improvement in the set number of generations.\n",
      " |      verbosity: int, optional (default: 0)\n",
      " |          How much information TPOT communicates while it's running.\n",
      " |          0 = none, 1 = minimal, 2 = high, 3 = all.\n",
      " |          A setting of 2 or higher will add a progress bar during the optimization procedure.\n",
      " |      disable_update_check: bool, optional (default: False)\n",
      " |          Flag indicating whether the TPOT version checker should be disabled.\n",
      " |      \n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      None\n",
      " |  \n",
      " |  clean_pipeline_string(self, individual)\n",
      " |      Provide a string of the individual without the parameter prefixes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      individual: individual\n",
      " |          Individual which should be represented by a pretty string\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      A string like str(individual), but with parameter prefixes removed.\n",
      " |  \n",
      " |  export(self, output_file_name, data_file_path='')\n",
      " |      Export the optimized pipeline as Python code.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      output_file_name: string\n",
      " |          String containing the path and file name of the desired output file\n",
      " |      data_file_path: string (default: '')\n",
      " |          By default, the path of input dataset is 'PATH/TO/DATA/FILE' by default.\n",
      " |          If data_file_path is another string, the path will be replaced.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      False if it skipped writing the pipeline to file\n",
      " |      True if the pipeline was actually written\n",
      " |  \n",
      " |  fit(self, features, target, sample_weight=None, groups=None)\n",
      " |      Fit an optimized machine learning pipeline.\n",
      " |      \n",
      " |      Uses genetic programming to optimize a machine learning pipeline that\n",
      " |      maximizes score on the provided features and target. Performs internal\n",
      " |      k-fold cross-validaton to avoid overfitting on the provided data. The\n",
      " |      best pipeline is then trained on the entire set of provided samples.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      features: array-like {n_samples, n_features}\n",
      " |          Feature matrix\n",
      " |      \n",
      " |          TPOT and all scikit-learn algorithms assume that the features will be numerical\n",
      " |          and there will be no missing values. As such, when a feature matrix is provided\n",
      " |          to TPOT, all missing values will automatically be replaced (i.e., imputed) using\n",
      " |          median value imputation.\n",
      " |      \n",
      " |          If you wish to use a different imputation strategy than median imputation, please\n",
      " |          make sure to apply imputation to your feature set prior to passing it to TPOT.\n",
      " |      target: array-like {n_samples}\n",
      " |          List of class labels for prediction\n",
      " |      sample_weight: array-like {n_samples}, optional\n",
      " |          Per-sample weights. Higher weights indicate more importance. If specified,\n",
      " |          sample_weight will be passed to any pipeline element whose fit() function accepts\n",
      " |          a sample_weight argument. By default, using sample_weight does not affect tpot's\n",
      " |          scoring functions, which determine preferences between pipelines.\n",
      " |      groups: array-like, with shape {n_samples, }, optional\n",
      " |          Group labels for the samples used when performing cross-validation.\n",
      " |          This parameter should only be used in conjunction with sklearn's Group cross-validation\n",
      " |          functions, such as sklearn.model_selection.GroupKFold\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self: object\n",
      " |          Returns a copy of the fitted TPOT object\n",
      " |  \n",
      " |  fit_predict(self, features, target, sample_weight=None, groups=None)\n",
      " |      Call fit and predict in sequence.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      features: array-like {n_samples, n_features}\n",
      " |          Feature matrix\n",
      " |      target: array-like {n_samples}\n",
      " |          List of class labels for prediction\n",
      " |      sample_weight: array-like {n_samples}, optional\n",
      " |          Per-sample weights. Higher weights force TPOT to put more emphasis on those points\n",
      " |      groups: array-like, with shape {n_samples, }, optional\n",
      " |          Group labels for the samples used when performing cross-validation.\n",
      " |          This parameter should only be used in conjunction with sklearn's Group cross-validation\n",
      " |          functions, such as sklearn.model_selection.GroupKFold\n",
      " |      \n",
      " |      Returns\n",
      " |      ----------\n",
      " |      array-like: {n_samples}\n",
      " |          Predicted target for the provided features\n",
      " |  \n",
      " |  predict(self, features)\n",
      " |      Use the optimized pipeline to predict the target for a feature set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      features: array-like {n_samples, n_features}\n",
      " |          Feature matrix\n",
      " |      \n",
      " |      Returns\n",
      " |      ----------\n",
      " |      array-like: {n_samples}\n",
      " |          Predicted target for the samples in the feature matrix\n",
      " |  \n",
      " |  predict_proba(self, features)\n",
      " |      Use the optimized pipeline to estimate the class probabilities for a feature set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      features: array-like {n_samples, n_features}\n",
      " |          Feature matrix of the testing set\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array-like: {n_samples, n_target}\n",
      " |          The class probabilities of the input samples\n",
      " |  \n",
      " |  score(self, testing_features, testing_target)\n",
      " |      Return the score on the given testing data using the user-specified scoring function.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      testing_features: array-like {n_samples, n_features}\n",
      " |          Feature matrix of the testing set\n",
      " |      testing_target: array-like {n_samples}\n",
      " |          List of class labels for prediction in the testing set\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      accuracy_score: float\n",
      " |          The estimated test set accuracy\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = [0.736,\n",
    "0.736,\n",
    "0.76,\n",
    "0.76,\n",
    "0.76,\n",
    "0.76,\n",
    "0.76,\n",
    "0.76,\n",
    "0.76,\n",
    "0.76,\n",
    "0.76,\n",
    "0.76,\n",
    "0.764,\n",
    "0.764,\n",
    "0.764,\n",
    "0.764,\n",
    "0.764,\n",
    "0.764,\n",
    "0.764,\n",
    "0.784,\n",
    "0.784,\n",
    "0.784,\n",
    "0.784,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "]\n",
    "\n",
    "generations = [i for i in range(1,51)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (0.9.0)\r\n",
      "Requirement already satisfied: numpy>=1.9.3 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from seaborn) (1.17.2)\r\n",
      "Requirement already satisfied: matplotlib>=1.4.3 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from seaborn) (3.1.1)\r\n",
      "Requirement already satisfied: pandas>=0.15.2 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from seaborn) (0.25.1)\r\n",
      "Requirement already satisfied: scipy>=0.14.0 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from seaborn) (1.2.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from matplotlib>=1.4.3->seaborn) (1.1.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from matplotlib>=1.4.3->seaborn) (2.8.0)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from matplotlib>=1.4.3->seaborn) (2.4.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from matplotlib>=1.4.3->seaborn) (0.10.0)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from pandas>=0.15.2->seaborn) (2019.2)\r\n",
      "Requirement already satisfied: setuptools in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.3->seaborn) (41.0.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib>=1.4.3->seaborn) (1.12.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.lineplot(x=generations, y=train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on TPOTClassifier in module tpot.tpot object:\n",
      "\n",
      "class TPOTClassifier(tpot.base.TPOTBase)\n",
      " |  TPOTClassifier(generations=100, population_size=100, offspring_size=None, mutation_rate=0.9, crossover_rate=0.1, scoring=None, cv=5, subsample=1.0, n_jobs=1, max_time_mins=None, max_eval_time_mins=5, random_state=None, config_dict=None, template=None, warm_start=False, memory=None, use_dask=False, periodic_checkpoint_folder=None, early_stop=None, verbosity=0, disable_update_check=False)\n",
      " |  \n",
      " |  TPOT estimator for classification problems.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TPOTClassifier\n",
      " |      tpot.base.TPOTBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  classification = True\n",
      " |  \n",
      " |  default_config_dict = {'sklearn.cluster.FeatureAgglomeration': {'affin...\n",
      " |  \n",
      " |  regression = False\n",
      " |  \n",
      " |  scoring_function = 'accuracy'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tpot.base.TPOTBase:\n",
      " |  \n",
      " |  __init__(self, generations=100, population_size=100, offspring_size=None, mutation_rate=0.9, crossover_rate=0.1, scoring=None, cv=5, subsample=1.0, n_jobs=1, max_time_mins=None, max_eval_time_mins=5, random_state=None, config_dict=None, template=None, warm_start=False, memory=None, use_dask=False, periodic_checkpoint_folder=None, early_stop=None, verbosity=0, disable_update_check=False)\n",
      " |      Set up the genetic programming algorithm for pipeline optimization.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      generations: int, optional (default: 100)\n",
      " |          Number of iterations to the run pipeline optimization process.\n",
      " |          Generally, TPOT will work better when you give it more generations (and\n",
      " |          therefore time) to optimize the pipeline. TPOT will evaluate\n",
      " |          POPULATION_SIZE + GENERATIONS x OFFSPRING_SIZE pipelines in total.\n",
      " |      population_size: int, optional (default: 100)\n",
      " |          Number of individuals to retain in the GP population every generation.\n",
      " |          Generally, TPOT will work better when you give it more individuals\n",
      " |          (and therefore time) to optimize the pipeline. TPOT will evaluate\n",
      " |          POPULATION_SIZE + GENERATIONS x OFFSPRING_SIZE pipelines in total.\n",
      " |      offspring_size: int, optional (default: None)\n",
      " |          Number of offspring to produce in each GP generation.\n",
      " |          By default, offspring_size = population_size.\n",
      " |      mutation_rate: float, optional (default: 0.9)\n",
      " |          Mutation rate for the genetic programming algorithm in the range [0.0, 1.0].\n",
      " |          This parameter tells the GP algorithm how many pipelines to apply random\n",
      " |          changes to every generation. We recommend using the default parameter unless\n",
      " |          you understand how the mutation rate affects GP algorithms.\n",
      " |      crossover_rate: float, optional (default: 0.1)\n",
      " |          Crossover rate for the genetic programming algorithm in the range [0.0, 1.0].\n",
      " |          This parameter tells the genetic programming algorithm how many pipelines to\n",
      " |          \"breed\" every generation. We recommend using the default parameter unless you\n",
      " |          understand how the mutation rate affects GP algorithms.\n",
      " |      scoring: string or callable, optional\n",
      " |          Function used to evaluate the quality of a given pipeline for the\n",
      " |          problem. By default, accuracy is used for classification problems and\n",
      " |          mean squared error (MSE) for regression problems.\n",
      " |      \n",
      " |          Offers the same options as sklearn.model_selection.cross_val_score as well as\n",
      " |          a built-in score 'balanced_accuracy'. Classification metrics:\n",
      " |      \n",
      " |          ['accuracy', 'adjusted_rand_score', 'average_precision', 'balanced_accuracy',\n",
      " |          'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted',\n",
      " |          'precision', 'precision_macro', 'precision_micro', 'precision_samples',\n",
      " |          'precision_weighted', 'recall', 'recall_macro', 'recall_micro',\n",
      " |          'recall_samples', 'recall_weighted', 'roc_auc']\n",
      " |      \n",
      " |          Regression metrics:\n",
      " |      \n",
      " |          ['neg_median_absolute_error', 'neg_mean_absolute_error',\n",
      " |          'neg_mean_squared_error', 'r2']\n",
      " |      \n",
      " |          If you would like to use a custom scoring function, you can pass a callable\n",
      " |          function to this parameter with the signature scorer(y_true, y_pred).\n",
      " |          See the section on scoring functions in the documentation for more details.\n",
      " |      \n",
      " |          TPOT assumes that any custom scoring function with \"error\" or \"loss\" in the\n",
      " |          name is meant to be minimized, whereas any other functions will be maximized.\n",
      " |      cv: int or cross-validation generator, optional (default: 5)\n",
      " |          If CV is a number, then it is the number of folds to evaluate each\n",
      " |          pipeline over in k-fold cross-validation during the TPOT optimization\n",
      " |           process. If it is an object then it is an object to be used as a\n",
      " |           cross-validation generator.\n",
      " |      subsample: float, optional (default: 1.0)\n",
      " |          Subsample ratio of the training instance. Setting it to 0.5 means that TPOT\n",
      " |          randomly collects half of training samples for pipeline optimization process.\n",
      " |      n_jobs: int, optional (default: 1)\n",
      " |          Number of CPUs for evaluating pipelines in parallel during the TPOT\n",
      " |          optimization process. Assigning this to -1 will use as many cores as available\n",
      " |          on the computer. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used.\n",
      " |          Thus for n_jobs = -2, all CPUs but one are used.\n",
      " |      max_time_mins: int, optional (default: None)\n",
      " |          How many minutes TPOT has to optimize the pipeline.\n",
      " |          If provided, this setting will override the \"generations\" parameter and allow\n",
      " |          TPOT to run until it runs out of time.\n",
      " |      max_eval_time_mins: float, optional (default: 5)\n",
      " |          How many minutes TPOT has to optimize a single pipeline.\n",
      " |          Setting this parameter to higher values will allow TPOT to explore more\n",
      " |          complex pipelines, but will also allow TPOT to run longer.\n",
      " |      random_state: int, optional (default: None)\n",
      " |          Random number generator seed for TPOT. Use this parameter to make sure\n",
      " |          that TPOT will give you the same results each time you run it against the\n",
      " |          same data set with that seed.\n",
      " |      config_dict: a Python dictionary or string, optional (default: None)\n",
      " |          Python dictionary:\n",
      " |              A dictionary customizing the operators and parameters that\n",
      " |              TPOT uses in the optimization process.\n",
      " |              For examples, see config_regressor.py and config_classifier.py\n",
      " |          Path for configuration file:\n",
      " |              A path to a configuration file for customizing the operators and parameters that\n",
      " |              TPOT uses in the optimization process.\n",
      " |              For examples, see config_regressor.py and config_classifier.py\n",
      " |          String 'TPOT light':\n",
      " |              TPOT uses a light version of operator configuration dictionary instead of\n",
      " |              the default one.\n",
      " |          String 'TPOT MDR':\n",
      " |              TPOT uses a list of TPOT-MDR operator configuration dictionary instead of\n",
      " |              the default one.\n",
      " |          String 'TPOT sparse':\n",
      " |              TPOT uses a configuration dictionary with a one-hot-encoder and the\n",
      " |              operators normally included in TPOT that also support sparse matrices.\n",
      " |      template: string (default: None)\n",
      " |          Template of predefined pipeline structure. The option is for specifying a desired structure\n",
      " |          for the machine learning pipeline evaluated in TPOT. So far this option only supports\n",
      " |          linear pipeline structure. Each step in the pipeline should be a main class of operators\n",
      " |          (Selector, Transformer, Classifier or Regressor) or a specific operator\n",
      " |          (e.g. SelectPercentile) defined in TPOT operator configuration. If one step is a main class,\n",
      " |          TPOT will randomly assign all subclass operators (subclasses of SelectorMixin,\n",
      " |          TransformerMixin, ClassifierMixin or RegressorMixin in scikit-learn) to that step.\n",
      " |          Steps in the template are delimited by \"-\", e.g. \"SelectPercentile-Transformer-Classifier\".\n",
      " |          By default value of template is None, TPOT generates tree-based pipeline randomly.\n",
      " |      warm_start: bool, optional (default: False)\n",
      " |          Flag indicating whether the TPOT instance will reuse the population from\n",
      " |          previous calls to fit().\n",
      " |      memory: a Memory object or string, optional (default: None)\n",
      " |          If supplied, pipeline will cache each transformer after calling fit. This feature\n",
      " |          is used to avoid computing the fit transformers within a pipeline if the parameters\n",
      " |          and input data are identical with another fitted pipeline during optimization process.\n",
      " |          String 'auto':\n",
      " |              TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n",
      " |          String path of a caching directory\n",
      " |              TPOT uses memory caching with the provided directory and TPOT does NOT clean\n",
      " |              the caching directory up upon shutdown. If the directory does not exist, TPOT will\n",
      " |              create it.\n",
      " |          Memory object:\n",
      " |              TPOT uses the instance of joblib.Memory for memory caching,\n",
      " |              and TPOT does NOT clean the caching directory up upon shutdown.\n",
      " |          None:\n",
      " |              TPOT does not use memory caching.\n",
      " |      use_dask: boolean, default False\n",
      " |          Whether to use Dask-ML's pipeline optimiziations. This avoid re-fitting\n",
      " |          the same estimator on the same split of data multiple times. It\n",
      " |          will also provide more detailed diagnostics when using Dask's\n",
      " |          distributed scheduler.\n",
      " |      \n",
      " |          See `avoid repeated work <https://dask-ml.readthedocs.io/en/latest/hyper-parameter-search.html#avoid-repeated-work>`__\n",
      " |          for more details.\n",
      " |      periodic_checkpoint_folder: path string, optional (default: None)\n",
      " |          If supplied, a folder in which tpot will periodically save pipelines in pareto front so far while optimizing.\n",
      " |          Currently once per generation but not more often than once per 30 seconds.\n",
      " |          Useful in multiple cases:\n",
      " |              Sudden death before tpot could save optimized pipeline\n",
      " |              Track its progress\n",
      " |              Grab pipelines while it's still optimizing\n",
      " |      early_stop: int or None (default: None)\n",
      " |          How many generations TPOT checks whether there is no improvement in optimization process.\n",
      " |          End optimization process if there is no improvement in the set number of generations.\n",
      " |      verbosity: int, optional (default: 0)\n",
      " |          How much information TPOT communicates while it's running.\n",
      " |          0 = none, 1 = minimal, 2 = high, 3 = all.\n",
      " |          A setting of 2 or higher will add a progress bar during the optimization procedure.\n",
      " |      disable_update_check: bool, optional (default: False)\n",
      " |          Flag indicating whether the TPOT version checker should be disabled.\n",
      " |      \n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      None\n",
      " |  \n",
      " |  clean_pipeline_string(self, individual)\n",
      " |      Provide a string of the individual without the parameter prefixes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      individual: individual\n",
      " |          Individual which should be represented by a pretty string\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      A string like str(individual), but with parameter prefixes removed.\n",
      " |  \n",
      " |  export(self, output_file_name, data_file_path='')\n",
      " |      Export the optimized pipeline as Python code.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      output_file_name: string\n",
      " |          String containing the path and file name of the desired output file\n",
      " |      data_file_path: string (default: '')\n",
      " |          By default, the path of input dataset is 'PATH/TO/DATA/FILE' by default.\n",
      " |          If data_file_path is another string, the path will be replaced.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      False if it skipped writing the pipeline to file\n",
      " |      True if the pipeline was actually written\n",
      " |  \n",
      " |  fit(self, features, target, sample_weight=None, groups=None)\n",
      " |      Fit an optimized machine learning pipeline.\n",
      " |      \n",
      " |      Uses genetic programming to optimize a machine learning pipeline that\n",
      " |      maximizes score on the provided features and target. Performs internal\n",
      " |      k-fold cross-validaton to avoid overfitting on the provided data. The\n",
      " |      best pipeline is then trained on the entire set of provided samples.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      features: array-like {n_samples, n_features}\n",
      " |          Feature matrix\n",
      " |      \n",
      " |          TPOT and all scikit-learn algorithms assume that the features will be numerical\n",
      " |          and there will be no missing values. As such, when a feature matrix is provided\n",
      " |          to TPOT, all missing values will automatically be replaced (i.e., imputed) using\n",
      " |          median value imputation.\n",
      " |      \n",
      " |          If you wish to use a different imputation strategy than median imputation, please\n",
      " |          make sure to apply imputation to your feature set prior to passing it to TPOT.\n",
      " |      target: array-like {n_samples}\n",
      " |          List of class labels for prediction\n",
      " |      sample_weight: array-like {n_samples}, optional\n",
      " |          Per-sample weights. Higher weights indicate more importance. If specified,\n",
      " |          sample_weight will be passed to any pipeline element whose fit() function accepts\n",
      " |          a sample_weight argument. By default, using sample_weight does not affect tpot's\n",
      " |          scoring functions, which determine preferences between pipelines.\n",
      " |      groups: array-like, with shape {n_samples, }, optional\n",
      " |          Group labels for the samples used when performing cross-validation.\n",
      " |          This parameter should only be used in conjunction with sklearn's Group cross-validation\n",
      " |          functions, such as sklearn.model_selection.GroupKFold\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self: object\n",
      " |          Returns a copy of the fitted TPOT object\n",
      " |  \n",
      " |  fit_predict(self, features, target, sample_weight=None, groups=None)\n",
      " |      Call fit and predict in sequence.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      features: array-like {n_samples, n_features}\n",
      " |          Feature matrix\n",
      " |      target: array-like {n_samples}\n",
      " |          List of class labels for prediction\n",
      " |      sample_weight: array-like {n_samples}, optional\n",
      " |          Per-sample weights. Higher weights force TPOT to put more emphasis on those points\n",
      " |      groups: array-like, with shape {n_samples, }, optional\n",
      " |          Group labels for the samples used when performing cross-validation.\n",
      " |          This parameter should only be used in conjunction with sklearn's Group cross-validation\n",
      " |          functions, such as sklearn.model_selection.GroupKFold\n",
      " |      \n",
      " |      Returns\n",
      " |      ----------\n",
      " |      array-like: {n_samples}\n",
      " |          Predicted target for the provided features\n",
      " |  \n",
      " |  predict(self, features)\n",
      " |      Use the optimized pipeline to predict the target for a feature set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      features: array-like {n_samples, n_features}\n",
      " |          Feature matrix\n",
      " |      \n",
      " |      Returns\n",
      " |      ----------\n",
      " |      array-like: {n_samples}\n",
      " |          Predicted target for the samples in the feature matrix\n",
      " |  \n",
      " |  predict_proba(self, features)\n",
      " |      Use the optimized pipeline to estimate the class probabilities for a feature set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      features: array-like {n_samples, n_features}\n",
      " |          Feature matrix of the testing set\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array-like: {n_samples, n_target}\n",
      " |          The class probabilities of the input samples\n",
      " |  \n",
      " |  score(self, testing_features, testing_target)\n",
      " |      Return the score on the given testing data using the user-specified scoring function.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      testing_features: array-like {n_samples, n_features}\n",
      " |          Feature matrix of the testing set\n",
      " |      testing_target: array-like {n_samples}\n",
      " |          List of class labels for prediction in the testing set\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      accuracy_score: float\n",
      " |          The estimated test set accuracy\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tpot\n",
    "import pandas as pd\n",
    "from tpot import TPOTClassifier\n",
    "from benchmark_utils import timer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"dataset/train.csv\")\n",
    "X_test = pd.read_csv(\"dataset/test.csv\")\n",
    "\n",
    "## Change target dtype \n",
    "y_train = pd.DataFrame(X_train.target,dtype='int')\n",
    "# Drop id and target from train frame\n",
    "X_train.drop(columns=['target','id'],inplace=True)\n",
    "## Copy test id's and drop id from test frame\n",
    "id_test = X_test.id\n",
    "X_test.drop(columns=['id'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>2.165</td>\n",
       "      <td>0.681</td>\n",
       "      <td>-0.614</td>\n",
       "      <td>1.309</td>\n",
       "      <td>-0.455</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>0.276</td>\n",
       "      <td>-2.246</td>\n",
       "      <td>1.825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867</td>\n",
       "      <td>1.347</td>\n",
       "      <td>0.504</td>\n",
       "      <td>-0.649</td>\n",
       "      <td>0.672</td>\n",
       "      <td>-2.097</td>\n",
       "      <td>1.051</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>1.038</td>\n",
       "      <td>-1.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.081</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>0.326</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>0.317</td>\n",
       "      <td>1.172</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-1.695</td>\n",
       "      <td>-1.257</td>\n",
       "      <td>1.359</td>\n",
       "      <td>-0.808</td>\n",
       "      <td>-1.624</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>-1.099</td>\n",
       "      <td>-0.936</td>\n",
       "      <td>0.973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.404</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-1.222</td>\n",
       "      <td>0.726</td>\n",
       "      <td>1.444</td>\n",
       "      <td>-1.165</td>\n",
       "      <td>-1.544</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.392</td>\n",
       "      <td>-1.637</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>-0.725</td>\n",
       "      <td>-1.035</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.274</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>0.640</td>\n",
       "      <td>-0.595</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.467</td>\n",
       "      <td>-0.562</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.533</td>\n",
       "      <td>0.238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.347</td>\n",
       "      <td>-0.831</td>\n",
       "      <td>0.511</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>1.225</td>\n",
       "      <td>1.594</td>\n",
       "      <td>0.585</td>\n",
       "      <td>1.509</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>2.198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.134</td>\n",
       "      <td>2.415</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-1.006</td>\n",
       "      <td>1.378</td>\n",
       "      <td>1.246</td>\n",
       "      <td>1.478</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3      4      5      6      7      8      9  ...  \\\n",
       "0 -0.098  2.165  0.681 -0.614  1.309 -0.455 -0.236  0.276 -2.246  1.825  ...   \n",
       "1  1.081 -0.973 -0.383  0.326 -0.428  0.317  1.172  0.352  0.004 -0.291  ...   \n",
       "2 -0.523 -0.089 -0.348  0.148 -0.022  0.404 -0.023 -0.172  0.137  0.183  ...   \n",
       "3  0.067 -0.021  0.392 -1.637 -0.446 -0.725 -1.035  0.834  0.503  0.274  ...   \n",
       "4  2.347 -0.831  0.511 -0.021  1.225  1.594  0.585  1.509 -0.012  2.198  ...   \n",
       "\n",
       "     290    291    292    293    294    295    296    297    298    299  \n",
       "0  0.867  1.347  0.504 -0.649  0.672 -2.097  1.051 -0.414  1.038 -1.065  \n",
       "1 -0.165 -1.695 -1.257  1.359 -0.808 -1.624 -0.458 -1.099 -0.936  0.973  \n",
       "2  0.013  0.263 -1.222  0.726  1.444 -1.165 -1.544  0.004  0.800 -1.211  \n",
       "3 -0.404  0.640 -0.595 -0.966  0.900  0.467 -0.562 -0.254 -0.533  0.238  \n",
       "4  0.898  0.134  2.415 -0.996 -1.006  1.378  1.246  1.478  0.428  0.253  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=1275, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 1 - Current Pareto front scores:\n",
      "-1\t0.736\tLogisticRegression(input_matrix, LogisticRegression__C=1.0, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
      "Generation 2 - Current Pareto front scores:\n",
      "-1\t0.736\tLogisticRegression(input_matrix, LogisticRegression__C=1.0, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.74\tLogisticRegression(GradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.35000000000000003, GradientBoostingClassifier__min_samples_leaf=3, GradientBoostingClassifier__min_samples_split=15, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.55), LogisticRegression__C=0.01, LogisticRegression__dual=False, LogisticRegression__penalty=l2)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299']\n",
      "expected 31, 24, 152, 146, 169, 121, 177, 254, 262, 159, 136, 161, 90, 144, 96, 139, 195, 282, 186, 263, 75, 227, 6, 281, 92, 72, 218, 74, 204, 65, 194, 59, 259, 29, 138, 148, 271, 39, 113, 104, 129, 1, 223, 70, 85, 18, 45, 77, 256, 252, 110, 185, 3, 105, 175, 290, 102, 69, 4, 27, 224, 135, 168, 294, 248, 202, 164, 178, 145, 234, 44, 73, 127, 240, 276, 279, 238, 81, 134, 66, 33, 213, 30, 124, 236, 253, 257, 265, 166, 26, 241, 277, 42, 258, 158, 61, 298, 43, 83, 149, 206, 267, 50, 47, 189, 38, 20, 131, 141, 237, 266, 163, 34, 95, 167, 264, 82, 84, 52, 37, 184, 247, 7, 91, 111, 203, 122, 162, 280, 109, 211, 35, 120, 270, 291, 292, 255, 21, 87, 101, 79, 140, 170, 173, 215, 217, 246, 249, 201, 284, 219, 242, 243, 272, 293, 130, 88, 209, 268, 233, 157, 78, 116, 125, 231, 192, 269, 36, 126, 11, 51, 62, 151, 295, 235, 250, 261, 58, 32, 191, 287, 230, 56, 229, 278, 288, 299, 285, 14, 103, 22, 118, 150, 160, 16, 182, 60, 208, 199, 53, 67, 0, 54, 97, 9, 155, 156, 190, 19, 10, 114, 289, 119, 143, 205, 147, 28, 17, 176, 225, 71, 212, 12, 13, 41, 193, 210, 165, 222, 239, 273, 286, 48, 46, 220, 89, 106, 283, 100, 153, 137, 68, 57, 187, 297, 274, 275, 15, 245, 112, 2, 55, 132, 172, 216, 183, 198, 214, 93, 123, 142, 86, 174, 98, 63, 200, 99, 181, 5, 128, 40, 171, 80, 94, 226, 188, 221, 228, 133, 64, 154, 117, 180, 207, 8, 49, 244, 251, 23, 115, 196, 25, 108, 179, 296, 107, 197, 260, 232, 76 in input data\n",
      "training data did not have the following fields: f153, f11, f247, f267, f160, f185, f32, f211, f145, f280, f100, f72, f163, f239, f12, f8, f75, f246, f95, f89, f50, f68, f65, f151, f152, f154, f212, f208, f209, f198, f213, f115, f90, f233, f63, f144, f287, f16, f183, f92, f206, f138, f44, f96, f56, f159, f79, f173, f228, f31, f84, f139, f184, f194, f104, f254, f156, f255, f91, f120, f105, f252, f73, f277, f286, f281, f290, f19, f40, f78, f201, f186, f204, f234, f236, f85, f140, f262, f288, f3, f53, f250, f42, f33, f117, f272, f242, f133, f229, f284, f131, f260, f230, f237, f21, f54, f83, f275, f297, f182, f5, f80, f1, f241, f295, f147, f285, f221, f70, f197, f294, f217, f130, f67, f265, f289, f13, f111, f14, f26, f121, f143, f162, f166, f119, f193, f224, f57, f45, f226, f76, f169, f58, f172, f36, f107, f161, f251, f176, f20, f232, f205, f124, f283, f214, f71, f27, f137, f218, f177, f64, f35, f167, f55, f52, f240, f69, f114, f135, f210, f257, f164, f157, f37, f225, f6, f88, f269, f227, f249, f29, f298, f34, f102, f271, f2, f43, f196, f170, f181, f18, f47, f30, f238, f258, f39, f219, f4, f245, f62, f141, f171, f235, f261, f66, f150, f174, f220, f291, f28, f106, f134, f207, f41, f215, f191, f49, f148, f180, f99, f203, f10, f168, f243, f264, f149, f108, f192, f74, f200, f244, f60, f123, f59, f136, f9, f299, f259, f274, f22, f165, f132, f93, f195, f94, f15, f116, f128, f256, f178, f17, f24, f38, f126, f77, f155, f199, f129, f46, f122, f223, f48, f273, f263, f282, f103, f179, f81, f86, f110, f187, f278, f231, f296, f142, f125, f190, f216, f118, f276, f101, f268, f253, f222, f113, f270, f97, f248, f82, f51, f112, f279, f266, f109, f188, f202, f293, f0, f87, f25, f158, f175, f146, f23, f7, f189, f127, f98, f61, f292.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 '(slice(None, None, None), 0)' is an invalid key.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "Generation 3 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "Generation 4 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "Generation 5 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-4\t0.764\tLogisticRegression(RFE(MinMaxScaler(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.0001)), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.35000000000000003, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.5), LogisticRegression__C=5.0, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299']\n",
      "expected 31, 24, 152, 146, 169, 121, 177, 254, 262, 159, 136, 161, 90, 144, 96, 139, 195, 282, 186, 263, 75, 227, 6, 281, 92, 72, 218, 74, 204, 65, 194, 59, 259, 29, 138, 148, 271, 39, 113, 104, 129, 1, 223, 70, 85, 18, 45, 77, 256, 252, 110, 185, 3, 105, 175, 290, 102, 69, 4, 27, 224, 135, 168, 294, 248, 202, 164, 178, 145, 234, 44, 73, 127, 240, 276, 279, 238, 81, 134, 66, 33, 213, 30, 124, 236, 253, 257, 265, 166, 26, 241, 277, 42, 258, 158, 61, 298, 43, 83, 149, 206, 267, 50, 47, 189, 38, 20, 131, 141, 237, 266, 163, 34, 95, 167, 264, 82, 84, 52, 37, 184, 247, 7, 91, 111, 203, 122, 162, 280, 109, 211, 35, 120, 270, 291, 292, 255, 21, 87, 101, 79, 140, 170, 173, 215, 217, 246, 249, 201, 284, 219, 242, 243, 272, 293, 130, 88, 209, 268, 233, 157, 78, 116, 125, 231, 192, 269, 36, 126, 11, 51, 62, 151, 295, 235, 250, 261, 58, 32, 191, 287, 230, 56, 229, 278, 288, 299, 285, 14, 103, 22, 118, 150, 160, 16, 182, 60, 208, 199, 53, 67, 0, 54, 97, 9, 155, 156, 190, 19, 10, 114, 289, 119, 143, 205, 147, 28, 17, 176, 225, 71, 212, 12, 13, 41, 193, 210, 165, 222, 239, 273, 286, 48, 46, 220, 89, 106, 283, 100, 153, 137, 68, 57, 187, 297, 274, 275, 15, 245, 112, 2, 55, 132, 172, 216, 183, 198, 214, 93, 123, 142, 86, 174, 98, 63, 200, 99, 181, 5, 128, 40, 171, 80, 94, 226, 188, 221, 228, 133, 64, 154, 117, 180, 207, 8, 49, 244, 251, 23, 115, 196, 25, 108, 179, 296, 107, 197, 260, 232, 76 in input data\n",
      "training data did not have the following fields: f153, f11, f247, f267, f160, f185, f32, f211, f145, f280, f100, f72, f163, f239, f12, f8, f75, f246, f95, f89, f50, f68, f65, f151, f152, f154, f212, f208, f209, f198, f213, f115, f90, f233, f63, f144, f287, f16, f183, f92, f206, f138, f44, f96, f56, f159, f79, f173, f228, f31, f84, f139, f184, f194, f104, f254, f156, f255, f91, f120, f105, f252, f73, f277, f286, f281, f290, f19, f40, f78, f201, f186, f204, f234, f236, f85, f140, f262, f288, f3, f53, f250, f42, f33, f117, f272, f242, f133, f229, f284, f131, f260, f230, f237, f21, f54, f83, f275, f297, f182, f5, f80, f1, f241, f295, f147, f285, f221, f70, f197, f294, f217, f130, f67, f265, f289, f13, f111, f14, f26, f121, f143, f162, f166, f119, f193, f224, f57, f45, f226, f76, f169, f58, f172, f36, f107, f161, f251, f176, f20, f232, f205, f124, f283, f214, f71, f27, f137, f218, f177, f64, f35, f167, f55, f52, f240, f69, f114, f135, f210, f257, f164, f157, f37, f225, f6, f88, f269, f227, f249, f29, f298, f34, f102, f271, f2, f43, f196, f170, f181, f18, f47, f30, f238, f258, f39, f219, f4, f245, f62, f141, f171, f235, f261, f66, f150, f174, f220, f291, f28, f106, f134, f207, f41, f215, f191, f49, f148, f180, f99, f203, f10, f168, f243, f264, f149, f108, f192, f74, f200, f244, f60, f123, f59, f136, f9, f299, f259, f274, f22, f165, f132, f93, f195, f94, f15, f116, f128, f256, f178, f17, f24, f38, f126, f77, f155, f199, f129, f46, f122, f223, f48, f273, f263, f282, f103, f179, f81, f86, f110, f187, f278, f231, f296, f142, f125, f190, f216, f118, f276, f101, f268, f253, f222, f113, f270, f97, f248, f82, f51, f112, f279, f266, f109, f188, f202, f293, f0, f87, f25, f158, f175, f146, f23, f7, f189, f127, f98, f61, f292.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299']\n",
      "expected 31, 24, 152, 146, 169, 121, 177, 254, 262, 159, 136, 161, 90, 144, 96, 139, 195, 282, 186, 263, 75, 227, 6, 281, 92, 72, 218, 74, 204, 65, 194, 59, 259, 29, 138, 148, 271, 39, 113, 104, 129, 1, 223, 70, 85, 18, 45, 77, 256, 252, 110, 185, 3, 105, 175, 290, 102, 69, 4, 27, 224, 135, 168, 294, 248, 202, 164, 178, 145, 234, 44, 73, 127, 240, 276, 279, 238, 81, 134, 66, 33, 213, 30, 124, 236, 253, 257, 265, 166, 26, 241, 277, 42, 258, 158, 61, 298, 43, 83, 149, 206, 267, 50, 47, 189, 38, 20, 131, 141, 237, 266, 163, 34, 95, 167, 264, 82, 84, 52, 37, 184, 247, 7, 91, 111, 203, 122, 162, 280, 109, 211, 35, 120, 270, 291, 292, 255, 21, 87, 101, 79, 140, 170, 173, 215, 217, 246, 249, 201, 284, 219, 242, 243, 272, 293, 130, 88, 209, 268, 233, 157, 78, 116, 125, 231, 192, 269, 36, 126, 11, 51, 62, 151, 295, 235, 250, 261, 58, 32, 191, 287, 230, 56, 229, 278, 288, 299, 285, 14, 103, 22, 118, 150, 160, 16, 182, 60, 208, 199, 53, 67, 0, 54, 97, 9, 155, 156, 190, 19, 10, 114, 289, 119, 143, 205, 147, 28, 17, 176, 225, 71, 212, 12, 13, 41, 193, 210, 165, 222, 239, 273, 286, 48, 46, 220, 89, 106, 283, 100, 153, 137, 68, 57, 187, 297, 274, 275, 15, 245, 112, 2, 55, 132, 172, 216, 183, 198, 214, 93, 123, 142, 86, 174, 98, 63, 200, 99, 181, 5, 128, 40, 171, 80, 94, 226, 188, 221, 228, 133, 64, 154, 117, 180, 207, 8, 49, 244, 251, 23, 115, 196, 25, 108, 179, 296, 107, 197, 260, 232, 76 in input data\n",
      "training data did not have the following fields: f153, f11, f247, f267, f160, f185, f32, f211, f145, f280, f100, f72, f163, f239, f12, f8, f75, f246, f95, f89, f50, f68, f65, f151, f152, f154, f212, f208, f209, f198, f213, f115, f90, f233, f63, f144, f287, f16, f183, f92, f206, f138, f44, f96, f56, f159, f79, f173, f228, f31, f84, f139, f184, f194, f104, f254, f156, f255, f91, f120, f105, f252, f73, f277, f286, f281, f290, f19, f40, f78, f201, f186, f204, f234, f236, f85, f140, f262, f288, f3, f53, f250, f42, f33, f117, f272, f242, f133, f229, f284, f131, f260, f230, f237, f21, f54, f83, f275, f297, f182, f5, f80, f1, f241, f295, f147, f285, f221, f70, f197, f294, f217, f130, f67, f265, f289, f13, f111, f14, f26, f121, f143, f162, f166, f119, f193, f224, f57, f45, f226, f76, f169, f58, f172, f36, f107, f161, f251, f176, f20, f232, f205, f124, f283, f214, f71, f27, f137, f218, f177, f64, f35, f167, f55, f52, f240, f69, f114, f135, f210, f257, f164, f157, f37, f225, f6, f88, f269, f227, f249, f29, f298, f34, f102, f271, f2, f43, f196, f170, f181, f18, f47, f30, f238, f258, f39, f219, f4, f245, f62, f141, f171, f235, f261, f66, f150, f174, f220, f291, f28, f106, f134, f207, f41, f215, f191, f49, f148, f180, f99, f203, f10, f168, f243, f264, f149, f108, f192, f74, f200, f244, f60, f123, f59, f136, f9, f299, f259, f274, f22, f165, f132, f93, f195, f94, f15, f116, f128, f256, f178, f17, f24, f38, f126, f77, f155, f199, f129, f46, f122, f223, f48, f273, f263, f282, f103, f179, f81, f86, f110, f187, f278, f231, f296, f142, f125, f190, f216, f118, f276, f101, f268, f253, f222, f113, f270, f97, f248, f82, f51, f112, f279, f266, f109, f188, f202, f293, f0, f87, f25, f158, f175, f146, f23, f7, f189, f127, f98, f61, f292.\n",
      "Generation 6 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-4\t0.764\tLogisticRegression(RFE(MinMaxScaler(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.0001)), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.35000000000000003, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.5), LogisticRegression__C=5.0, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 7 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-3\t0.764\tLogisticRegression(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.35000000000000003, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.5), LogisticRegression__C=5.0, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299']\n",
      "expected 31, 24, 152, 146, 169, 121, 177, 254, 262, 159, 136, 161, 90, 144, 96, 139, 195, 282, 186, 263, 75, 227, 6, 281, 92, 72, 218, 74, 204, 65, 194, 59, 259, 29, 138, 148, 271, 39, 113, 104, 129, 1, 223, 70, 85, 18, 45, 77, 256, 252, 110, 185, 3, 105, 175, 290, 102, 69, 4, 27, 224, 135, 168, 294, 248, 202, 164, 178, 145, 234, 44, 73, 127, 240, 276, 279, 238, 81, 134, 66, 33, 213, 30, 124, 236, 253, 257, 265, 166, 26, 241, 277, 42, 258, 158, 61, 298, 43, 83, 149, 206, 267, 50, 47, 189, 38, 20, 131, 141, 237, 266, 163, 34, 95, 167, 264, 82, 84, 52, 37, 184, 247, 7, 91, 111, 203, 122, 162, 280, 109, 211, 35, 120, 270, 291, 292, 255, 21, 87, 101, 79, 140, 170, 173, 215, 217, 246, 249, 201, 284, 219, 242, 243, 272, 293, 130, 88, 209, 268, 233, 157, 78, 116, 125, 231, 192, 269, 36, 126, 11, 51, 62, 151, 295, 235, 250, 261, 58, 32, 191, 287, 230, 56, 229, 278, 288, 299, 285, 14, 103, 22, 118, 150, 160, 16, 182, 60, 208, 199, 53, 67, 0, 54, 97, 9, 155, 156, 190, 19, 10, 114, 289, 119, 143, 205, 147, 28, 17, 176, 225, 71, 212, 12, 13, 41, 193, 210, 165, 222, 239, 273, 286, 48, 46, 220, 89, 106, 283, 100, 153, 137, 68, 57, 187, 297, 274, 275, 15, 245, 112, 2, 55, 132, 172, 216, 183, 198, 214, 93, 123, 142, 86, 174, 98, 63, 200, 99, 181, 5, 128, 40, 171, 80, 94, 226, 188, 221, 228, 133, 64, 154, 117, 180, 207, 8, 49, 244, 251, 23, 115, 196, 25, 108, 179, 296, 107, 197, 260, 232, 76 in input data\n",
      "training data did not have the following fields: f153, f11, f247, f267, f160, f185, f32, f211, f145, f280, f100, f72, f163, f239, f12, f8, f75, f246, f95, f89, f50, f68, f65, f151, f152, f154, f212, f208, f209, f198, f213, f115, f90, f233, f63, f144, f287, f16, f183, f92, f206, f138, f44, f96, f56, f159, f79, f173, f228, f31, f84, f139, f184, f194, f104, f254, f156, f255, f91, f120, f105, f252, f73, f277, f286, f281, f290, f19, f40, f78, f201, f186, f204, f234, f236, f85, f140, f262, f288, f3, f53, f250, f42, f33, f117, f272, f242, f133, f229, f284, f131, f260, f230, f237, f21, f54, f83, f275, f297, f182, f5, f80, f1, f241, f295, f147, f285, f221, f70, f197, f294, f217, f130, f67, f265, f289, f13, f111, f14, f26, f121, f143, f162, f166, f119, f193, f224, f57, f45, f226, f76, f169, f58, f172, f36, f107, f161, f251, f176, f20, f232, f205, f124, f283, f214, f71, f27, f137, f218, f177, f64, f35, f167, f55, f52, f240, f69, f114, f135, f210, f257, f164, f157, f37, f225, f6, f88, f269, f227, f249, f29, f298, f34, f102, f271, f2, f43, f196, f170, f181, f18, f47, f30, f238, f258, f39, f219, f4, f245, f62, f141, f171, f235, f261, f66, f150, f174, f220, f291, f28, f106, f134, f207, f41, f215, f191, f49, f148, f180, f99, f203, f10, f168, f243, f264, f149, f108, f192, f74, f200, f244, f60, f123, f59, f136, f9, f299, f259, f274, f22, f165, f132, f93, f195, f94, f15, f116, f128, f256, f178, f17, f24, f38, f126, f77, f155, f199, f129, f46, f122, f223, f48, f273, f263, f282, f103, f179, f81, f86, f110, f187, f278, f231, f296, f142, f125, f190, f216, f118, f276, f101, f268, f253, f222, f113, f270, f97, f248, f82, f51, f112, f279, f266, f109, f188, f202, f293, f0, f87, f25, f158, f175, f146, f23, f7, f189, f127, f98, f61, f292.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Generation 8 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-3\t0.764\tLogisticRegression(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.35000000000000003, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.5), LogisticRegression__C=5.0, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.\n",
      "Generation 9 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-3\t0.764\tLogisticRegression(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.35000000000000003, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.5), LogisticRegression__C=5.0, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Generation 10 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-3\t0.764\tLogisticRegression(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.35000000000000003, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.5), LogisticRegression__C=5.0, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-4\t0.768\tLogisticRegression(RFE(MinMaxScaler(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.0001)), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.35000000000000003, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.5), LogisticRegression__C=10.0, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 X contains negative values..\n",
      "Generation 11 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-3\t0.764\tLogisticRegression(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.35000000000000003, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.5), LogisticRegression__C=5.0, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-4\t0.768\tLogisticRegression(RFE(MinMaxScaler(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.0001)), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.35000000000000003, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.5), LogisticRegression__C=10.0, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 12 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.768\tGradientBoostingClassifier(SelectPercentile(input_matrix, SelectPercentile__percentile=65), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=1, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=20, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.55)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
      "Generation 13 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.768\tGradientBoostingClassifier(SelectPercentile(input_matrix, SelectPercentile__percentile=65), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=1, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=20, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.55)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "Skipped pipeline #367 due to time out. Continuing to the next pipeline.\n",
      "Generation 14 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.768\tGradientBoostingClassifier(SelectPercentile(input_matrix, SelectPercentile__percentile=65), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=1, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=20, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.55)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "Generation 15 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.768\tGradientBoostingClassifier(SelectPercentile(input_matrix, SelectPercentile__percentile=65), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=1, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=20, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.55)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 16 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.768\tGradientBoostingClassifier(SelectPercentile(input_matrix, SelectPercentile__percentile=65), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=1, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=20, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.55)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 17 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.768\tGradientBoostingClassifier(SelectPercentile(input_matrix, SelectPercentile__percentile=65), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=1, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=20, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.55)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299']\n",
      "expected 31, 24, 152, 146, 169, 121, 177, 254, 262, 159, 136, 161, 90, 144, 96, 139, 195, 282, 186, 263, 75, 227, 6, 281, 92, 72, 218, 74, 204, 65, 194, 59, 259, 29, 138, 148, 271, 39, 113, 104, 129, 1, 223, 70, 85, 18, 45, 77, 256, 252, 110, 185, 3, 105, 175, 290, 102, 69, 4, 27, 224, 135, 168, 294, 248, 202, 164, 178, 145, 234, 44, 73, 127, 240, 276, 279, 238, 81, 134, 66, 33, 213, 30, 124, 236, 253, 257, 265, 166, 26, 241, 277, 42, 258, 158, 61, 298, 43, 83, 149, 206, 267, 50, 47, 189, 38, 20, 131, 141, 237, 266, 163, 34, 95, 167, 264, 82, 84, 52, 37, 184, 247, 7, 91, 111, 203, 122, 162, 280, 109, 211, 35, 120, 270, 291, 292, 255, 21, 87, 101, 79, 140, 170, 173, 215, 217, 246, 249, 201, 284, 219, 242, 243, 272, 293, 130, 88, 209, 268, 233, 157, 78, 116, 125, 231, 192, 269, 36, 126, 11, 51, 62, 151, 295, 235, 250, 261, 58, 32, 191, 287, 230, 56, 229, 278, 288, 299, 285, 14, 103, 22, 118, 150, 160, 16, 182, 60, 208, 199, 53, 67, 0, 54, 97, 9, 155, 156, 190, 19, 10, 114, 289, 119, 143, 205, 147, 28, 17, 176, 225, 71, 212, 12, 13, 41, 193, 210, 165, 222, 239, 273, 286, 48, 46, 220, 89, 106, 283, 100, 153, 137, 68, 57, 187, 297, 274, 275, 15, 245, 112, 2, 55, 132, 172, 216, 183, 198, 214, 93, 123, 142, 86, 174, 98, 63, 200, 99, 181, 5, 128, 40, 171, 80, 94, 226, 188, 221, 228, 133, 64, 154, 117, 180, 207, 8, 49, 244, 251, 23, 115, 196, 25, 108, 179, 296, 107, 197, 260, 232, 76 in input data\n",
      "training data did not have the following fields: f153, f11, f247, f267, f160, f185, f32, f211, f145, f280, f100, f72, f163, f239, f12, f8, f75, f246, f95, f89, f50, f68, f65, f151, f152, f154, f212, f208, f209, f198, f213, f115, f90, f233, f63, f144, f287, f16, f183, f92, f206, f138, f44, f96, f56, f159, f79, f173, f228, f31, f84, f139, f184, f194, f104, f254, f156, f255, f91, f120, f105, f252, f73, f277, f286, f281, f290, f19, f40, f78, f201, f186, f204, f234, f236, f85, f140, f262, f288, f3, f53, f250, f42, f33, f117, f272, f242, f133, f229, f284, f131, f260, f230, f237, f21, f54, f83, f275, f297, f182, f5, f80, f1, f241, f295, f147, f285, f221, f70, f197, f294, f217, f130, f67, f265, f289, f13, f111, f14, f26, f121, f143, f162, f166, f119, f193, f224, f57, f45, f226, f76, f169, f58, f172, f36, f107, f161, f251, f176, f20, f232, f205, f124, f283, f214, f71, f27, f137, f218, f177, f64, f35, f167, f55, f52, f240, f69, f114, f135, f210, f257, f164, f157, f37, f225, f6, f88, f269, f227, f249, f29, f298, f34, f102, f271, f2, f43, f196, f170, f181, f18, f47, f30, f238, f258, f39, f219, f4, f245, f62, f141, f171, f235, f261, f66, f150, f174, f220, f291, f28, f106, f134, f207, f41, f215, f191, f49, f148, f180, f99, f203, f10, f168, f243, f264, f149, f108, f192, f74, f200, f244, f60, f123, f59, f136, f9, f299, f259, f274, f22, f165, f132, f93, f195, f94, f15, f116, f128, f256, f178, f17, f24, f38, f126, f77, f155, f199, f129, f46, f122, f223, f48, f273, f263, f282, f103, f179, f81, f86, f110, f187, f278, f231, f296, f142, f125, f190, f216, f118, f276, f101, f268, f253, f222, f113, f270, f97, f248, f82, f51, f112, f279, f266, f109, f188, f202, f293, f0, f87, f25, f158, f175, f146, f23, f7, f189, f127, f98, f61, f292.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299']\n",
      "expected 31, 24, 152, 146, 169, 121, 177, 254, 262, 159, 136, 161, 90, 144, 96, 139, 195, 282, 186, 263, 75, 227, 6, 281, 92, 72, 218, 74, 204, 65, 194, 59, 259, 29, 138, 148, 271, 39, 113, 104, 129, 1, 223, 70, 85, 18, 45, 77, 256, 252, 110, 185, 3, 105, 175, 290, 102, 69, 4, 27, 224, 135, 168, 294, 248, 202, 164, 178, 145, 234, 44, 73, 127, 240, 276, 279, 238, 81, 134, 66, 33, 213, 30, 124, 236, 253, 257, 265, 166, 26, 241, 277, 42, 258, 158, 61, 298, 43, 83, 149, 206, 267, 50, 47, 189, 38, 20, 131, 141, 237, 266, 163, 34, 95, 167, 264, 82, 84, 52, 37, 184, 247, 7, 91, 111, 203, 122, 162, 280, 109, 211, 35, 120, 270, 291, 292, 255, 21, 87, 101, 79, 140, 170, 173, 215, 217, 246, 249, 201, 284, 219, 242, 243, 272, 293, 130, 88, 209, 268, 233, 157, 78, 116, 125, 231, 192, 269, 36, 126, 11, 51, 62, 151, 295, 235, 250, 261, 58, 32, 191, 287, 230, 56, 229, 278, 288, 299, 285, 14, 103, 22, 118, 150, 160, 16, 182, 60, 208, 199, 53, 67, 0, 54, 97, 9, 155, 156, 190, 19, 10, 114, 289, 119, 143, 205, 147, 28, 17, 176, 225, 71, 212, 12, 13, 41, 193, 210, 165, 222, 239, 273, 286, 48, 46, 220, 89, 106, 283, 100, 153, 137, 68, 57, 187, 297, 274, 275, 15, 245, 112, 2, 55, 132, 172, 216, 183, 198, 214, 93, 123, 142, 86, 174, 98, 63, 200, 99, 181, 5, 128, 40, 171, 80, 94, 226, 188, 221, 228, 133, 64, 154, 117, 180, 207, 8, 49, 244, 251, 23, 115, 196, 25, 108, 179, 296, 107, 197, 260, 232, 76 in input data\n",
      "training data did not have the following fields: f153, f11, f247, f267, f160, f185, f32, f211, f145, f280, f100, f72, f163, f239, f12, f8, f75, f246, f95, f89, f50, f68, f65, f151, f152, f154, f212, f208, f209, f198, f213, f115, f90, f233, f63, f144, f287, f16, f183, f92, f206, f138, f44, f96, f56, f159, f79, f173, f228, f31, f84, f139, f184, f194, f104, f254, f156, f255, f91, f120, f105, f252, f73, f277, f286, f281, f290, f19, f40, f78, f201, f186, f204, f234, f236, f85, f140, f262, f288, f3, f53, f250, f42, f33, f117, f272, f242, f133, f229, f284, f131, f260, f230, f237, f21, f54, f83, f275, f297, f182, f5, f80, f1, f241, f295, f147, f285, f221, f70, f197, f294, f217, f130, f67, f265, f289, f13, f111, f14, f26, f121, f143, f162, f166, f119, f193, f224, f57, f45, f226, f76, f169, f58, f172, f36, f107, f161, f251, f176, f20, f232, f205, f124, f283, f214, f71, f27, f137, f218, f177, f64, f35, f167, f55, f52, f240, f69, f114, f135, f210, f257, f164, f157, f37, f225, f6, f88, f269, f227, f249, f29, f298, f34, f102, f271, f2, f43, f196, f170, f181, f18, f47, f30, f238, f258, f39, f219, f4, f245, f62, f141, f171, f235, f261, f66, f150, f174, f220, f291, f28, f106, f134, f207, f41, f215, f191, f49, f148, f180, f99, f203, f10, f168, f243, f264, f149, f108, f192, f74, f200, f244, f60, f123, f59, f136, f9, f299, f259, f274, f22, f165, f132, f93, f195, f94, f15, f116, f128, f256, f178, f17, f24, f38, f126, f77, f155, f199, f129, f46, f122, f223, f48, f273, f263, f282, f103, f179, f81, f86, f110, f187, f278, f231, f296, f142, f125, f190, f216, f118, f276, f101, f268, f253, f222, f113, f270, f97, f248, f82, f51, f112, f279, f266, f109, f188, f202, f293, f0, f87, f25, f158, f175, f146, f23, f7, f189, f127, f98, f61, f292.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 18 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.768\tGradientBoostingClassifier(SelectPercentile(input_matrix, SelectPercentile__percentile=65), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=1, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=20, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.55)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 19 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.768\tGradientBoostingClassifier(SelectPercentile(input_matrix, SelectPercentile__percentile=65), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=1, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=20, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.55)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 20 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.768\tGradientBoostingClassifier(SelectPercentile(input_matrix, SelectPercentile__percentile=65), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=1, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=20, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.55)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299']\n",
      "expected 31, 24, 152, 146, 169, 121, 177, 254, 262, 159, 136, 161, 90, 144, 96, 139, 195, 282, 186, 263, 75, 227, 6, 281, 92, 72, 218, 74, 204, 65, 194, 59, 259, 29, 138, 148, 271, 39, 113, 104, 129, 1, 223, 70, 85, 18, 45, 77, 256, 252, 110, 185, 3, 105, 175, 290, 102, 69, 4, 27, 224, 135, 168, 294, 248, 202, 164, 178, 145, 234, 44, 73, 127, 240, 276, 279, 238, 81, 134, 66, 33, 213, 30, 124, 236, 253, 257, 265, 166, 26, 241, 277, 42, 258, 158, 61, 298, 43, 83, 149, 206, 267, 50, 47, 189, 38, 20, 131, 141, 237, 266, 163, 34, 95, 167, 264, 82, 84, 52, 37, 184, 247, 7, 91, 111, 203, 122, 162, 280, 109, 211, 35, 120, 270, 291, 292, 255, 21, 87, 101, 79, 140, 170, 173, 215, 217, 246, 249, 201, 284, 219, 242, 243, 272, 293, 130, 88, 209, 268, 233, 157, 78, 116, 125, 231, 192, 269, 36, 126, 11, 51, 62, 151, 295, 235, 250, 261, 58, 32, 191, 287, 230, 56, 229, 278, 288, 299, 285, 14, 103, 22, 118, 150, 160, 16, 182, 60, 208, 199, 53, 67, 0, 54, 97, 9, 155, 156, 190, 19, 10, 114, 289, 119, 143, 205, 147, 28, 17, 176, 225, 71, 212, 12, 13, 41, 193, 210, 165, 222, 239, 273, 286, 48, 46, 220, 89, 106, 283, 100, 153, 137, 68, 57, 187, 297, 274, 275, 15, 245, 112, 2, 55, 132, 172, 216, 183, 198, 214, 93, 123, 142, 86, 174, 98, 63, 200, 99, 181, 5, 128, 40, 171, 80, 94, 226, 188, 221, 228, 133, 64, 154, 117, 180, 207, 8, 49, 244, 251, 23, 115, 196, 25, 108, 179, 296, 107, 197, 260, 232, 76 in input data\n",
      "training data did not have the following fields: f153, f11, f247, f267, f160, f185, f32, f211, f145, f280, f100, f72, f163, f239, f12, f8, f75, f246, f95, f89, f50, f68, f65, f151, f152, f154, f212, f208, f209, f198, f213, f115, f90, f233, f63, f144, f287, f16, f183, f92, f206, f138, f44, f96, f56, f159, f79, f173, f228, f31, f84, f139, f184, f194, f104, f254, f156, f255, f91, f120, f105, f252, f73, f277, f286, f281, f290, f19, f40, f78, f201, f186, f204, f234, f236, f85, f140, f262, f288, f3, f53, f250, f42, f33, f117, f272, f242, f133, f229, f284, f131, f260, f230, f237, f21, f54, f83, f275, f297, f182, f5, f80, f1, f241, f295, f147, f285, f221, f70, f197, f294, f217, f130, f67, f265, f289, f13, f111, f14, f26, f121, f143, f162, f166, f119, f193, f224, f57, f45, f226, f76, f169, f58, f172, f36, f107, f161, f251, f176, f20, f232, f205, f124, f283, f214, f71, f27, f137, f218, f177, f64, f35, f167, f55, f52, f240, f69, f114, f135, f210, f257, f164, f157, f37, f225, f6, f88, f269, f227, f249, f29, f298, f34, f102, f271, f2, f43, f196, f170, f181, f18, f47, f30, f238, f258, f39, f219, f4, f245, f62, f141, f171, f235, f261, f66, f150, f174, f220, f291, f28, f106, f134, f207, f41, f215, f191, f49, f148, f180, f99, f203, f10, f168, f243, f264, f149, f108, f192, f74, f200, f244, f60, f123, f59, f136, f9, f299, f259, f274, f22, f165, f132, f93, f195, f94, f15, f116, f128, f256, f178, f17, f24, f38, f126, f77, f155, f199, f129, f46, f122, f223, f48, f273, f263, f282, f103, f179, f81, f86, f110, f187, f278, f231, f296, f142, f125, f190, f216, f118, f276, f101, f268, f253, f222, f113, f270, f97, f248, f82, f51, f112, f279, f266, f109, f188, f202, f293, f0, f87, f25, f158, f175, f146, f23, f7, f189, f127, f98, f61, f292.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299']\n",
      "expected 31, 24, 152, 146, 169, 121, 177, 254, 262, 159, 136, 161, 90, 144, 96, 139, 195, 282, 186, 263, 75, 227, 6, 281, 92, 72, 218, 74, 204, 65, 194, 59, 259, 29, 138, 148, 271, 39, 113, 104, 129, 1, 223, 70, 85, 18, 45, 77, 256, 252, 110, 185, 3, 105, 175, 290, 102, 69, 4, 27, 224, 135, 168, 294, 248, 202, 164, 178, 145, 234, 44, 73, 127, 240, 276, 279, 238, 81, 134, 66, 33, 213, 30, 124, 236, 253, 257, 265, 166, 26, 241, 277, 42, 258, 158, 61, 298, 43, 83, 149, 206, 267, 50, 47, 189, 38, 20, 131, 141, 237, 266, 163, 34, 95, 167, 264, 82, 84, 52, 37, 184, 247, 7, 91, 111, 203, 122, 162, 280, 109, 211, 35, 120, 270, 291, 292, 255, 21, 87, 101, 79, 140, 170, 173, 215, 217, 246, 249, 201, 284, 219, 242, 243, 272, 293, 130, 88, 209, 268, 233, 157, 78, 116, 125, 231, 192, 269, 36, 126, 11, 51, 62, 151, 295, 235, 250, 261, 58, 32, 191, 287, 230, 56, 229, 278, 288, 299, 285, 14, 103, 22, 118, 150, 160, 16, 182, 60, 208, 199, 53, 67, 0, 54, 97, 9, 155, 156, 190, 19, 10, 114, 289, 119, 143, 205, 147, 28, 17, 176, 225, 71, 212, 12, 13, 41, 193, 210, 165, 222, 239, 273, 286, 48, 46, 220, 89, 106, 283, 100, 153, 137, 68, 57, 187, 297, 274, 275, 15, 245, 112, 2, 55, 132, 172, 216, 183, 198, 214, 93, 123, 142, 86, 174, 98, 63, 200, 99, 181, 5, 128, 40, 171, 80, 94, 226, 188, 221, 228, 133, 64, 154, 117, 180, 207, 8, 49, 244, 251, 23, 115, 196, 25, 108, 179, 296, 107, 197, 260, 232, 76 in input data\n",
      "training data did not have the following fields: f153, f11, f247, f267, f160, f185, f32, f211, f145, f280, f100, f72, f163, f239, f12, f8, f75, f246, f95, f89, f50, f68, f65, f151, f152, f154, f212, f208, f209, f198, f213, f115, f90, f233, f63, f144, f287, f16, f183, f92, f206, f138, f44, f96, f56, f159, f79, f173, f228, f31, f84, f139, f184, f194, f104, f254, f156, f255, f91, f120, f105, f252, f73, f277, f286, f281, f290, f19, f40, f78, f201, f186, f204, f234, f236, f85, f140, f262, f288, f3, f53, f250, f42, f33, f117, f272, f242, f133, f229, f284, f131, f260, f230, f237, f21, f54, f83, f275, f297, f182, f5, f80, f1, f241, f295, f147, f285, f221, f70, f197, f294, f217, f130, f67, f265, f289, f13, f111, f14, f26, f121, f143, f162, f166, f119, f193, f224, f57, f45, f226, f76, f169, f58, f172, f36, f107, f161, f251, f176, f20, f232, f205, f124, f283, f214, f71, f27, f137, f218, f177, f64, f35, f167, f55, f52, f240, f69, f114, f135, f210, f257, f164, f157, f37, f225, f6, f88, f269, f227, f249, f29, f298, f34, f102, f271, f2, f43, f196, f170, f181, f18, f47, f30, f238, f258, f39, f219, f4, f245, f62, f141, f171, f235, f261, f66, f150, f174, f220, f291, f28, f106, f134, f207, f41, f215, f191, f49, f148, f180, f99, f203, f10, f168, f243, f264, f149, f108, f192, f74, f200, f244, f60, f123, f59, f136, f9, f299, f259, f274, f22, f165, f132, f93, f195, f94, f15, f116, f128, f256, f178, f17, f24, f38, f126, f77, f155, f199, f129, f46, f122, f223, f48, f273, f263, f282, f103, f179, f81, f86, f110, f187, f278, f231, f296, f142, f125, f190, f216, f118, f276, f101, f268, f253, f222, f113, f270, f97, f248, f82, f51, f112, f279, f266, f109, f188, f202, f293, f0, f87, f25, f158, f175, f146, f23, f7, f189, f127, f98, f61, f292.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 21 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.768\tGradientBoostingClassifier(SelectPercentile(input_matrix, SelectPercentile__percentile=65), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=1, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=20, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.55)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299']\n",
      "expected 31, 24, 152, 146, 169, 121, 177, 254, 262, 159, 136, 161, 90, 144, 96, 139, 195, 282, 186, 263, 75, 227, 6, 281, 92, 72, 218, 74, 204, 65, 194, 59, 259, 29, 138, 148, 271, 39, 113, 104, 129, 1, 223, 70, 85, 18, 45, 77, 256, 252, 110, 185, 3, 105, 175, 290, 102, 69, 4, 27, 224, 135, 168, 294, 248, 202, 164, 178, 145, 234, 44, 73, 127, 240, 276, 279, 238, 81, 134, 66, 33, 213, 30, 124, 236, 253, 257, 265, 166, 26, 241, 277, 42, 258, 158, 61, 298, 43, 83, 149, 206, 267, 50, 47, 189, 38, 20, 131, 141, 237, 266, 163, 34, 95, 167, 264, 82, 84, 52, 37, 184, 247, 7, 91, 111, 203, 122, 162, 280, 109, 211, 35, 120, 270, 291, 292, 255, 21, 87, 101, 79, 140, 170, 173, 215, 217, 246, 249, 201, 284, 219, 242, 243, 272, 293, 130, 88, 209, 268, 233, 157, 78, 116, 125, 231, 192, 269, 36, 126, 11, 51, 62, 151, 295, 235, 250, 261, 58, 32, 191, 287, 230, 56, 229, 278, 288, 299, 285, 14, 103, 22, 118, 150, 160, 16, 182, 60, 208, 199, 53, 67, 0, 54, 97, 9, 155, 156, 190, 19, 10, 114, 289, 119, 143, 205, 147, 28, 17, 176, 225, 71, 212, 12, 13, 41, 193, 210, 165, 222, 239, 273, 286, 48, 46, 220, 89, 106, 283, 100, 153, 137, 68, 57, 187, 297, 274, 275, 15, 245, 112, 2, 55, 132, 172, 216, 183, 198, 214, 93, 123, 142, 86, 174, 98, 63, 200, 99, 181, 5, 128, 40, 171, 80, 94, 226, 188, 221, 228, 133, 64, 154, 117, 180, 207, 8, 49, 244, 251, 23, 115, 196, 25, 108, 179, 296, 107, 197, 260, 232, 76 in input data\n",
      "training data did not have the following fields: f153, f11, f247, f267, f160, f185, f32, f211, f145, f280, f100, f72, f163, f239, f12, f8, f75, f246, f95, f89, f50, f68, f65, f151, f152, f154, f212, f208, f209, f198, f213, f115, f90, f233, f63, f144, f287, f16, f183, f92, f206, f138, f44, f96, f56, f159, f79, f173, f228, f31, f84, f139, f184, f194, f104, f254, f156, f255, f91, f120, f105, f252, f73, f277, f286, f281, f290, f19, f40, f78, f201, f186, f204, f234, f236, f85, f140, f262, f288, f3, f53, f250, f42, f33, f117, f272, f242, f133, f229, f284, f131, f260, f230, f237, f21, f54, f83, f275, f297, f182, f5, f80, f1, f241, f295, f147, f285, f221, f70, f197, f294, f217, f130, f67, f265, f289, f13, f111, f14, f26, f121, f143, f162, f166, f119, f193, f224, f57, f45, f226, f76, f169, f58, f172, f36, f107, f161, f251, f176, f20, f232, f205, f124, f283, f214, f71, f27, f137, f218, f177, f64, f35, f167, f55, f52, f240, f69, f114, f135, f210, f257, f164, f157, f37, f225, f6, f88, f269, f227, f249, f29, f298, f34, f102, f271, f2, f43, f196, f170, f181, f18, f47, f30, f238, f258, f39, f219, f4, f245, f62, f141, f171, f235, f261, f66, f150, f174, f220, f291, f28, f106, f134, f207, f41, f215, f191, f49, f148, f180, f99, f203, f10, f168, f243, f264, f149, f108, f192, f74, f200, f244, f60, f123, f59, f136, f9, f299, f259, f274, f22, f165, f132, f93, f195, f94, f15, f116, f128, f256, f178, f17, f24, f38, f126, f77, f155, f199, f129, f46, f122, f223, f48, f273, f263, f282, f103, f179, f81, f86, f110, f187, f278, f231, f296, f142, f125, f190, f216, f118, f276, f101, f268, f253, f222, f113, f270, f97, f248, f82, f51, f112, f279, f266, f109, f188, f202, f293, f0, f87, f25, f158, f175, f146, f23, f7, f189, f127, f98, f61, f292.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 22 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.768\tGradientBoostingClassifier(SelectPercentile(input_matrix, SelectPercentile__percentile=65), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=1, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=20, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.55)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Generation 23 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.768\tGradientBoostingClassifier(SelectPercentile(input_matrix, SelectPercentile__percentile=65), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=1, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=20, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.55)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 24 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.768\tGradientBoostingClassifier(SelectPercentile(input_matrix, SelectPercentile__percentile=65), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=1, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=20, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.55)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 25 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.768\tGradientBoostingClassifier(SelectPercentile(input_matrix, SelectPercentile__percentile=65), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=1, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=20, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.55)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299']\n",
      "expected 31, 24, 152, 146, 169, 121, 177, 254, 262, 159, 136, 161, 90, 144, 96, 139, 195, 282, 186, 263, 75, 227, 6, 281, 92, 72, 218, 74, 204, 65, 194, 59, 259, 29, 138, 148, 271, 39, 113, 104, 129, 1, 223, 70, 85, 18, 45, 77, 256, 252, 110, 185, 3, 105, 175, 290, 102, 69, 4, 27, 224, 135, 168, 294, 248, 202, 164, 178, 145, 234, 44, 73, 127, 240, 276, 279, 238, 81, 134, 66, 33, 213, 30, 124, 236, 253, 257, 265, 166, 26, 241, 277, 42, 258, 158, 61, 298, 43, 83, 149, 206, 267, 50, 47, 189, 38, 20, 131, 141, 237, 266, 163, 34, 95, 167, 264, 82, 84, 52, 37, 184, 247, 7, 91, 111, 203, 122, 162, 280, 109, 211, 35, 120, 270, 291, 292, 255, 21, 87, 101, 79, 140, 170, 173, 215, 217, 246, 249, 201, 284, 219, 242, 243, 272, 293, 130, 88, 209, 268, 233, 157, 78, 116, 125, 231, 192, 269, 36, 126, 11, 51, 62, 151, 295, 235, 250, 261, 58, 32, 191, 287, 230, 56, 229, 278, 288, 299, 285, 14, 103, 22, 118, 150, 160, 16, 182, 60, 208, 199, 53, 67, 0, 54, 97, 9, 155, 156, 190, 19, 10, 114, 289, 119, 143, 205, 147, 28, 17, 176, 225, 71, 212, 12, 13, 41, 193, 210, 165, 222, 239, 273, 286, 48, 46, 220, 89, 106, 283, 100, 153, 137, 68, 57, 187, 297, 274, 275, 15, 245, 112, 2, 55, 132, 172, 216, 183, 198, 214, 93, 123, 142, 86, 174, 98, 63, 200, 99, 181, 5, 128, 40, 171, 80, 94, 226, 188, 221, 228, 133, 64, 154, 117, 180, 207, 8, 49, 244, 251, 23, 115, 196, 25, 108, 179, 296, 107, 197, 260, 232, 76 in input data\n",
      "training data did not have the following fields: f153, f11, f247, f267, f160, f185, f32, f211, f145, f280, f100, f72, f163, f239, f12, f8, f75, f246, f95, f89, f50, f68, f65, f151, f152, f154, f212, f208, f209, f198, f213, f115, f90, f233, f63, f144, f287, f16, f183, f92, f206, f138, f44, f96, f56, f159, f79, f173, f228, f31, f84, f139, f184, f194, f104, f254, f156, f255, f91, f120, f105, f252, f73, f277, f286, f281, f290, f19, f40, f78, f201, f186, f204, f234, f236, f85, f140, f262, f288, f3, f53, f250, f42, f33, f117, f272, f242, f133, f229, f284, f131, f260, f230, f237, f21, f54, f83, f275, f297, f182, f5, f80, f1, f241, f295, f147, f285, f221, f70, f197, f294, f217, f130, f67, f265, f289, f13, f111, f14, f26, f121, f143, f162, f166, f119, f193, f224, f57, f45, f226, f76, f169, f58, f172, f36, f107, f161, f251, f176, f20, f232, f205, f124, f283, f214, f71, f27, f137, f218, f177, f64, f35, f167, f55, f52, f240, f69, f114, f135, f210, f257, f164, f157, f37, f225, f6, f88, f269, f227, f249, f29, f298, f34, f102, f271, f2, f43, f196, f170, f181, f18, f47, f30, f238, f258, f39, f219, f4, f245, f62, f141, f171, f235, f261, f66, f150, f174, f220, f291, f28, f106, f134, f207, f41, f215, f191, f49, f148, f180, f99, f203, f10, f168, f243, f264, f149, f108, f192, f74, f200, f244, f60, f123, f59, f136, f9, f299, f259, f274, f22, f165, f132, f93, f195, f94, f15, f116, f128, f256, f178, f17, f24, f38, f126, f77, f155, f199, f129, f46, f122, f223, f48, f273, f263, f282, f103, f179, f81, f86, f110, f187, f278, f231, f296, f142, f125, f190, f216, f118, f276, f101, f268, f253, f222, f113, f270, f97, f248, f82, f51, f112, f279, f266, f109, f188, f202, f293, f0, f87, f25, f158, f175, f146, f23, f7, f189, f127, f98, f61, f292.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 26 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.768\tGradientBoostingClassifier(SelectPercentile(input_matrix, SelectPercentile__percentile=65), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=1, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=20, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.55)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299']\n",
      "expected 31, 24, 152, 146, 169, 121, 177, 254, 262, 159, 136, 161, 90, 144, 96, 139, 195, 282, 186, 263, 75, 227, 6, 281, 92, 72, 218, 74, 204, 65, 194, 59, 259, 29, 138, 148, 271, 39, 113, 104, 129, 1, 223, 70, 85, 18, 45, 77, 256, 252, 110, 185, 3, 105, 175, 290, 102, 69, 4, 27, 224, 135, 168, 294, 248, 202, 164, 178, 145, 234, 44, 73, 127, 240, 276, 279, 238, 81, 134, 66, 33, 213, 30, 124, 236, 253, 257, 265, 166, 26, 241, 277, 42, 258, 158, 61, 298, 43, 83, 149, 206, 267, 50, 47, 189, 38, 20, 131, 141, 237, 266, 163, 34, 95, 167, 264, 82, 84, 52, 37, 184, 247, 7, 91, 111, 203, 122, 162, 280, 109, 211, 35, 120, 270, 291, 292, 255, 21, 87, 101, 79, 140, 170, 173, 215, 217, 246, 249, 201, 284, 219, 242, 243, 272, 293, 130, 88, 209, 268, 233, 157, 78, 116, 125, 231, 192, 269, 36, 126, 11, 51, 62, 151, 295, 235, 250, 261, 58, 32, 191, 287, 230, 56, 229, 278, 288, 299, 285, 14, 103, 22, 118, 150, 160, 16, 182, 60, 208, 199, 53, 67, 0, 54, 97, 9, 155, 156, 190, 19, 10, 114, 289, 119, 143, 205, 147, 28, 17, 176, 225, 71, 212, 12, 13, 41, 193, 210, 165, 222, 239, 273, 286, 48, 46, 220, 89, 106, 283, 100, 153, 137, 68, 57, 187, 297, 274, 275, 15, 245, 112, 2, 55, 132, 172, 216, 183, 198, 214, 93, 123, 142, 86, 174, 98, 63, 200, 99, 181, 5, 128, 40, 171, 80, 94, 226, 188, 221, 228, 133, 64, 154, 117, 180, 207, 8, 49, 244, 251, 23, 115, 196, 25, 108, 179, 296, 107, 197, 260, 232, 76 in input data\n",
      "training data did not have the following fields: f153, f11, f247, f267, f160, f185, f32, f211, f145, f280, f100, f72, f163, f239, f12, f8, f75, f246, f95, f89, f50, f68, f65, f151, f152, f154, f212, f208, f209, f198, f213, f115, f90, f233, f63, f144, f287, f16, f183, f92, f206, f138, f44, f96, f56, f159, f79, f173, f228, f31, f84, f139, f184, f194, f104, f254, f156, f255, f91, f120, f105, f252, f73, f277, f286, f281, f290, f19, f40, f78, f201, f186, f204, f234, f236, f85, f140, f262, f288, f3, f53, f250, f42, f33, f117, f272, f242, f133, f229, f284, f131, f260, f230, f237, f21, f54, f83, f275, f297, f182, f5, f80, f1, f241, f295, f147, f285, f221, f70, f197, f294, f217, f130, f67, f265, f289, f13, f111, f14, f26, f121, f143, f162, f166, f119, f193, f224, f57, f45, f226, f76, f169, f58, f172, f36, f107, f161, f251, f176, f20, f232, f205, f124, f283, f214, f71, f27, f137, f218, f177, f64, f35, f167, f55, f52, f240, f69, f114, f135, f210, f257, f164, f157, f37, f225, f6, f88, f269, f227, f249, f29, f298, f34, f102, f271, f2, f43, f196, f170, f181, f18, f47, f30, f238, f258, f39, f219, f4, f245, f62, f141, f171, f235, f261, f66, f150, f174, f220, f291, f28, f106, f134, f207, f41, f215, f191, f49, f148, f180, f99, f203, f10, f168, f243, f264, f149, f108, f192, f74, f200, f244, f60, f123, f59, f136, f9, f299, f259, f274, f22, f165, f132, f93, f195, f94, f15, f116, f128, f256, f178, f17, f24, f38, f126, f77, f155, f199, f129, f46, f122, f223, f48, f273, f263, f282, f103, f179, f81, f86, f110, f187, f278, f231, f296, f142, f125, f190, f216, f118, f276, f101, f268, f253, f222, f113, f270, f97, f248, f82, f51, f112, f279, f266, f109, f188, f202, f293, f0, f87, f25, f158, f175, f146, f23, f7, f189, f127, f98, f61, f292.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 27 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.768\tGradientBoostingClassifier(SelectPercentile(input_matrix, SelectPercentile__percentile=65), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=1, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=20, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.55)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "Generation 28 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.768\tGradientBoostingClassifier(SelectPercentile(input_matrix, SelectPercentile__percentile=65), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=1, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=20, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.55)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299']\n",
      "expected 31, 24, 152, 146, 169, 121, 177, 254, 262, 159, 136, 161, 90, 144, 96, 139, 195, 282, 186, 263, 75, 227, 6, 281, 92, 72, 218, 74, 204, 65, 194, 59, 259, 29, 138, 148, 271, 39, 113, 104, 129, 1, 223, 70, 85, 18, 45, 77, 256, 252, 110, 185, 3, 105, 175, 290, 102, 69, 4, 27, 224, 135, 168, 294, 248, 202, 164, 178, 145, 234, 44, 73, 127, 240, 276, 279, 238, 81, 134, 66, 33, 213, 30, 124, 236, 253, 257, 265, 166, 26, 241, 277, 42, 258, 158, 61, 298, 43, 83, 149, 206, 267, 50, 47, 189, 38, 20, 131, 141, 237, 266, 163, 34, 95, 167, 264, 82, 84, 52, 37, 184, 247, 7, 91, 111, 203, 122, 162, 280, 109, 211, 35, 120, 270, 291, 292, 255, 21, 87, 101, 79, 140, 170, 173, 215, 217, 246, 249, 201, 284, 219, 242, 243, 272, 293, 130, 88, 209, 268, 233, 157, 78, 116, 125, 231, 192, 269, 36, 126, 11, 51, 62, 151, 295, 235, 250, 261, 58, 32, 191, 287, 230, 56, 229, 278, 288, 299, 285, 14, 103, 22, 118, 150, 160, 16, 182, 60, 208, 199, 53, 67, 0, 54, 97, 9, 155, 156, 190, 19, 10, 114, 289, 119, 143, 205, 147, 28, 17, 176, 225, 71, 212, 12, 13, 41, 193, 210, 165, 222, 239, 273, 286, 48, 46, 220, 89, 106, 283, 100, 153, 137, 68, 57, 187, 297, 274, 275, 15, 245, 112, 2, 55, 132, 172, 216, 183, 198, 214, 93, 123, 142, 86, 174, 98, 63, 200, 99, 181, 5, 128, 40, 171, 80, 94, 226, 188, 221, 228, 133, 64, 154, 117, 180, 207, 8, 49, 244, 251, 23, 115, 196, 25, 108, 179, 296, 107, 197, 260, 232, 76 in input data\n",
      "training data did not have the following fields: f153, f11, f247, f267, f160, f185, f32, f211, f145, f280, f100, f72, f163, f239, f12, f8, f75, f246, f95, f89, f50, f68, f65, f151, f152, f154, f212, f208, f209, f198, f213, f115, f90, f233, f63, f144, f287, f16, f183, f92, f206, f138, f44, f96, f56, f159, f79, f173, f228, f31, f84, f139, f184, f194, f104, f254, f156, f255, f91, f120, f105, f252, f73, f277, f286, f281, f290, f19, f40, f78, f201, f186, f204, f234, f236, f85, f140, f262, f288, f3, f53, f250, f42, f33, f117, f272, f242, f133, f229, f284, f131, f260, f230, f237, f21, f54, f83, f275, f297, f182, f5, f80, f1, f241, f295, f147, f285, f221, f70, f197, f294, f217, f130, f67, f265, f289, f13, f111, f14, f26, f121, f143, f162, f166, f119, f193, f224, f57, f45, f226, f76, f169, f58, f172, f36, f107, f161, f251, f176, f20, f232, f205, f124, f283, f214, f71, f27, f137, f218, f177, f64, f35, f167, f55, f52, f240, f69, f114, f135, f210, f257, f164, f157, f37, f225, f6, f88, f269, f227, f249, f29, f298, f34, f102, f271, f2, f43, f196, f170, f181, f18, f47, f30, f238, f258, f39, f219, f4, f245, f62, f141, f171, f235, f261, f66, f150, f174, f220, f291, f28, f106, f134, f207, f41, f215, f191, f49, f148, f180, f99, f203, f10, f168, f243, f264, f149, f108, f192, f74, f200, f244, f60, f123, f59, f136, f9, f299, f259, f274, f22, f165, f132, f93, f195, f94, f15, f116, f128, f256, f178, f17, f24, f38, f126, f77, f155, f199, f129, f46, f122, f223, f48, f273, f263, f282, f103, f179, f81, f86, f110, f187, f278, f231, f296, f142, f125, f190, f216, f118, f276, f101, f268, f253, f222, f113, f270, f97, f248, f82, f51, f112, f279, f266, f109, f188, f202, f293, f0, f87, f25, f158, f175, f146, f23, f7, f189, f127, f98, f61, f292.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299']\n",
      "expected 31, 24, 152, 146, 169, 121, 177, 254, 262, 159, 136, 161, 90, 144, 96, 139, 195, 282, 186, 263, 75, 227, 6, 281, 92, 72, 218, 74, 204, 65, 194, 59, 259, 29, 138, 148, 271, 39, 113, 104, 129, 1, 223, 70, 85, 18, 45, 77, 256, 252, 110, 185, 3, 105, 175, 290, 102, 69, 4, 27, 224, 135, 168, 294, 248, 202, 164, 178, 145, 234, 44, 73, 127, 240, 276, 279, 238, 81, 134, 66, 33, 213, 30, 124, 236, 253, 257, 265, 166, 26, 241, 277, 42, 258, 158, 61, 298, 43, 83, 149, 206, 267, 50, 47, 189, 38, 20, 131, 141, 237, 266, 163, 34, 95, 167, 264, 82, 84, 52, 37, 184, 247, 7, 91, 111, 203, 122, 162, 280, 109, 211, 35, 120, 270, 291, 292, 255, 21, 87, 101, 79, 140, 170, 173, 215, 217, 246, 249, 201, 284, 219, 242, 243, 272, 293, 130, 88, 209, 268, 233, 157, 78, 116, 125, 231, 192, 269, 36, 126, 11, 51, 62, 151, 295, 235, 250, 261, 58, 32, 191, 287, 230, 56, 229, 278, 288, 299, 285, 14, 103, 22, 118, 150, 160, 16, 182, 60, 208, 199, 53, 67, 0, 54, 97, 9, 155, 156, 190, 19, 10, 114, 289, 119, 143, 205, 147, 28, 17, 176, 225, 71, 212, 12, 13, 41, 193, 210, 165, 222, 239, 273, 286, 48, 46, 220, 89, 106, 283, 100, 153, 137, 68, 57, 187, 297, 274, 275, 15, 245, 112, 2, 55, 132, 172, 216, 183, 198, 214, 93, 123, 142, 86, 174, 98, 63, 200, 99, 181, 5, 128, 40, 171, 80, 94, 226, 188, 221, 228, 133, 64, 154, 117, 180, 207, 8, 49, 244, 251, 23, 115, 196, 25, 108, 179, 296, 107, 197, 260, 232, 76 in input data\n",
      "training data did not have the following fields: f153, f11, f247, f267, f160, f185, f32, f211, f145, f280, f100, f72, f163, f239, f12, f8, f75, f246, f95, f89, f50, f68, f65, f151, f152, f154, f212, f208, f209, f198, f213, f115, f90, f233, f63, f144, f287, f16, f183, f92, f206, f138, f44, f96, f56, f159, f79, f173, f228, f31, f84, f139, f184, f194, f104, f254, f156, f255, f91, f120, f105, f252, f73, f277, f286, f281, f290, f19, f40, f78, f201, f186, f204, f234, f236, f85, f140, f262, f288, f3, f53, f250, f42, f33, f117, f272, f242, f133, f229, f284, f131, f260, f230, f237, f21, f54, f83, f275, f297, f182, f5, f80, f1, f241, f295, f147, f285, f221, f70, f197, f294, f217, f130, f67, f265, f289, f13, f111, f14, f26, f121, f143, f162, f166, f119, f193, f224, f57, f45, f226, f76, f169, f58, f172, f36, f107, f161, f251, f176, f20, f232, f205, f124, f283, f214, f71, f27, f137, f218, f177, f64, f35, f167, f55, f52, f240, f69, f114, f135, f210, f257, f164, f157, f37, f225, f6, f88, f269, f227, f249, f29, f298, f34, f102, f271, f2, f43, f196, f170, f181, f18, f47, f30, f238, f258, f39, f219, f4, f245, f62, f141, f171, f235, f261, f66, f150, f174, f220, f291, f28, f106, f134, f207, f41, f215, f191, f49, f148, f180, f99, f203, f10, f168, f243, f264, f149, f108, f192, f74, f200, f244, f60, f123, f59, f136, f9, f299, f259, f274, f22, f165, f132, f93, f195, f94, f15, f116, f128, f256, f178, f17, f24, f38, f126, f77, f155, f199, f129, f46, f122, f223, f48, f273, f263, f282, f103, f179, f81, f86, f110, f187, f278, f231, f296, f142, f125, f190, f216, f118, f276, f101, f268, f253, f222, f113, f270, f97, f248, f82, f51, f112, f279, f266, f109, f188, f202, f293, f0, f87, f25, f158, f175, f146, f23, f7, f189, f127, f98, f61, f292.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 29 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.772\tLogisticRegression(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.9000000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.5), LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299']\n",
      "expected 31, 24, 152, 146, 169, 121, 177, 254, 262, 159, 136, 161, 90, 144, 96, 139, 195, 282, 186, 263, 75, 227, 6, 281, 92, 72, 218, 74, 204, 65, 194, 59, 259, 29, 138, 148, 271, 39, 113, 104, 129, 1, 223, 70, 85, 18, 45, 77, 256, 252, 110, 185, 3, 105, 175, 290, 102, 69, 4, 27, 224, 135, 168, 294, 248, 202, 164, 178, 145, 234, 44, 73, 127, 240, 276, 279, 238, 81, 134, 66, 33, 213, 30, 124, 236, 253, 257, 265, 166, 26, 241, 277, 42, 258, 158, 61, 298, 43, 83, 149, 206, 267, 50, 47, 189, 38, 20, 131, 141, 237, 266, 163, 34, 95, 167, 264, 82, 84, 52, 37, 184, 247, 7, 91, 111, 203, 122, 162, 280, 109, 211, 35, 120, 270, 291, 292, 255, 21, 87, 101, 79, 140, 170, 173, 215, 217, 246, 249, 201, 284, 219, 242, 243, 272, 293, 130, 88, 209, 268, 233, 157, 78, 116, 125, 231, 192, 269, 36, 126, 11, 51, 62, 151, 295, 235, 250, 261, 58, 32, 191, 287, 230, 56, 229, 278, 288, 299, 285, 14, 103, 22, 118, 150, 160, 16, 182, 60, 208, 199, 53, 67, 0, 54, 97, 9, 155, 156, 190, 19, 10, 114, 289, 119, 143, 205, 147, 28, 17, 176, 225, 71, 212, 12, 13, 41, 193, 210, 165, 222, 239, 273, 286, 48, 46, 220, 89, 106, 283, 100, 153, 137, 68, 57, 187, 297, 274, 275, 15, 245, 112, 2, 55, 132, 172, 216, 183, 198, 214, 93, 123, 142, 86, 174, 98, 63, 200, 99, 181, 5, 128, 40, 171, 80, 94, 226, 188, 221, 228, 133, 64, 154, 117, 180, 207, 8, 49, 244, 251, 23, 115, 196, 25, 108, 179, 296, 107, 197, 260, 232, 76 in input data\n",
      "training data did not have the following fields: f153, f11, f247, f267, f160, f185, f32, f211, f145, f280, f100, f72, f163, f239, f12, f8, f75, f246, f95, f89, f50, f68, f65, f151, f152, f154, f212, f208, f209, f198, f213, f115, f90, f233, f63, f144, f287, f16, f183, f92, f206, f138, f44, f96, f56, f159, f79, f173, f228, f31, f84, f139, f184, f194, f104, f254, f156, f255, f91, f120, f105, f252, f73, f277, f286, f281, f290, f19, f40, f78, f201, f186, f204, f234, f236, f85, f140, f262, f288, f3, f53, f250, f42, f33, f117, f272, f242, f133, f229, f284, f131, f260, f230, f237, f21, f54, f83, f275, f297, f182, f5, f80, f1, f241, f295, f147, f285, f221, f70, f197, f294, f217, f130, f67, f265, f289, f13, f111, f14, f26, f121, f143, f162, f166, f119, f193, f224, f57, f45, f226, f76, f169, f58, f172, f36, f107, f161, f251, f176, f20, f232, f205, f124, f283, f214, f71, f27, f137, f218, f177, f64, f35, f167, f55, f52, f240, f69, f114, f135, f210, f257, f164, f157, f37, f225, f6, f88, f269, f227, f249, f29, f298, f34, f102, f271, f2, f43, f196, f170, f181, f18, f47, f30, f238, f258, f39, f219, f4, f245, f62, f141, f171, f235, f261, f66, f150, f174, f220, f291, f28, f106, f134, f207, f41, f215, f191, f49, f148, f180, f99, f203, f10, f168, f243, f264, f149, f108, f192, f74, f200, f244, f60, f123, f59, f136, f9, f299, f259, f274, f22, f165, f132, f93, f195, f94, f15, f116, f128, f256, f178, f17, f24, f38, f126, f77, f155, f199, f129, f46, f122, f223, f48, f273, f263, f282, f103, f179, f81, f86, f110, f187, f278, f231, f296, f142, f125, f190, f216, f118, f276, f101, f268, f253, f222, f113, f270, f97, f248, f82, f51, f112, f279, f266, f109, f188, f202, f293, f0, f87, f25, f158, f175, f146, f23, f7, f189, f127, f98, f61, f292.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 30 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.772\tLogisticRegression(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.9000000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.5), LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299']\n",
      "expected 31, 24, 152, 146, 169, 121, 177, 254, 262, 159, 136, 161, 90, 144, 96, 139, 195, 282, 186, 263, 75, 227, 6, 281, 92, 72, 218, 74, 204, 65, 194, 59, 259, 29, 138, 148, 271, 39, 113, 104, 129, 1, 223, 70, 85, 18, 45, 77, 256, 252, 110, 185, 3, 105, 175, 290, 102, 69, 4, 27, 224, 135, 168, 294, 248, 202, 164, 178, 145, 234, 44, 73, 127, 240, 276, 279, 238, 81, 134, 66, 33, 213, 30, 124, 236, 253, 257, 265, 166, 26, 241, 277, 42, 258, 158, 61, 298, 43, 83, 149, 206, 267, 50, 47, 189, 38, 20, 131, 141, 237, 266, 163, 34, 95, 167, 264, 82, 84, 52, 37, 184, 247, 7, 91, 111, 203, 122, 162, 280, 109, 211, 35, 120, 270, 291, 292, 255, 21, 87, 101, 79, 140, 170, 173, 215, 217, 246, 249, 201, 284, 219, 242, 243, 272, 293, 130, 88, 209, 268, 233, 157, 78, 116, 125, 231, 192, 269, 36, 126, 11, 51, 62, 151, 295, 235, 250, 261, 58, 32, 191, 287, 230, 56, 229, 278, 288, 299, 285, 14, 103, 22, 118, 150, 160, 16, 182, 60, 208, 199, 53, 67, 0, 54, 97, 9, 155, 156, 190, 19, 10, 114, 289, 119, 143, 205, 147, 28, 17, 176, 225, 71, 212, 12, 13, 41, 193, 210, 165, 222, 239, 273, 286, 48, 46, 220, 89, 106, 283, 100, 153, 137, 68, 57, 187, 297, 274, 275, 15, 245, 112, 2, 55, 132, 172, 216, 183, 198, 214, 93, 123, 142, 86, 174, 98, 63, 200, 99, 181, 5, 128, 40, 171, 80, 94, 226, 188, 221, 228, 133, 64, 154, 117, 180, 207, 8, 49, 244, 251, 23, 115, 196, 25, 108, 179, 296, 107, 197, 260, 232, 76 in input data\n",
      "training data did not have the following fields: f153, f11, f247, f267, f160, f185, f32, f211, f145, f280, f100, f72, f163, f239, f12, f8, f75, f246, f95, f89, f50, f68, f65, f151, f152, f154, f212, f208, f209, f198, f213, f115, f90, f233, f63, f144, f287, f16, f183, f92, f206, f138, f44, f96, f56, f159, f79, f173, f228, f31, f84, f139, f184, f194, f104, f254, f156, f255, f91, f120, f105, f252, f73, f277, f286, f281, f290, f19, f40, f78, f201, f186, f204, f234, f236, f85, f140, f262, f288, f3, f53, f250, f42, f33, f117, f272, f242, f133, f229, f284, f131, f260, f230, f237, f21, f54, f83, f275, f297, f182, f5, f80, f1, f241, f295, f147, f285, f221, f70, f197, f294, f217, f130, f67, f265, f289, f13, f111, f14, f26, f121, f143, f162, f166, f119, f193, f224, f57, f45, f226, f76, f169, f58, f172, f36, f107, f161, f251, f176, f20, f232, f205, f124, f283, f214, f71, f27, f137, f218, f177, f64, f35, f167, f55, f52, f240, f69, f114, f135, f210, f257, f164, f157, f37, f225, f6, f88, f269, f227, f249, f29, f298, f34, f102, f271, f2, f43, f196, f170, f181, f18, f47, f30, f238, f258, f39, f219, f4, f245, f62, f141, f171, f235, f261, f66, f150, f174, f220, f291, f28, f106, f134, f207, f41, f215, f191, f49, f148, f180, f99, f203, f10, f168, f243, f264, f149, f108, f192, f74, f200, f244, f60, f123, f59, f136, f9, f299, f259, f274, f22, f165, f132, f93, f195, f94, f15, f116, f128, f256, f178, f17, f24, f38, f126, f77, f155, f199, f129, f46, f122, f223, f48, f273, f263, f282, f103, f179, f81, f86, f110, f187, f278, f231, f296, f142, f125, f190, f216, f118, f276, f101, f268, f253, f222, f113, f270, f97, f248, f82, f51, f112, f279, f266, f109, f188, f202, f293, f0, f87, f25, f158, f175, f146, f23, f7, f189, f127, f98, f61, f292.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299']\n",
      "expected 31, 24, 152, 146, 169, 121, 177, 254, 262, 159, 136, 161, 90, 144, 96, 139, 195, 282, 186, 263, 75, 227, 6, 281, 92, 72, 218, 74, 204, 65, 194, 59, 259, 29, 138, 148, 271, 39, 113, 104, 129, 1, 223, 70, 85, 18, 45, 77, 256, 252, 110, 185, 3, 105, 175, 290, 102, 69, 4, 27, 224, 135, 168, 294, 248, 202, 164, 178, 145, 234, 44, 73, 127, 240, 276, 279, 238, 81, 134, 66, 33, 213, 30, 124, 236, 253, 257, 265, 166, 26, 241, 277, 42, 258, 158, 61, 298, 43, 83, 149, 206, 267, 50, 47, 189, 38, 20, 131, 141, 237, 266, 163, 34, 95, 167, 264, 82, 84, 52, 37, 184, 247, 7, 91, 111, 203, 122, 162, 280, 109, 211, 35, 120, 270, 291, 292, 255, 21, 87, 101, 79, 140, 170, 173, 215, 217, 246, 249, 201, 284, 219, 242, 243, 272, 293, 130, 88, 209, 268, 233, 157, 78, 116, 125, 231, 192, 269, 36, 126, 11, 51, 62, 151, 295, 235, 250, 261, 58, 32, 191, 287, 230, 56, 229, 278, 288, 299, 285, 14, 103, 22, 118, 150, 160, 16, 182, 60, 208, 199, 53, 67, 0, 54, 97, 9, 155, 156, 190, 19, 10, 114, 289, 119, 143, 205, 147, 28, 17, 176, 225, 71, 212, 12, 13, 41, 193, 210, 165, 222, 239, 273, 286, 48, 46, 220, 89, 106, 283, 100, 153, 137, 68, 57, 187, 297, 274, 275, 15, 245, 112, 2, 55, 132, 172, 216, 183, 198, 214, 93, 123, 142, 86, 174, 98, 63, 200, 99, 181, 5, 128, 40, 171, 80, 94, 226, 188, 221, 228, 133, 64, 154, 117, 180, 207, 8, 49, 244, 251, 23, 115, 196, 25, 108, 179, 296, 107, 197, 260, 232, 76 in input data\n",
      "training data did not have the following fields: f153, f11, f247, f267, f160, f185, f32, f211, f145, f280, f100, f72, f163, f239, f12, f8, f75, f246, f95, f89, f50, f68, f65, f151, f152, f154, f212, f208, f209, f198, f213, f115, f90, f233, f63, f144, f287, f16, f183, f92, f206, f138, f44, f96, f56, f159, f79, f173, f228, f31, f84, f139, f184, f194, f104, f254, f156, f255, f91, f120, f105, f252, f73, f277, f286, f281, f290, f19, f40, f78, f201, f186, f204, f234, f236, f85, f140, f262, f288, f3, f53, f250, f42, f33, f117, f272, f242, f133, f229, f284, f131, f260, f230, f237, f21, f54, f83, f275, f297, f182, f5, f80, f1, f241, f295, f147, f285, f221, f70, f197, f294, f217, f130, f67, f265, f289, f13, f111, f14, f26, f121, f143, f162, f166, f119, f193, f224, f57, f45, f226, f76, f169, f58, f172, f36, f107, f161, f251, f176, f20, f232, f205, f124, f283, f214, f71, f27, f137, f218, f177, f64, f35, f167, f55, f52, f240, f69, f114, f135, f210, f257, f164, f157, f37, f225, f6, f88, f269, f227, f249, f29, f298, f34, f102, f271, f2, f43, f196, f170, f181, f18, f47, f30, f238, f258, f39, f219, f4, f245, f62, f141, f171, f235, f261, f66, f150, f174, f220, f291, f28, f106, f134, f207, f41, f215, f191, f49, f148, f180, f99, f203, f10, f168, f243, f264, f149, f108, f192, f74, f200, f244, f60, f123, f59, f136, f9, f299, f259, f274, f22, f165, f132, f93, f195, f94, f15, f116, f128, f256, f178, f17, f24, f38, f126, f77, f155, f199, f129, f46, f122, f223, f48, f273, f263, f282, f103, f179, f81, f86, f110, f187, f278, f231, f296, f142, f125, f190, f216, f118, f276, f101, f268, f253, f222, f113, f270, f97, f248, f82, f51, f112, f279, f266, f109, f188, f202, f293, f0, f87, f25, f158, f175, f146, f23, f7, f189, f127, f98, f61, f292.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299']\n",
      "expected 31, 24, 152, 146, 169, 121, 177, 254, 262, 159, 136, 161, 90, 144, 96, 139, 195, 282, 186, 263, 75, 227, 6, 281, 92, 72, 218, 74, 204, 65, 194, 59, 259, 29, 138, 148, 271, 39, 113, 104, 129, 1, 223, 70, 85, 18, 45, 77, 256, 252, 110, 185, 3, 105, 175, 290, 102, 69, 4, 27, 224, 135, 168, 294, 248, 202, 164, 178, 145, 234, 44, 73, 127, 240, 276, 279, 238, 81, 134, 66, 33, 213, 30, 124, 236, 253, 257, 265, 166, 26, 241, 277, 42, 258, 158, 61, 298, 43, 83, 149, 206, 267, 50, 47, 189, 38, 20, 131, 141, 237, 266, 163, 34, 95, 167, 264, 82, 84, 52, 37, 184, 247, 7, 91, 111, 203, 122, 162, 280, 109, 211, 35, 120, 270, 291, 292, 255, 21, 87, 101, 79, 140, 170, 173, 215, 217, 246, 249, 201, 284, 219, 242, 243, 272, 293, 130, 88, 209, 268, 233, 157, 78, 116, 125, 231, 192, 269, 36, 126, 11, 51, 62, 151, 295, 235, 250, 261, 58, 32, 191, 287, 230, 56, 229, 278, 288, 299, 285, 14, 103, 22, 118, 150, 160, 16, 182, 60, 208, 199, 53, 67, 0, 54, 97, 9, 155, 156, 190, 19, 10, 114, 289, 119, 143, 205, 147, 28, 17, 176, 225, 71, 212, 12, 13, 41, 193, 210, 165, 222, 239, 273, 286, 48, 46, 220, 89, 106, 283, 100, 153, 137, 68, 57, 187, 297, 274, 275, 15, 245, 112, 2, 55, 132, 172, 216, 183, 198, 214, 93, 123, 142, 86, 174, 98, 63, 200, 99, 181, 5, 128, 40, 171, 80, 94, 226, 188, 221, 228, 133, 64, 154, 117, 180, 207, 8, 49, 244, 251, 23, 115, 196, 25, 108, 179, 296, 107, 197, 260, 232, 76 in input data\n",
      "training data did not have the following fields: f153, f11, f247, f267, f160, f185, f32, f211, f145, f280, f100, f72, f163, f239, f12, f8, f75, f246, f95, f89, f50, f68, f65, f151, f152, f154, f212, f208, f209, f198, f213, f115, f90, f233, f63, f144, f287, f16, f183, f92, f206, f138, f44, f96, f56, f159, f79, f173, f228, f31, f84, f139, f184, f194, f104, f254, f156, f255, f91, f120, f105, f252, f73, f277, f286, f281, f290, f19, f40, f78, f201, f186, f204, f234, f236, f85, f140, f262, f288, f3, f53, f250, f42, f33, f117, f272, f242, f133, f229, f284, f131, f260, f230, f237, f21, f54, f83, f275, f297, f182, f5, f80, f1, f241, f295, f147, f285, f221, f70, f197, f294, f217, f130, f67, f265, f289, f13, f111, f14, f26, f121, f143, f162, f166, f119, f193, f224, f57, f45, f226, f76, f169, f58, f172, f36, f107, f161, f251, f176, f20, f232, f205, f124, f283, f214, f71, f27, f137, f218, f177, f64, f35, f167, f55, f52, f240, f69, f114, f135, f210, f257, f164, f157, f37, f225, f6, f88, f269, f227, f249, f29, f298, f34, f102, f271, f2, f43, f196, f170, f181, f18, f47, f30, f238, f258, f39, f219, f4, f245, f62, f141, f171, f235, f261, f66, f150, f174, f220, f291, f28, f106, f134, f207, f41, f215, f191, f49, f148, f180, f99, f203, f10, f168, f243, f264, f149, f108, f192, f74, f200, f244, f60, f123, f59, f136, f9, f299, f259, f274, f22, f165, f132, f93, f195, f94, f15, f116, f128, f256, f178, f17, f24, f38, f126, f77, f155, f199, f129, f46, f122, f223, f48, f273, f263, f282, f103, f179, f81, f86, f110, f187, f278, f231, f296, f142, f125, f190, f216, f118, f276, f101, f268, f253, f222, f113, f270, f97, f248, f82, f51, f112, f279, f266, f109, f188, f202, f293, f0, f87, f25, f158, f175, f146, f23, f7, f189, f127, f98, f61, f292.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 31 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.772\tLogisticRegression(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.9000000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.5), LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 32 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.772\tLogisticRegression(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.9000000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.5), LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299']\n",
      "expected 31, 24, 152, 146, 169, 121, 177, 254, 262, 159, 136, 161, 90, 144, 96, 139, 195, 282, 186, 263, 75, 227, 6, 281, 92, 72, 218, 74, 204, 65, 194, 59, 259, 29, 138, 148, 271, 39, 113, 104, 129, 1, 223, 70, 85, 18, 45, 77, 256, 252, 110, 185, 3, 105, 175, 290, 102, 69, 4, 27, 224, 135, 168, 294, 248, 202, 164, 178, 145, 234, 44, 73, 127, 240, 276, 279, 238, 81, 134, 66, 33, 213, 30, 124, 236, 253, 257, 265, 166, 26, 241, 277, 42, 258, 158, 61, 298, 43, 83, 149, 206, 267, 50, 47, 189, 38, 20, 131, 141, 237, 266, 163, 34, 95, 167, 264, 82, 84, 52, 37, 184, 247, 7, 91, 111, 203, 122, 162, 280, 109, 211, 35, 120, 270, 291, 292, 255, 21, 87, 101, 79, 140, 170, 173, 215, 217, 246, 249, 201, 284, 219, 242, 243, 272, 293, 130, 88, 209, 268, 233, 157, 78, 116, 125, 231, 192, 269, 36, 126, 11, 51, 62, 151, 295, 235, 250, 261, 58, 32, 191, 287, 230, 56, 229, 278, 288, 299, 285, 14, 103, 22, 118, 150, 160, 16, 182, 60, 208, 199, 53, 67, 0, 54, 97, 9, 155, 156, 190, 19, 10, 114, 289, 119, 143, 205, 147, 28, 17, 176, 225, 71, 212, 12, 13, 41, 193, 210, 165, 222, 239, 273, 286, 48, 46, 220, 89, 106, 283, 100, 153, 137, 68, 57, 187, 297, 274, 275, 15, 245, 112, 2, 55, 132, 172, 216, 183, 198, 214, 93, 123, 142, 86, 174, 98, 63, 200, 99, 181, 5, 128, 40, 171, 80, 94, 226, 188, 221, 228, 133, 64, 154, 117, 180, 207, 8, 49, 244, 251, 23, 115, 196, 25, 108, 179, 296, 107, 197, 260, 232, 76 in input data\n",
      "training data did not have the following fields: f153, f11, f247, f267, f160, f185, f32, f211, f145, f280, f100, f72, f163, f239, f12, f8, f75, f246, f95, f89, f50, f68, f65, f151, f152, f154, f212, f208, f209, f198, f213, f115, f90, f233, f63, f144, f287, f16, f183, f92, f206, f138, f44, f96, f56, f159, f79, f173, f228, f31, f84, f139, f184, f194, f104, f254, f156, f255, f91, f120, f105, f252, f73, f277, f286, f281, f290, f19, f40, f78, f201, f186, f204, f234, f236, f85, f140, f262, f288, f3, f53, f250, f42, f33, f117, f272, f242, f133, f229, f284, f131, f260, f230, f237, f21, f54, f83, f275, f297, f182, f5, f80, f1, f241, f295, f147, f285, f221, f70, f197, f294, f217, f130, f67, f265, f289, f13, f111, f14, f26, f121, f143, f162, f166, f119, f193, f224, f57, f45, f226, f76, f169, f58, f172, f36, f107, f161, f251, f176, f20, f232, f205, f124, f283, f214, f71, f27, f137, f218, f177, f64, f35, f167, f55, f52, f240, f69, f114, f135, f210, f257, f164, f157, f37, f225, f6, f88, f269, f227, f249, f29, f298, f34, f102, f271, f2, f43, f196, f170, f181, f18, f47, f30, f238, f258, f39, f219, f4, f245, f62, f141, f171, f235, f261, f66, f150, f174, f220, f291, f28, f106, f134, f207, f41, f215, f191, f49, f148, f180, f99, f203, f10, f168, f243, f264, f149, f108, f192, f74, f200, f244, f60, f123, f59, f136, f9, f299, f259, f274, f22, f165, f132, f93, f195, f94, f15, f116, f128, f256, f178, f17, f24, f38, f126, f77, f155, f199, f129, f46, f122, f223, f48, f273, f263, f282, f103, f179, f81, f86, f110, f187, f278, f231, f296, f142, f125, f190, f216, f118, f276, f101, f268, f253, f222, f113, f270, f97, f248, f82, f51, f112, f279, f266, f109, f188, f202, f293, f0, f87, f25, f158, f175, f146, f23, f7, f189, f127, f98, f61, f292.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 33 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.772\tLogisticRegression(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.9000000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.5), LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299']\n",
      "expected 31, 24, 152, 146, 169, 121, 177, 254, 262, 159, 136, 161, 90, 144, 96, 139, 195, 282, 186, 263, 75, 227, 6, 281, 92, 72, 218, 74, 204, 65, 194, 59, 259, 29, 138, 148, 271, 39, 113, 104, 129, 1, 223, 70, 85, 18, 45, 77, 256, 252, 110, 185, 3, 105, 175, 290, 102, 69, 4, 27, 224, 135, 168, 294, 248, 202, 164, 178, 145, 234, 44, 73, 127, 240, 276, 279, 238, 81, 134, 66, 33, 213, 30, 124, 236, 253, 257, 265, 166, 26, 241, 277, 42, 258, 158, 61, 298, 43, 83, 149, 206, 267, 50, 47, 189, 38, 20, 131, 141, 237, 266, 163, 34, 95, 167, 264, 82, 84, 52, 37, 184, 247, 7, 91, 111, 203, 122, 162, 280, 109, 211, 35, 120, 270, 291, 292, 255, 21, 87, 101, 79, 140, 170, 173, 215, 217, 246, 249, 201, 284, 219, 242, 243, 272, 293, 130, 88, 209, 268, 233, 157, 78, 116, 125, 231, 192, 269, 36, 126, 11, 51, 62, 151, 295, 235, 250, 261, 58, 32, 191, 287, 230, 56, 229, 278, 288, 299, 285, 14, 103, 22, 118, 150, 160, 16, 182, 60, 208, 199, 53, 67, 0, 54, 97, 9, 155, 156, 190, 19, 10, 114, 289, 119, 143, 205, 147, 28, 17, 176, 225, 71, 212, 12, 13, 41, 193, 210, 165, 222, 239, 273, 286, 48, 46, 220, 89, 106, 283, 100, 153, 137, 68, 57, 187, 297, 274, 275, 15, 245, 112, 2, 55, 132, 172, 216, 183, 198, 214, 93, 123, 142, 86, 174, 98, 63, 200, 99, 181, 5, 128, 40, 171, 80, 94, 226, 188, 221, 228, 133, 64, 154, 117, 180, 207, 8, 49, 244, 251, 23, 115, 196, 25, 108, 179, 296, 107, 197, 260, 232, 76 in input data\n",
      "training data did not have the following fields: f153, f11, f247, f267, f160, f185, f32, f211, f145, f280, f100, f72, f163, f239, f12, f8, f75, f246, f95, f89, f50, f68, f65, f151, f152, f154, f212, f208, f209, f198, f213, f115, f90, f233, f63, f144, f287, f16, f183, f92, f206, f138, f44, f96, f56, f159, f79, f173, f228, f31, f84, f139, f184, f194, f104, f254, f156, f255, f91, f120, f105, f252, f73, f277, f286, f281, f290, f19, f40, f78, f201, f186, f204, f234, f236, f85, f140, f262, f288, f3, f53, f250, f42, f33, f117, f272, f242, f133, f229, f284, f131, f260, f230, f237, f21, f54, f83, f275, f297, f182, f5, f80, f1, f241, f295, f147, f285, f221, f70, f197, f294, f217, f130, f67, f265, f289, f13, f111, f14, f26, f121, f143, f162, f166, f119, f193, f224, f57, f45, f226, f76, f169, f58, f172, f36, f107, f161, f251, f176, f20, f232, f205, f124, f283, f214, f71, f27, f137, f218, f177, f64, f35, f167, f55, f52, f240, f69, f114, f135, f210, f257, f164, f157, f37, f225, f6, f88, f269, f227, f249, f29, f298, f34, f102, f271, f2, f43, f196, f170, f181, f18, f47, f30, f238, f258, f39, f219, f4, f245, f62, f141, f171, f235, f261, f66, f150, f174, f220, f291, f28, f106, f134, f207, f41, f215, f191, f49, f148, f180, f99, f203, f10, f168, f243, f264, f149, f108, f192, f74, f200, f244, f60, f123, f59, f136, f9, f299, f259, f274, f22, f165, f132, f93, f195, f94, f15, f116, f128, f256, f178, f17, f24, f38, f126, f77, f155, f199, f129, f46, f122, f223, f48, f273, f263, f282, f103, f179, f81, f86, f110, f187, f278, f231, f296, f142, f125, f190, f216, f118, f276, f101, f268, f253, f222, f113, f270, f97, f248, f82, f51, f112, f279, f266, f109, f188, f202, f293, f0, f87, f25, f158, f175, f146, f23, f7, f189, f127, f98, f61, f292.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 '(slice(None, None, None), 0)' is an invalid key.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 34 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.772\tLogisticRegression(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.9000000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.5), LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 35 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.772\tLogisticRegression(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.9000000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.5), LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 36 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.772\tLogisticRegression(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.9000000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.5), LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 37 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.772\tLogisticRegression(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.9000000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.5), LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299']\n",
      "expected 31, 24, 152, 146, 169, 121, 177, 254, 262, 159, 136, 161, 90, 144, 96, 139, 195, 282, 186, 263, 75, 227, 6, 281, 92, 72, 218, 74, 204, 65, 194, 59, 259, 29, 138, 148, 271, 39, 113, 104, 129, 1, 223, 70, 85, 18, 45, 77, 256, 252, 110, 185, 3, 105, 175, 290, 102, 69, 4, 27, 224, 135, 168, 294, 248, 202, 164, 178, 145, 234, 44, 73, 127, 240, 276, 279, 238, 81, 134, 66, 33, 213, 30, 124, 236, 253, 257, 265, 166, 26, 241, 277, 42, 258, 158, 61, 298, 43, 83, 149, 206, 267, 50, 47, 189, 38, 20, 131, 141, 237, 266, 163, 34, 95, 167, 264, 82, 84, 52, 37, 184, 247, 7, 91, 111, 203, 122, 162, 280, 109, 211, 35, 120, 270, 291, 292, 255, 21, 87, 101, 79, 140, 170, 173, 215, 217, 246, 249, 201, 284, 219, 242, 243, 272, 293, 130, 88, 209, 268, 233, 157, 78, 116, 125, 231, 192, 269, 36, 126, 11, 51, 62, 151, 295, 235, 250, 261, 58, 32, 191, 287, 230, 56, 229, 278, 288, 299, 285, 14, 103, 22, 118, 150, 160, 16, 182, 60, 208, 199, 53, 67, 0, 54, 97, 9, 155, 156, 190, 19, 10, 114, 289, 119, 143, 205, 147, 28, 17, 176, 225, 71, 212, 12, 13, 41, 193, 210, 165, 222, 239, 273, 286, 48, 46, 220, 89, 106, 283, 100, 153, 137, 68, 57, 187, 297, 274, 275, 15, 245, 112, 2, 55, 132, 172, 216, 183, 198, 214, 93, 123, 142, 86, 174, 98, 63, 200, 99, 181, 5, 128, 40, 171, 80, 94, 226, 188, 221, 228, 133, 64, 154, 117, 180, 207, 8, 49, 244, 251, 23, 115, 196, 25, 108, 179, 296, 107, 197, 260, 232, 76 in input data\n",
      "training data did not have the following fields: f153, f11, f247, f267, f160, f185, f32, f211, f145, f280, f100, f72, f163, f239, f12, f8, f75, f246, f95, f89, f50, f68, f65, f151, f152, f154, f212, f208, f209, f198, f213, f115, f90, f233, f63, f144, f287, f16, f183, f92, f206, f138, f44, f96, f56, f159, f79, f173, f228, f31, f84, f139, f184, f194, f104, f254, f156, f255, f91, f120, f105, f252, f73, f277, f286, f281, f290, f19, f40, f78, f201, f186, f204, f234, f236, f85, f140, f262, f288, f3, f53, f250, f42, f33, f117, f272, f242, f133, f229, f284, f131, f260, f230, f237, f21, f54, f83, f275, f297, f182, f5, f80, f1, f241, f295, f147, f285, f221, f70, f197, f294, f217, f130, f67, f265, f289, f13, f111, f14, f26, f121, f143, f162, f166, f119, f193, f224, f57, f45, f226, f76, f169, f58, f172, f36, f107, f161, f251, f176, f20, f232, f205, f124, f283, f214, f71, f27, f137, f218, f177, f64, f35, f167, f55, f52, f240, f69, f114, f135, f210, f257, f164, f157, f37, f225, f6, f88, f269, f227, f249, f29, f298, f34, f102, f271, f2, f43, f196, f170, f181, f18, f47, f30, f238, f258, f39, f219, f4, f245, f62, f141, f171, f235, f261, f66, f150, f174, f220, f291, f28, f106, f134, f207, f41, f215, f191, f49, f148, f180, f99, f203, f10, f168, f243, f264, f149, f108, f192, f74, f200, f244, f60, f123, f59, f136, f9, f299, f259, f274, f22, f165, f132, f93, f195, f94, f15, f116, f128, f256, f178, f17, f24, f38, f126, f77, f155, f199, f129, f46, f122, f223, f48, f273, f263, f282, f103, f179, f81, f86, f110, f187, f278, f231, f296, f142, f125, f190, f216, f118, f276, f101, f268, f253, f222, f113, f270, f97, f248, f82, f51, f112, f279, f266, f109, f188, f202, f293, f0, f87, f25, f158, f175, f146, f23, f7, f189, f127, f98, f61, f292.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 38 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.772\tLogisticRegression(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.9000000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.5), LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 39 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.772\tLogisticRegression(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.9000000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.5), LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 40 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.7799999999999999\tLogisticRegression(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=16), LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299']\n",
      "expected 31, 24, 152, 146, 169, 121, 177, 254, 262, 159, 136, 161, 90, 144, 96, 139, 195, 282, 186, 263, 75, 227, 6, 281, 92, 72, 218, 74, 204, 65, 194, 59, 259, 29, 138, 148, 271, 39, 113, 104, 129, 1, 223, 70, 85, 18, 45, 77, 256, 252, 110, 185, 3, 105, 175, 290, 102, 69, 4, 27, 224, 135, 168, 294, 248, 202, 164, 178, 145, 234, 44, 73, 127, 240, 276, 279, 238, 81, 134, 66, 33, 213, 30, 124, 236, 253, 257, 265, 166, 26, 241, 277, 42, 258, 158, 61, 298, 43, 83, 149, 206, 267, 50, 47, 189, 38, 20, 131, 141, 237, 266, 163, 34, 95, 167, 264, 82, 84, 52, 37, 184, 247, 7, 91, 111, 203, 122, 162, 280, 109, 211, 35, 120, 270, 291, 292, 255, 21, 87, 101, 79, 140, 170, 173, 215, 217, 246, 249, 201, 284, 219, 242, 243, 272, 293, 130, 88, 209, 268, 233, 157, 78, 116, 125, 231, 192, 269, 36, 126, 11, 51, 62, 151, 295, 235, 250, 261, 58, 32, 191, 287, 230, 56, 229, 278, 288, 299, 285, 14, 103, 22, 118, 150, 160, 16, 182, 60, 208, 199, 53, 67, 0, 54, 97, 9, 155, 156, 190, 19, 10, 114, 289, 119, 143, 205, 147, 28, 17, 176, 225, 71, 212, 12, 13, 41, 193, 210, 165, 222, 239, 273, 286, 48, 46, 220, 89, 106, 283, 100, 153, 137, 68, 57, 187, 297, 274, 275, 15, 245, 112, 2, 55, 132, 172, 216, 183, 198, 214, 93, 123, 142, 86, 174, 98, 63, 200, 99, 181, 5, 128, 40, 171, 80, 94, 226, 188, 221, 228, 133, 64, 154, 117, 180, 207, 8, 49, 244, 251, 23, 115, 196, 25, 108, 179, 296, 107, 197, 260, 232, 76 in input data\n",
      "training data did not have the following fields: f153, f11, f247, f267, f160, f185, f32, f211, f145, f280, f100, f72, f163, f239, f12, f8, f75, f246, f95, f89, f50, f68, f65, f151, f152, f154, f212, f208, f209, f198, f213, f115, f90, f233, f63, f144, f287, f16, f183, f92, f206, f138, f44, f96, f56, f159, f79, f173, f228, f31, f84, f139, f184, f194, f104, f254, f156, f255, f91, f120, f105, f252, f73, f277, f286, f281, f290, f19, f40, f78, f201, f186, f204, f234, f236, f85, f140, f262, f288, f3, f53, f250, f42, f33, f117, f272, f242, f133, f229, f284, f131, f260, f230, f237, f21, f54, f83, f275, f297, f182, f5, f80, f1, f241, f295, f147, f285, f221, f70, f197, f294, f217, f130, f67, f265, f289, f13, f111, f14, f26, f121, f143, f162, f166, f119, f193, f224, f57, f45, f226, f76, f169, f58, f172, f36, f107, f161, f251, f176, f20, f232, f205, f124, f283, f214, f71, f27, f137, f218, f177, f64, f35, f167, f55, f52, f240, f69, f114, f135, f210, f257, f164, f157, f37, f225, f6, f88, f269, f227, f249, f29, f298, f34, f102, f271, f2, f43, f196, f170, f181, f18, f47, f30, f238, f258, f39, f219, f4, f245, f62, f141, f171, f235, f261, f66, f150, f174, f220, f291, f28, f106, f134, f207, f41, f215, f191, f49, f148, f180, f99, f203, f10, f168, f243, f264, f149, f108, f192, f74, f200, f244, f60, f123, f59, f136, f9, f299, f259, f274, f22, f165, f132, f93, f195, f94, f15, f116, f128, f256, f178, f17, f24, f38, f126, f77, f155, f199, f129, f46, f122, f223, f48, f273, f263, f282, f103, f179, f81, f86, f110, f187, f278, f231, f296, f142, f125, f190, f216, f118, f276, f101, f268, f253, f222, f113, f270, f97, f248, f82, f51, f112, f279, f266, f109, f188, f202, f293, f0, f87, f25, f158, f175, f146, f23, f7, f189, f127, f98, f61, f292.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 41 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.7799999999999999\tLogisticRegression(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=16), LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299']\n",
      "expected 31, 24, 152, 146, 169, 121, 177, 254, 262, 159, 136, 161, 90, 144, 96, 139, 195, 282, 186, 263, 75, 227, 6, 281, 92, 72, 218, 74, 204, 65, 194, 59, 259, 29, 138, 148, 271, 39, 113, 104, 129, 1, 223, 70, 85, 18, 45, 77, 256, 252, 110, 185, 3, 105, 175, 290, 102, 69, 4, 27, 224, 135, 168, 294, 248, 202, 164, 178, 145, 234, 44, 73, 127, 240, 276, 279, 238, 81, 134, 66, 33, 213, 30, 124, 236, 253, 257, 265, 166, 26, 241, 277, 42, 258, 158, 61, 298, 43, 83, 149, 206, 267, 50, 47, 189, 38, 20, 131, 141, 237, 266, 163, 34, 95, 167, 264, 82, 84, 52, 37, 184, 247, 7, 91, 111, 203, 122, 162, 280, 109, 211, 35, 120, 270, 291, 292, 255, 21, 87, 101, 79, 140, 170, 173, 215, 217, 246, 249, 201, 284, 219, 242, 243, 272, 293, 130, 88, 209, 268, 233, 157, 78, 116, 125, 231, 192, 269, 36, 126, 11, 51, 62, 151, 295, 235, 250, 261, 58, 32, 191, 287, 230, 56, 229, 278, 288, 299, 285, 14, 103, 22, 118, 150, 160, 16, 182, 60, 208, 199, 53, 67, 0, 54, 97, 9, 155, 156, 190, 19, 10, 114, 289, 119, 143, 205, 147, 28, 17, 176, 225, 71, 212, 12, 13, 41, 193, 210, 165, 222, 239, 273, 286, 48, 46, 220, 89, 106, 283, 100, 153, 137, 68, 57, 187, 297, 274, 275, 15, 245, 112, 2, 55, 132, 172, 216, 183, 198, 214, 93, 123, 142, 86, 174, 98, 63, 200, 99, 181, 5, 128, 40, 171, 80, 94, 226, 188, 221, 228, 133, 64, 154, 117, 180, 207, 8, 49, 244, 251, 23, 115, 196, 25, 108, 179, 296, 107, 197, 260, 232, 76 in input data\n",
      "training data did not have the following fields: f153, f11, f247, f267, f160, f185, f32, f211, f145, f280, f100, f72, f163, f239, f12, f8, f75, f246, f95, f89, f50, f68, f65, f151, f152, f154, f212, f208, f209, f198, f213, f115, f90, f233, f63, f144, f287, f16, f183, f92, f206, f138, f44, f96, f56, f159, f79, f173, f228, f31, f84, f139, f184, f194, f104, f254, f156, f255, f91, f120, f105, f252, f73, f277, f286, f281, f290, f19, f40, f78, f201, f186, f204, f234, f236, f85, f140, f262, f288, f3, f53, f250, f42, f33, f117, f272, f242, f133, f229, f284, f131, f260, f230, f237, f21, f54, f83, f275, f297, f182, f5, f80, f1, f241, f295, f147, f285, f221, f70, f197, f294, f217, f130, f67, f265, f289, f13, f111, f14, f26, f121, f143, f162, f166, f119, f193, f224, f57, f45, f226, f76, f169, f58, f172, f36, f107, f161, f251, f176, f20, f232, f205, f124, f283, f214, f71, f27, f137, f218, f177, f64, f35, f167, f55, f52, f240, f69, f114, f135, f210, f257, f164, f157, f37, f225, f6, f88, f269, f227, f249, f29, f298, f34, f102, f271, f2, f43, f196, f170, f181, f18, f47, f30, f238, f258, f39, f219, f4, f245, f62, f141, f171, f235, f261, f66, f150, f174, f220, f291, f28, f106, f134, f207, f41, f215, f191, f49, f148, f180, f99, f203, f10, f168, f243, f264, f149, f108, f192, f74, f200, f244, f60, f123, f59, f136, f9, f299, f259, f274, f22, f165, f132, f93, f195, f94, f15, f116, f128, f256, f178, f17, f24, f38, f126, f77, f155, f199, f129, f46, f122, f223, f48, f273, f263, f282, f103, f179, f81, f86, f110, f187, f278, f231, f296, f142, f125, f190, f216, f118, f276, f101, f268, f253, f222, f113, f270, f97, f248, f82, f51, f112, f279, f266, f109, f188, f202, f293, f0, f87, f25, f158, f175, f146, f23, f7, f189, f127, f98, f61, f292.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299']\n",
      "expected 31, 24, 152, 146, 169, 121, 177, 254, 262, 159, 136, 161, 90, 144, 96, 139, 195, 282, 186, 263, 75, 227, 6, 281, 92, 72, 218, 74, 204, 65, 194, 59, 259, 29, 138, 148, 271, 39, 113, 104, 129, 1, 223, 70, 85, 18, 45, 77, 256, 252, 110, 185, 3, 105, 175, 290, 102, 69, 4, 27, 224, 135, 168, 294, 248, 202, 164, 178, 145, 234, 44, 73, 127, 240, 276, 279, 238, 81, 134, 66, 33, 213, 30, 124, 236, 253, 257, 265, 166, 26, 241, 277, 42, 258, 158, 61, 298, 43, 83, 149, 206, 267, 50, 47, 189, 38, 20, 131, 141, 237, 266, 163, 34, 95, 167, 264, 82, 84, 52, 37, 184, 247, 7, 91, 111, 203, 122, 162, 280, 109, 211, 35, 120, 270, 291, 292, 255, 21, 87, 101, 79, 140, 170, 173, 215, 217, 246, 249, 201, 284, 219, 242, 243, 272, 293, 130, 88, 209, 268, 233, 157, 78, 116, 125, 231, 192, 269, 36, 126, 11, 51, 62, 151, 295, 235, 250, 261, 58, 32, 191, 287, 230, 56, 229, 278, 288, 299, 285, 14, 103, 22, 118, 150, 160, 16, 182, 60, 208, 199, 53, 67, 0, 54, 97, 9, 155, 156, 190, 19, 10, 114, 289, 119, 143, 205, 147, 28, 17, 176, 225, 71, 212, 12, 13, 41, 193, 210, 165, 222, 239, 273, 286, 48, 46, 220, 89, 106, 283, 100, 153, 137, 68, 57, 187, 297, 274, 275, 15, 245, 112, 2, 55, 132, 172, 216, 183, 198, 214, 93, 123, 142, 86, 174, 98, 63, 200, 99, 181, 5, 128, 40, 171, 80, 94, 226, 188, 221, 228, 133, 64, 154, 117, 180, 207, 8, 49, 244, 251, 23, 115, 196, 25, 108, 179, 296, 107, 197, 260, 232, 76 in input data\n",
      "training data did not have the following fields: f153, f11, f247, f267, f160, f185, f32, f211, f145, f280, f100, f72, f163, f239, f12, f8, f75, f246, f95, f89, f50, f68, f65, f151, f152, f154, f212, f208, f209, f198, f213, f115, f90, f233, f63, f144, f287, f16, f183, f92, f206, f138, f44, f96, f56, f159, f79, f173, f228, f31, f84, f139, f184, f194, f104, f254, f156, f255, f91, f120, f105, f252, f73, f277, f286, f281, f290, f19, f40, f78, f201, f186, f204, f234, f236, f85, f140, f262, f288, f3, f53, f250, f42, f33, f117, f272, f242, f133, f229, f284, f131, f260, f230, f237, f21, f54, f83, f275, f297, f182, f5, f80, f1, f241, f295, f147, f285, f221, f70, f197, f294, f217, f130, f67, f265, f289, f13, f111, f14, f26, f121, f143, f162, f166, f119, f193, f224, f57, f45, f226, f76, f169, f58, f172, f36, f107, f161, f251, f176, f20, f232, f205, f124, f283, f214, f71, f27, f137, f218, f177, f64, f35, f167, f55, f52, f240, f69, f114, f135, f210, f257, f164, f157, f37, f225, f6, f88, f269, f227, f249, f29, f298, f34, f102, f271, f2, f43, f196, f170, f181, f18, f47, f30, f238, f258, f39, f219, f4, f245, f62, f141, f171, f235, f261, f66, f150, f174, f220, f291, f28, f106, f134, f207, f41, f215, f191, f49, f148, f180, f99, f203, f10, f168, f243, f264, f149, f108, f192, f74, f200, f244, f60, f123, f59, f136, f9, f299, f259, f274, f22, f165, f132, f93, f195, f94, f15, f116, f128, f256, f178, f17, f24, f38, f126, f77, f155, f199, f129, f46, f122, f223, f48, f273, f263, f282, f103, f179, f81, f86, f110, f187, f278, f231, f296, f142, f125, f190, f216, f118, f276, f101, f268, f253, f222, f113, f270, f97, f248, f82, f51, f112, f279, f266, f109, f188, f202, f293, f0, f87, f25, f158, f175, f146, f23, f7, f189, f127, f98, f61, f292.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 42 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.7799999999999999\tLogisticRegression(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=16), LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299']\n",
      "expected 31, 24, 152, 146, 169, 121, 177, 254, 262, 159, 136, 161, 90, 144, 96, 139, 195, 282, 186, 263, 75, 227, 6, 281, 92, 72, 218, 74, 204, 65, 194, 59, 259, 29, 138, 148, 271, 39, 113, 104, 129, 1, 223, 70, 85, 18, 45, 77, 256, 252, 110, 185, 3, 105, 175, 290, 102, 69, 4, 27, 224, 135, 168, 294, 248, 202, 164, 178, 145, 234, 44, 73, 127, 240, 276, 279, 238, 81, 134, 66, 33, 213, 30, 124, 236, 253, 257, 265, 166, 26, 241, 277, 42, 258, 158, 61, 298, 43, 83, 149, 206, 267, 50, 47, 189, 38, 20, 131, 141, 237, 266, 163, 34, 95, 167, 264, 82, 84, 52, 37, 184, 247, 7, 91, 111, 203, 122, 162, 280, 109, 211, 35, 120, 270, 291, 292, 255, 21, 87, 101, 79, 140, 170, 173, 215, 217, 246, 249, 201, 284, 219, 242, 243, 272, 293, 130, 88, 209, 268, 233, 157, 78, 116, 125, 231, 192, 269, 36, 126, 11, 51, 62, 151, 295, 235, 250, 261, 58, 32, 191, 287, 230, 56, 229, 278, 288, 299, 285, 14, 103, 22, 118, 150, 160, 16, 182, 60, 208, 199, 53, 67, 0, 54, 97, 9, 155, 156, 190, 19, 10, 114, 289, 119, 143, 205, 147, 28, 17, 176, 225, 71, 212, 12, 13, 41, 193, 210, 165, 222, 239, 273, 286, 48, 46, 220, 89, 106, 283, 100, 153, 137, 68, 57, 187, 297, 274, 275, 15, 245, 112, 2, 55, 132, 172, 216, 183, 198, 214, 93, 123, 142, 86, 174, 98, 63, 200, 99, 181, 5, 128, 40, 171, 80, 94, 226, 188, 221, 228, 133, 64, 154, 117, 180, 207, 8, 49, 244, 251, 23, 115, 196, 25, 108, 179, 296, 107, 197, 260, 232, 76 in input data\n",
      "training data did not have the following fields: f153, f11, f247, f267, f160, f185, f32, f211, f145, f280, f100, f72, f163, f239, f12, f8, f75, f246, f95, f89, f50, f68, f65, f151, f152, f154, f212, f208, f209, f198, f213, f115, f90, f233, f63, f144, f287, f16, f183, f92, f206, f138, f44, f96, f56, f159, f79, f173, f228, f31, f84, f139, f184, f194, f104, f254, f156, f255, f91, f120, f105, f252, f73, f277, f286, f281, f290, f19, f40, f78, f201, f186, f204, f234, f236, f85, f140, f262, f288, f3, f53, f250, f42, f33, f117, f272, f242, f133, f229, f284, f131, f260, f230, f237, f21, f54, f83, f275, f297, f182, f5, f80, f1, f241, f295, f147, f285, f221, f70, f197, f294, f217, f130, f67, f265, f289, f13, f111, f14, f26, f121, f143, f162, f166, f119, f193, f224, f57, f45, f226, f76, f169, f58, f172, f36, f107, f161, f251, f176, f20, f232, f205, f124, f283, f214, f71, f27, f137, f218, f177, f64, f35, f167, f55, f52, f240, f69, f114, f135, f210, f257, f164, f157, f37, f225, f6, f88, f269, f227, f249, f29, f298, f34, f102, f271, f2, f43, f196, f170, f181, f18, f47, f30, f238, f258, f39, f219, f4, f245, f62, f141, f171, f235, f261, f66, f150, f174, f220, f291, f28, f106, f134, f207, f41, f215, f191, f49, f148, f180, f99, f203, f10, f168, f243, f264, f149, f108, f192, f74, f200, f244, f60, f123, f59, f136, f9, f299, f259, f274, f22, f165, f132, f93, f195, f94, f15, f116, f128, f256, f178, f17, f24, f38, f126, f77, f155, f199, f129, f46, f122, f223, f48, f273, f263, f282, f103, f179, f81, f86, f110, f187, f278, f231, f296, f142, f125, f190, f216, f118, f276, f101, f268, f253, f222, f113, f270, f97, f248, f82, f51, f112, f279, f266, f109, f188, f202, f293, f0, f87, f25, f158, f175, f146, f23, f7, f189, f127, f98, f61, f292.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299']\n",
      "expected 31, 24, 152, 146, 169, 121, 177, 254, 262, 159, 136, 161, 90, 144, 96, 139, 195, 282, 186, 263, 75, 227, 6, 281, 92, 72, 218, 74, 204, 65, 194, 59, 259, 29, 138, 148, 271, 39, 113, 104, 129, 1, 223, 70, 85, 18, 45, 77, 256, 252, 110, 185, 3, 105, 175, 290, 102, 69, 4, 27, 224, 135, 168, 294, 248, 202, 164, 178, 145, 234, 44, 73, 127, 240, 276, 279, 238, 81, 134, 66, 33, 213, 30, 124, 236, 253, 257, 265, 166, 26, 241, 277, 42, 258, 158, 61, 298, 43, 83, 149, 206, 267, 50, 47, 189, 38, 20, 131, 141, 237, 266, 163, 34, 95, 167, 264, 82, 84, 52, 37, 184, 247, 7, 91, 111, 203, 122, 162, 280, 109, 211, 35, 120, 270, 291, 292, 255, 21, 87, 101, 79, 140, 170, 173, 215, 217, 246, 249, 201, 284, 219, 242, 243, 272, 293, 130, 88, 209, 268, 233, 157, 78, 116, 125, 231, 192, 269, 36, 126, 11, 51, 62, 151, 295, 235, 250, 261, 58, 32, 191, 287, 230, 56, 229, 278, 288, 299, 285, 14, 103, 22, 118, 150, 160, 16, 182, 60, 208, 199, 53, 67, 0, 54, 97, 9, 155, 156, 190, 19, 10, 114, 289, 119, 143, 205, 147, 28, 17, 176, 225, 71, 212, 12, 13, 41, 193, 210, 165, 222, 239, 273, 286, 48, 46, 220, 89, 106, 283, 100, 153, 137, 68, 57, 187, 297, 274, 275, 15, 245, 112, 2, 55, 132, 172, 216, 183, 198, 214, 93, 123, 142, 86, 174, 98, 63, 200, 99, 181, 5, 128, 40, 171, 80, 94, 226, 188, 221, 228, 133, 64, 154, 117, 180, 207, 8, 49, 244, 251, 23, 115, 196, 25, 108, 179, 296, 107, 197, 260, 232, 76 in input data\n",
      "training data did not have the following fields: f153, f11, f247, f267, f160, f185, f32, f211, f145, f280, f100, f72, f163, f239, f12, f8, f75, f246, f95, f89, f50, f68, f65, f151, f152, f154, f212, f208, f209, f198, f213, f115, f90, f233, f63, f144, f287, f16, f183, f92, f206, f138, f44, f96, f56, f159, f79, f173, f228, f31, f84, f139, f184, f194, f104, f254, f156, f255, f91, f120, f105, f252, f73, f277, f286, f281, f290, f19, f40, f78, f201, f186, f204, f234, f236, f85, f140, f262, f288, f3, f53, f250, f42, f33, f117, f272, f242, f133, f229, f284, f131, f260, f230, f237, f21, f54, f83, f275, f297, f182, f5, f80, f1, f241, f295, f147, f285, f221, f70, f197, f294, f217, f130, f67, f265, f289, f13, f111, f14, f26, f121, f143, f162, f166, f119, f193, f224, f57, f45, f226, f76, f169, f58, f172, f36, f107, f161, f251, f176, f20, f232, f205, f124, f283, f214, f71, f27, f137, f218, f177, f64, f35, f167, f55, f52, f240, f69, f114, f135, f210, f257, f164, f157, f37, f225, f6, f88, f269, f227, f249, f29, f298, f34, f102, f271, f2, f43, f196, f170, f181, f18, f47, f30, f238, f258, f39, f219, f4, f245, f62, f141, f171, f235, f261, f66, f150, f174, f220, f291, f28, f106, f134, f207, f41, f215, f191, f49, f148, f180, f99, f203, f10, f168, f243, f264, f149, f108, f192, f74, f200, f244, f60, f123, f59, f136, f9, f299, f259, f274, f22, f165, f132, f93, f195, f94, f15, f116, f128, f256, f178, f17, f24, f38, f126, f77, f155, f199, f129, f46, f122, f223, f48, f273, f263, f282, f103, f179, f81, f86, f110, f187, f278, f231, f296, f142, f125, f190, f216, f118, f276, f101, f268, f253, f222, f113, f270, f97, f248, f82, f51, f112, f279, f266, f109, f188, f202, f293, f0, f87, f25, f158, f175, f146, f23, f7, f189, f127, f98, f61, f292.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299']\n",
      "expected 31, 24, 152, 146, 169, 121, 177, 254, 262, 159, 136, 161, 90, 144, 96, 139, 195, 282, 186, 263, 75, 227, 6, 281, 92, 72, 218, 74, 204, 65, 194, 59, 259, 29, 138, 148, 271, 39, 113, 104, 129, 1, 223, 70, 85, 18, 45, 77, 256, 252, 110, 185, 3, 105, 175, 290, 102, 69, 4, 27, 224, 135, 168, 294, 248, 202, 164, 178, 145, 234, 44, 73, 127, 240, 276, 279, 238, 81, 134, 66, 33, 213, 30, 124, 236, 253, 257, 265, 166, 26, 241, 277, 42, 258, 158, 61, 298, 43, 83, 149, 206, 267, 50, 47, 189, 38, 20, 131, 141, 237, 266, 163, 34, 95, 167, 264, 82, 84, 52, 37, 184, 247, 7, 91, 111, 203, 122, 162, 280, 109, 211, 35, 120, 270, 291, 292, 255, 21, 87, 101, 79, 140, 170, 173, 215, 217, 246, 249, 201, 284, 219, 242, 243, 272, 293, 130, 88, 209, 268, 233, 157, 78, 116, 125, 231, 192, 269, 36, 126, 11, 51, 62, 151, 295, 235, 250, 261, 58, 32, 191, 287, 230, 56, 229, 278, 288, 299, 285, 14, 103, 22, 118, 150, 160, 16, 182, 60, 208, 199, 53, 67, 0, 54, 97, 9, 155, 156, 190, 19, 10, 114, 289, 119, 143, 205, 147, 28, 17, 176, 225, 71, 212, 12, 13, 41, 193, 210, 165, 222, 239, 273, 286, 48, 46, 220, 89, 106, 283, 100, 153, 137, 68, 57, 187, 297, 274, 275, 15, 245, 112, 2, 55, 132, 172, 216, 183, 198, 214, 93, 123, 142, 86, 174, 98, 63, 200, 99, 181, 5, 128, 40, 171, 80, 94, 226, 188, 221, 228, 133, 64, 154, 117, 180, 207, 8, 49, 244, 251, 23, 115, 196, 25, 108, 179, 296, 107, 197, 260, 232, 76 in input data\n",
      "training data did not have the following fields: f153, f11, f247, f267, f160, f185, f32, f211, f145, f280, f100, f72, f163, f239, f12, f8, f75, f246, f95, f89, f50, f68, f65, f151, f152, f154, f212, f208, f209, f198, f213, f115, f90, f233, f63, f144, f287, f16, f183, f92, f206, f138, f44, f96, f56, f159, f79, f173, f228, f31, f84, f139, f184, f194, f104, f254, f156, f255, f91, f120, f105, f252, f73, f277, f286, f281, f290, f19, f40, f78, f201, f186, f204, f234, f236, f85, f140, f262, f288, f3, f53, f250, f42, f33, f117, f272, f242, f133, f229, f284, f131, f260, f230, f237, f21, f54, f83, f275, f297, f182, f5, f80, f1, f241, f295, f147, f285, f221, f70, f197, f294, f217, f130, f67, f265, f289, f13, f111, f14, f26, f121, f143, f162, f166, f119, f193, f224, f57, f45, f226, f76, f169, f58, f172, f36, f107, f161, f251, f176, f20, f232, f205, f124, f283, f214, f71, f27, f137, f218, f177, f64, f35, f167, f55, f52, f240, f69, f114, f135, f210, f257, f164, f157, f37, f225, f6, f88, f269, f227, f249, f29, f298, f34, f102, f271, f2, f43, f196, f170, f181, f18, f47, f30, f238, f258, f39, f219, f4, f245, f62, f141, f171, f235, f261, f66, f150, f174, f220, f291, f28, f106, f134, f207, f41, f215, f191, f49, f148, f180, f99, f203, f10, f168, f243, f264, f149, f108, f192, f74, f200, f244, f60, f123, f59, f136, f9, f299, f259, f274, f22, f165, f132, f93, f195, f94, f15, f116, f128, f256, f178, f17, f24, f38, f126, f77, f155, f199, f129, f46, f122, f223, f48, f273, f263, f282, f103, f179, f81, f86, f110, f187, f278, f231, f296, f142, f125, f190, f216, f118, f276, f101, f268, f253, f222, f113, f270, f97, f248, f82, f51, f112, f279, f266, f109, f188, f202, f293, f0, f87, f25, f158, f175, f146, f23, f7, f189, f127, f98, f61, f292.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 43 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.7799999999999999\tLogisticRegression(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=16), LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299']\n",
      "expected 31, 24, 152, 146, 169, 121, 177, 254, 262, 159, 136, 161, 90, 144, 96, 139, 195, 282, 186, 263, 75, 227, 6, 281, 92, 72, 218, 74, 204, 65, 194, 59, 259, 29, 138, 148, 271, 39, 113, 104, 129, 1, 223, 70, 85, 18, 45, 77, 256, 252, 110, 185, 3, 105, 175, 290, 102, 69, 4, 27, 224, 135, 168, 294, 248, 202, 164, 178, 145, 234, 44, 73, 127, 240, 276, 279, 238, 81, 134, 66, 33, 213, 30, 124, 236, 253, 257, 265, 166, 26, 241, 277, 42, 258, 158, 61, 298, 43, 83, 149, 206, 267, 50, 47, 189, 38, 20, 131, 141, 237, 266, 163, 34, 95, 167, 264, 82, 84, 52, 37, 184, 247, 7, 91, 111, 203, 122, 162, 280, 109, 211, 35, 120, 270, 291, 292, 255, 21, 87, 101, 79, 140, 170, 173, 215, 217, 246, 249, 201, 284, 219, 242, 243, 272, 293, 130, 88, 209, 268, 233, 157, 78, 116, 125, 231, 192, 269, 36, 126, 11, 51, 62, 151, 295, 235, 250, 261, 58, 32, 191, 287, 230, 56, 229, 278, 288, 299, 285, 14, 103, 22, 118, 150, 160, 16, 182, 60, 208, 199, 53, 67, 0, 54, 97, 9, 155, 156, 190, 19, 10, 114, 289, 119, 143, 205, 147, 28, 17, 176, 225, 71, 212, 12, 13, 41, 193, 210, 165, 222, 239, 273, 286, 48, 46, 220, 89, 106, 283, 100, 153, 137, 68, 57, 187, 297, 274, 275, 15, 245, 112, 2, 55, 132, 172, 216, 183, 198, 214, 93, 123, 142, 86, 174, 98, 63, 200, 99, 181, 5, 128, 40, 171, 80, 94, 226, 188, 221, 228, 133, 64, 154, 117, 180, 207, 8, 49, 244, 251, 23, 115, 196, 25, 108, 179, 296, 107, 197, 260, 232, 76 in input data\n",
      "training data did not have the following fields: f153, f11, f247, f267, f160, f185, f32, f211, f145, f280, f100, f72, f163, f239, f12, f8, f75, f246, f95, f89, f50, f68, f65, f151, f152, f154, f212, f208, f209, f198, f213, f115, f90, f233, f63, f144, f287, f16, f183, f92, f206, f138, f44, f96, f56, f159, f79, f173, f228, f31, f84, f139, f184, f194, f104, f254, f156, f255, f91, f120, f105, f252, f73, f277, f286, f281, f290, f19, f40, f78, f201, f186, f204, f234, f236, f85, f140, f262, f288, f3, f53, f250, f42, f33, f117, f272, f242, f133, f229, f284, f131, f260, f230, f237, f21, f54, f83, f275, f297, f182, f5, f80, f1, f241, f295, f147, f285, f221, f70, f197, f294, f217, f130, f67, f265, f289, f13, f111, f14, f26, f121, f143, f162, f166, f119, f193, f224, f57, f45, f226, f76, f169, f58, f172, f36, f107, f161, f251, f176, f20, f232, f205, f124, f283, f214, f71, f27, f137, f218, f177, f64, f35, f167, f55, f52, f240, f69, f114, f135, f210, f257, f164, f157, f37, f225, f6, f88, f269, f227, f249, f29, f298, f34, f102, f271, f2, f43, f196, f170, f181, f18, f47, f30, f238, f258, f39, f219, f4, f245, f62, f141, f171, f235, f261, f66, f150, f174, f220, f291, f28, f106, f134, f207, f41, f215, f191, f49, f148, f180, f99, f203, f10, f168, f243, f264, f149, f108, f192, f74, f200, f244, f60, f123, f59, f136, f9, f299, f259, f274, f22, f165, f132, f93, f195, f94, f15, f116, f128, f256, f178, f17, f24, f38, f126, f77, f155, f199, f129, f46, f122, f223, f48, f273, f263, f282, f103, f179, f81, f86, f110, f187, f278, f231, f296, f142, f125, f190, f216, f118, f276, f101, f268, f253, f222, f113, f270, f97, f248, f82, f51, f112, f279, f266, f109, f188, f202, f293, f0, f87, f25, f158, f175, f146, f23, f7, f189, f127, f98, f61, f292.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 44 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.7799999999999999\tLogisticRegression(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=16), LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 45 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.7799999999999999\tLogisticRegression(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=16), LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299']\n",
      "expected 31, 24, 152, 146, 169, 121, 177, 254, 262, 159, 136, 161, 90, 144, 96, 139, 195, 282, 186, 263, 75, 227, 6, 281, 92, 72, 218, 74, 204, 65, 194, 59, 259, 29, 138, 148, 271, 39, 113, 104, 129, 1, 223, 70, 85, 18, 45, 77, 256, 252, 110, 185, 3, 105, 175, 290, 102, 69, 4, 27, 224, 135, 168, 294, 248, 202, 164, 178, 145, 234, 44, 73, 127, 240, 276, 279, 238, 81, 134, 66, 33, 213, 30, 124, 236, 253, 257, 265, 166, 26, 241, 277, 42, 258, 158, 61, 298, 43, 83, 149, 206, 267, 50, 47, 189, 38, 20, 131, 141, 237, 266, 163, 34, 95, 167, 264, 82, 84, 52, 37, 184, 247, 7, 91, 111, 203, 122, 162, 280, 109, 211, 35, 120, 270, 291, 292, 255, 21, 87, 101, 79, 140, 170, 173, 215, 217, 246, 249, 201, 284, 219, 242, 243, 272, 293, 130, 88, 209, 268, 233, 157, 78, 116, 125, 231, 192, 269, 36, 126, 11, 51, 62, 151, 295, 235, 250, 261, 58, 32, 191, 287, 230, 56, 229, 278, 288, 299, 285, 14, 103, 22, 118, 150, 160, 16, 182, 60, 208, 199, 53, 67, 0, 54, 97, 9, 155, 156, 190, 19, 10, 114, 289, 119, 143, 205, 147, 28, 17, 176, 225, 71, 212, 12, 13, 41, 193, 210, 165, 222, 239, 273, 286, 48, 46, 220, 89, 106, 283, 100, 153, 137, 68, 57, 187, 297, 274, 275, 15, 245, 112, 2, 55, 132, 172, 216, 183, 198, 214, 93, 123, 142, 86, 174, 98, 63, 200, 99, 181, 5, 128, 40, 171, 80, 94, 226, 188, 221, 228, 133, 64, 154, 117, 180, 207, 8, 49, 244, 251, 23, 115, 196, 25, 108, 179, 296, 107, 197, 260, 232, 76 in input data\n",
      "training data did not have the following fields: f153, f11, f247, f267, f160, f185, f32, f211, f145, f280, f100, f72, f163, f239, f12, f8, f75, f246, f95, f89, f50, f68, f65, f151, f152, f154, f212, f208, f209, f198, f213, f115, f90, f233, f63, f144, f287, f16, f183, f92, f206, f138, f44, f96, f56, f159, f79, f173, f228, f31, f84, f139, f184, f194, f104, f254, f156, f255, f91, f120, f105, f252, f73, f277, f286, f281, f290, f19, f40, f78, f201, f186, f204, f234, f236, f85, f140, f262, f288, f3, f53, f250, f42, f33, f117, f272, f242, f133, f229, f284, f131, f260, f230, f237, f21, f54, f83, f275, f297, f182, f5, f80, f1, f241, f295, f147, f285, f221, f70, f197, f294, f217, f130, f67, f265, f289, f13, f111, f14, f26, f121, f143, f162, f166, f119, f193, f224, f57, f45, f226, f76, f169, f58, f172, f36, f107, f161, f251, f176, f20, f232, f205, f124, f283, f214, f71, f27, f137, f218, f177, f64, f35, f167, f55, f52, f240, f69, f114, f135, f210, f257, f164, f157, f37, f225, f6, f88, f269, f227, f249, f29, f298, f34, f102, f271, f2, f43, f196, f170, f181, f18, f47, f30, f238, f258, f39, f219, f4, f245, f62, f141, f171, f235, f261, f66, f150, f174, f220, f291, f28, f106, f134, f207, f41, f215, f191, f49, f148, f180, f99, f203, f10, f168, f243, f264, f149, f108, f192, f74, f200, f244, f60, f123, f59, f136, f9, f299, f259, f274, f22, f165, f132, f93, f195, f94, f15, f116, f128, f256, f178, f17, f24, f38, f126, f77, f155, f199, f129, f46, f122, f223, f48, f273, f263, f282, f103, f179, f81, f86, f110, f187, f278, f231, f296, f142, f125, f190, f216, f118, f276, f101, f268, f253, f222, f113, f270, f97, f248, f82, f51, f112, f279, f266, f109, f188, f202, f293, f0, f87, f25, f158, f175, f146, f23, f7, f189, f127, f98, f61, f292.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 66.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=5 '(slice(None, None, None), 0)' is an invalid key.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 46 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.7799999999999999\tLogisticRegression(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=16), LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299']\n",
      "expected 31, 24, 152, 146, 169, 121, 177, 254, 262, 159, 136, 161, 90, 144, 96, 139, 195, 282, 186, 263, 75, 227, 6, 281, 92, 72, 218, 74, 204, 65, 194, 59, 259, 29, 138, 148, 271, 39, 113, 104, 129, 1, 223, 70, 85, 18, 45, 77, 256, 252, 110, 185, 3, 105, 175, 290, 102, 69, 4, 27, 224, 135, 168, 294, 248, 202, 164, 178, 145, 234, 44, 73, 127, 240, 276, 279, 238, 81, 134, 66, 33, 213, 30, 124, 236, 253, 257, 265, 166, 26, 241, 277, 42, 258, 158, 61, 298, 43, 83, 149, 206, 267, 50, 47, 189, 38, 20, 131, 141, 237, 266, 163, 34, 95, 167, 264, 82, 84, 52, 37, 184, 247, 7, 91, 111, 203, 122, 162, 280, 109, 211, 35, 120, 270, 291, 292, 255, 21, 87, 101, 79, 140, 170, 173, 215, 217, 246, 249, 201, 284, 219, 242, 243, 272, 293, 130, 88, 209, 268, 233, 157, 78, 116, 125, 231, 192, 269, 36, 126, 11, 51, 62, 151, 295, 235, 250, 261, 58, 32, 191, 287, 230, 56, 229, 278, 288, 299, 285, 14, 103, 22, 118, 150, 160, 16, 182, 60, 208, 199, 53, 67, 0, 54, 97, 9, 155, 156, 190, 19, 10, 114, 289, 119, 143, 205, 147, 28, 17, 176, 225, 71, 212, 12, 13, 41, 193, 210, 165, 222, 239, 273, 286, 48, 46, 220, 89, 106, 283, 100, 153, 137, 68, 57, 187, 297, 274, 275, 15, 245, 112, 2, 55, 132, 172, 216, 183, 198, 214, 93, 123, 142, 86, 174, 98, 63, 200, 99, 181, 5, 128, 40, 171, 80, 94, 226, 188, 221, 228, 133, 64, 154, 117, 180, 207, 8, 49, 244, 251, 23, 115, 196, 25, 108, 179, 296, 107, 197, 260, 232, 76 in input data\n",
      "training data did not have the following fields: f153, f11, f247, f267, f160, f185, f32, f211, f145, f280, f100, f72, f163, f239, f12, f8, f75, f246, f95, f89, f50, f68, f65, f151, f152, f154, f212, f208, f209, f198, f213, f115, f90, f233, f63, f144, f287, f16, f183, f92, f206, f138, f44, f96, f56, f159, f79, f173, f228, f31, f84, f139, f184, f194, f104, f254, f156, f255, f91, f120, f105, f252, f73, f277, f286, f281, f290, f19, f40, f78, f201, f186, f204, f234, f236, f85, f140, f262, f288, f3, f53, f250, f42, f33, f117, f272, f242, f133, f229, f284, f131, f260, f230, f237, f21, f54, f83, f275, f297, f182, f5, f80, f1, f241, f295, f147, f285, f221, f70, f197, f294, f217, f130, f67, f265, f289, f13, f111, f14, f26, f121, f143, f162, f166, f119, f193, f224, f57, f45, f226, f76, f169, f58, f172, f36, f107, f161, f251, f176, f20, f232, f205, f124, f283, f214, f71, f27, f137, f218, f177, f64, f35, f167, f55, f52, f240, f69, f114, f135, f210, f257, f164, f157, f37, f225, f6, f88, f269, f227, f249, f29, f298, f34, f102, f271, f2, f43, f196, f170, f181, f18, f47, f30, f238, f258, f39, f219, f4, f245, f62, f141, f171, f235, f261, f66, f150, f174, f220, f291, f28, f106, f134, f207, f41, f215, f191, f49, f148, f180, f99, f203, f10, f168, f243, f264, f149, f108, f192, f74, f200, f244, f60, f123, f59, f136, f9, f299, f259, f274, f22, f165, f132, f93, f195, f94, f15, f116, f128, f256, f178, f17, f24, f38, f126, f77, f155, f199, f129, f46, f122, f223, f48, f273, f263, f282, f103, f179, f81, f86, f110, f187, f278, f231, f296, f142, f125, f190, f216, f118, f276, f101, f268, f253, f222, f113, f270, f97, f248, f82, f51, f112, f279, f266, f109, f188, f202, f293, f0, f87, f25, f158, f175, f146, f23, f7, f189, f127, f98, f61, f292.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 47 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.7799999999999999\tLogisticRegression(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=16), LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299']\n",
      "expected 31, 24, 152, 146, 169, 121, 177, 254, 262, 159, 136, 161, 90, 144, 96, 139, 195, 282, 186, 263, 75, 227, 6, 281, 92, 72, 218, 74, 204, 65, 194, 59, 259, 29, 138, 148, 271, 39, 113, 104, 129, 1, 223, 70, 85, 18, 45, 77, 256, 252, 110, 185, 3, 105, 175, 290, 102, 69, 4, 27, 224, 135, 168, 294, 248, 202, 164, 178, 145, 234, 44, 73, 127, 240, 276, 279, 238, 81, 134, 66, 33, 213, 30, 124, 236, 253, 257, 265, 166, 26, 241, 277, 42, 258, 158, 61, 298, 43, 83, 149, 206, 267, 50, 47, 189, 38, 20, 131, 141, 237, 266, 163, 34, 95, 167, 264, 82, 84, 52, 37, 184, 247, 7, 91, 111, 203, 122, 162, 280, 109, 211, 35, 120, 270, 291, 292, 255, 21, 87, 101, 79, 140, 170, 173, 215, 217, 246, 249, 201, 284, 219, 242, 243, 272, 293, 130, 88, 209, 268, 233, 157, 78, 116, 125, 231, 192, 269, 36, 126, 11, 51, 62, 151, 295, 235, 250, 261, 58, 32, 191, 287, 230, 56, 229, 278, 288, 299, 285, 14, 103, 22, 118, 150, 160, 16, 182, 60, 208, 199, 53, 67, 0, 54, 97, 9, 155, 156, 190, 19, 10, 114, 289, 119, 143, 205, 147, 28, 17, 176, 225, 71, 212, 12, 13, 41, 193, 210, 165, 222, 239, 273, 286, 48, 46, 220, 89, 106, 283, 100, 153, 137, 68, 57, 187, 297, 274, 275, 15, 245, 112, 2, 55, 132, 172, 216, 183, 198, 214, 93, 123, 142, 86, 174, 98, 63, 200, 99, 181, 5, 128, 40, 171, 80, 94, 226, 188, 221, 228, 133, 64, 154, 117, 180, 207, 8, 49, 244, 251, 23, 115, 196, 25, 108, 179, 296, 107, 197, 260, 232, 76 in input data\n",
      "training data did not have the following fields: f153, f11, f247, f267, f160, f185, f32, f211, f145, f280, f100, f72, f163, f239, f12, f8, f75, f246, f95, f89, f50, f68, f65, f151, f152, f154, f212, f208, f209, f198, f213, f115, f90, f233, f63, f144, f287, f16, f183, f92, f206, f138, f44, f96, f56, f159, f79, f173, f228, f31, f84, f139, f184, f194, f104, f254, f156, f255, f91, f120, f105, f252, f73, f277, f286, f281, f290, f19, f40, f78, f201, f186, f204, f234, f236, f85, f140, f262, f288, f3, f53, f250, f42, f33, f117, f272, f242, f133, f229, f284, f131, f260, f230, f237, f21, f54, f83, f275, f297, f182, f5, f80, f1, f241, f295, f147, f285, f221, f70, f197, f294, f217, f130, f67, f265, f289, f13, f111, f14, f26, f121, f143, f162, f166, f119, f193, f224, f57, f45, f226, f76, f169, f58, f172, f36, f107, f161, f251, f176, f20, f232, f205, f124, f283, f214, f71, f27, f137, f218, f177, f64, f35, f167, f55, f52, f240, f69, f114, f135, f210, f257, f164, f157, f37, f225, f6, f88, f269, f227, f249, f29, f298, f34, f102, f271, f2, f43, f196, f170, f181, f18, f47, f30, f238, f258, f39, f219, f4, f245, f62, f141, f171, f235, f261, f66, f150, f174, f220, f291, f28, f106, f134, f207, f41, f215, f191, f49, f148, f180, f99, f203, f10, f168, f243, f264, f149, f108, f192, f74, f200, f244, f60, f123, f59, f136, f9, f299, f259, f274, f22, f165, f132, f93, f195, f94, f15, f116, f128, f256, f178, f17, f24, f38, f126, f77, f155, f199, f129, f46, f122, f223, f48, f273, f263, f282, f103, f179, f81, f86, f110, f187, f278, f231, f296, f142, f125, f190, f216, f118, f276, f101, f268, f253, f222, f113, f270, f97, f248, f82, f51, f112, f279, f266, f109, f188, f202, f293, f0, f87, f25, f158, f175, f146, f23, f7, f189, f127, f98, f61, f292.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 48 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.7799999999999999\tLogisticRegression(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=16), LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299']\n",
      "expected 31, 24, 152, 146, 169, 121, 177, 254, 262, 159, 136, 161, 90, 144, 96, 139, 195, 282, 186, 263, 75, 227, 6, 281, 92, 72, 218, 74, 204, 65, 194, 59, 259, 29, 138, 148, 271, 39, 113, 104, 129, 1, 223, 70, 85, 18, 45, 77, 256, 252, 110, 185, 3, 105, 175, 290, 102, 69, 4, 27, 224, 135, 168, 294, 248, 202, 164, 178, 145, 234, 44, 73, 127, 240, 276, 279, 238, 81, 134, 66, 33, 213, 30, 124, 236, 253, 257, 265, 166, 26, 241, 277, 42, 258, 158, 61, 298, 43, 83, 149, 206, 267, 50, 47, 189, 38, 20, 131, 141, 237, 266, 163, 34, 95, 167, 264, 82, 84, 52, 37, 184, 247, 7, 91, 111, 203, 122, 162, 280, 109, 211, 35, 120, 270, 291, 292, 255, 21, 87, 101, 79, 140, 170, 173, 215, 217, 246, 249, 201, 284, 219, 242, 243, 272, 293, 130, 88, 209, 268, 233, 157, 78, 116, 125, 231, 192, 269, 36, 126, 11, 51, 62, 151, 295, 235, 250, 261, 58, 32, 191, 287, 230, 56, 229, 278, 288, 299, 285, 14, 103, 22, 118, 150, 160, 16, 182, 60, 208, 199, 53, 67, 0, 54, 97, 9, 155, 156, 190, 19, 10, 114, 289, 119, 143, 205, 147, 28, 17, 176, 225, 71, 212, 12, 13, 41, 193, 210, 165, 222, 239, 273, 286, 48, 46, 220, 89, 106, 283, 100, 153, 137, 68, 57, 187, 297, 274, 275, 15, 245, 112, 2, 55, 132, 172, 216, 183, 198, 214, 93, 123, 142, 86, 174, 98, 63, 200, 99, 181, 5, 128, 40, 171, 80, 94, 226, 188, 221, 228, 133, 64, 154, 117, 180, 207, 8, 49, 244, 251, 23, 115, 196, 25, 108, 179, 296, 107, 197, 260, 232, 76 in input data\n",
      "training data did not have the following fields: f153, f11, f247, f267, f160, f185, f32, f211, f145, f280, f100, f72, f163, f239, f12, f8, f75, f246, f95, f89, f50, f68, f65, f151, f152, f154, f212, f208, f209, f198, f213, f115, f90, f233, f63, f144, f287, f16, f183, f92, f206, f138, f44, f96, f56, f159, f79, f173, f228, f31, f84, f139, f184, f194, f104, f254, f156, f255, f91, f120, f105, f252, f73, f277, f286, f281, f290, f19, f40, f78, f201, f186, f204, f234, f236, f85, f140, f262, f288, f3, f53, f250, f42, f33, f117, f272, f242, f133, f229, f284, f131, f260, f230, f237, f21, f54, f83, f275, f297, f182, f5, f80, f1, f241, f295, f147, f285, f221, f70, f197, f294, f217, f130, f67, f265, f289, f13, f111, f14, f26, f121, f143, f162, f166, f119, f193, f224, f57, f45, f226, f76, f169, f58, f172, f36, f107, f161, f251, f176, f20, f232, f205, f124, f283, f214, f71, f27, f137, f218, f177, f64, f35, f167, f55, f52, f240, f69, f114, f135, f210, f257, f164, f157, f37, f225, f6, f88, f269, f227, f249, f29, f298, f34, f102, f271, f2, f43, f196, f170, f181, f18, f47, f30, f238, f258, f39, f219, f4, f245, f62, f141, f171, f235, f261, f66, f150, f174, f220, f291, f28, f106, f134, f207, f41, f215, f191, f49, f148, f180, f99, f203, f10, f168, f243, f264, f149, f108, f192, f74, f200, f244, f60, f123, f59, f136, f9, f299, f259, f274, f22, f165, f132, f93, f195, f94, f15, f116, f128, f256, f178, f17, f24, f38, f126, f77, f155, f199, f129, f46, f122, f223, f48, f273, f263, f282, f103, f179, f81, f86, f110, f187, f278, f231, f296, f142, f125, f190, f216, f118, f276, f101, f268, f253, f222, f113, f270, f97, f248, f82, f51, f112, f279, f266, f109, f188, f202, f293, f0, f87, f25, f158, f175, f146, f23, f7, f189, f127, f98, f61, f292.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 49 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.7799999999999999\tLogisticRegression(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=16), LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 50 - Current Pareto front scores:\n",
      "-1\t0.76\tLogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "-2\t0.7799999999999999\tLogisticRegression(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=16), LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "\r"
     ]
    }
   ],
   "source": [
    "start_time = timer(None)\n",
    "tp = TPOTClassifier(generations=50, population_size=25,\n",
    "                                    random_state=42, verbosity=3)\n",
    "\n",
    "tp.fit(X_train, y_train)\n",
    "time = timer(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "tp.export('tpot_pipeline_dont_overfit.py')\n",
    "preds = tp.predict(X_test)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on TPOTClassifier in module tpot.tpot object:\n",
      "\n",
      "class TPOTClassifier(tpot.base.TPOTBase)\n",
      " |  TPOTClassifier(generations=100, population_size=100, offspring_size=None, mutation_rate=0.9, crossover_rate=0.1, scoring=None, cv=5, subsample=1.0, n_jobs=1, max_time_mins=None, max_eval_time_mins=5, random_state=None, config_dict=None, template=None, warm_start=False, memory=None, use_dask=False, periodic_checkpoint_folder=None, early_stop=None, verbosity=0, disable_update_check=False)\n",
      " |  \n",
      " |  TPOT estimator for classification problems.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TPOTClassifier\n",
      " |      tpot.base.TPOTBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  classification = True\n",
      " |  \n",
      " |  default_config_dict = {'sklearn.cluster.FeatureAgglomeration': {'affin...\n",
      " |  \n",
      " |  regression = False\n",
      " |  \n",
      " |  scoring_function = 'accuracy'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tpot.base.TPOTBase:\n",
      " |  \n",
      " |  __init__(self, generations=100, population_size=100, offspring_size=None, mutation_rate=0.9, crossover_rate=0.1, scoring=None, cv=5, subsample=1.0, n_jobs=1, max_time_mins=None, max_eval_time_mins=5, random_state=None, config_dict=None, template=None, warm_start=False, memory=None, use_dask=False, periodic_checkpoint_folder=None, early_stop=None, verbosity=0, disable_update_check=False)\n",
      " |      Set up the genetic programming algorithm for pipeline optimization.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      generations: int, optional (default: 100)\n",
      " |          Number of iterations to the run pipeline optimization process.\n",
      " |          Generally, TPOT will work better when you give it more generations (and\n",
      " |          therefore time) to optimize the pipeline. TPOT will evaluate\n",
      " |          POPULATION_SIZE + GENERATIONS x OFFSPRING_SIZE pipelines in total.\n",
      " |      population_size: int, optional (default: 100)\n",
      " |          Number of individuals to retain in the GP population every generation.\n",
      " |          Generally, TPOT will work better when you give it more individuals\n",
      " |          (and therefore time) to optimize the pipeline. TPOT will evaluate\n",
      " |          POPULATION_SIZE + GENERATIONS x OFFSPRING_SIZE pipelines in total.\n",
      " |      offspring_size: int, optional (default: None)\n",
      " |          Number of offspring to produce in each GP generation.\n",
      " |          By default, offspring_size = population_size.\n",
      " |      mutation_rate: float, optional (default: 0.9)\n",
      " |          Mutation rate for the genetic programming algorithm in the range [0.0, 1.0].\n",
      " |          This parameter tells the GP algorithm how many pipelines to apply random\n",
      " |          changes to every generation. We recommend using the default parameter unless\n",
      " |          you understand how the mutation rate affects GP algorithms.\n",
      " |      crossover_rate: float, optional (default: 0.1)\n",
      " |          Crossover rate for the genetic programming algorithm in the range [0.0, 1.0].\n",
      " |          This parameter tells the genetic programming algorithm how many pipelines to\n",
      " |          \"breed\" every generation. We recommend using the default parameter unless you\n",
      " |          understand how the mutation rate affects GP algorithms.\n",
      " |      scoring: string or callable, optional\n",
      " |          Function used to evaluate the quality of a given pipeline for the\n",
      " |          problem. By default, accuracy is used for classification problems and\n",
      " |          mean squared error (MSE) for regression problems.\n",
      " |      \n",
      " |          Offers the same options as sklearn.model_selection.cross_val_score as well as\n",
      " |          a built-in score 'balanced_accuracy'. Classification metrics:\n",
      " |      \n",
      " |          ['accuracy', 'adjusted_rand_score', 'average_precision', 'balanced_accuracy',\n",
      " |          'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted',\n",
      " |          'precision', 'precision_macro', 'precision_micro', 'precision_samples',\n",
      " |          'precision_weighted', 'recall', 'recall_macro', 'recall_micro',\n",
      " |          'recall_samples', 'recall_weighted', 'roc_auc']\n",
      " |      \n",
      " |          Regression metrics:\n",
      " |      \n",
      " |          ['neg_median_absolute_error', 'neg_mean_absolute_error',\n",
      " |          'neg_mean_squared_error', 'r2']\n",
      " |      \n",
      " |          If you would like to use a custom scoring function, you can pass a callable\n",
      " |          function to this parameter with the signature scorer(y_true, y_pred).\n",
      " |          See the section on scoring functions in the documentation for more details.\n",
      " |      \n",
      " |          TPOT assumes that any custom scoring function with \"error\" or \"loss\" in the\n",
      " |          name is meant to be minimized, whereas any other functions will be maximized.\n",
      " |      cv: int or cross-validation generator, optional (default: 5)\n",
      " |          If CV is a number, then it is the number of folds to evaluate each\n",
      " |          pipeline over in k-fold cross-validation during the TPOT optimization\n",
      " |           process. If it is an object then it is an object to be used as a\n",
      " |           cross-validation generator.\n",
      " |      subsample: float, optional (default: 1.0)\n",
      " |          Subsample ratio of the training instance. Setting it to 0.5 means that TPOT\n",
      " |          randomly collects half of training samples for pipeline optimization process.\n",
      " |      n_jobs: int, optional (default: 1)\n",
      " |          Number of CPUs for evaluating pipelines in parallel during the TPOT\n",
      " |          optimization process. Assigning this to -1 will use as many cores as available\n",
      " |          on the computer. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used.\n",
      " |          Thus for n_jobs = -2, all CPUs but one are used.\n",
      " |      max_time_mins: int, optional (default: None)\n",
      " |          How many minutes TPOT has to optimize the pipeline.\n",
      " |          If provided, this setting will override the \"generations\" parameter and allow\n",
      " |          TPOT to run until it runs out of time.\n",
      " |      max_eval_time_mins: float, optional (default: 5)\n",
      " |          How many minutes TPOT has to optimize a single pipeline.\n",
      " |          Setting this parameter to higher values will allow TPOT to explore more\n",
      " |          complex pipelines, but will also allow TPOT to run longer.\n",
      " |      random_state: int, optional (default: None)\n",
      " |          Random number generator seed for TPOT. Use this parameter to make sure\n",
      " |          that TPOT will give you the same results each time you run it against the\n",
      " |          same data set with that seed.\n",
      " |      config_dict: a Python dictionary or string, optional (default: None)\n",
      " |          Python dictionary:\n",
      " |              A dictionary customizing the operators and parameters that\n",
      " |              TPOT uses in the optimization process.\n",
      " |              For examples, see config_regressor.py and config_classifier.py\n",
      " |          Path for configuration file:\n",
      " |              A path to a configuration file for customizing the operators and parameters that\n",
      " |              TPOT uses in the optimization process.\n",
      " |              For examples, see config_regressor.py and config_classifier.py\n",
      " |          String 'TPOT light':\n",
      " |              TPOT uses a light version of operator configuration dictionary instead of\n",
      " |              the default one.\n",
      " |          String 'TPOT MDR':\n",
      " |              TPOT uses a list of TPOT-MDR operator configuration dictionary instead of\n",
      " |              the default one.\n",
      " |          String 'TPOT sparse':\n",
      " |              TPOT uses a configuration dictionary with a one-hot-encoder and the\n",
      " |              operators normally included in TPOT that also support sparse matrices.\n",
      " |      template: string (default: None)\n",
      " |          Template of predefined pipeline structure. The option is for specifying a desired structure\n",
      " |          for the machine learning pipeline evaluated in TPOT. So far this option only supports\n",
      " |          linear pipeline structure. Each step in the pipeline should be a main class of operators\n",
      " |          (Selector, Transformer, Classifier or Regressor) or a specific operator\n",
      " |          (e.g. SelectPercentile) defined in TPOT operator configuration. If one step is a main class,\n",
      " |          TPOT will randomly assign all subclass operators (subclasses of SelectorMixin,\n",
      " |          TransformerMixin, ClassifierMixin or RegressorMixin in scikit-learn) to that step.\n",
      " |          Steps in the template are delimited by \"-\", e.g. \"SelectPercentile-Transformer-Classifier\".\n",
      " |          By default value of template is None, TPOT generates tree-based pipeline randomly.\n",
      " |      warm_start: bool, optional (default: False)\n",
      " |          Flag indicating whether the TPOT instance will reuse the population from\n",
      " |          previous calls to fit().\n",
      " |      memory: a Memory object or string, optional (default: None)\n",
      " |          If supplied, pipeline will cache each transformer after calling fit. This feature\n",
      " |          is used to avoid computing the fit transformers within a pipeline if the parameters\n",
      " |          and input data are identical with another fitted pipeline during optimization process.\n",
      " |          String 'auto':\n",
      " |              TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n",
      " |          String path of a caching directory\n",
      " |              TPOT uses memory caching with the provided directory and TPOT does NOT clean\n",
      " |              the caching directory up upon shutdown. If the directory does not exist, TPOT will\n",
      " |              create it.\n",
      " |          Memory object:\n",
      " |              TPOT uses the instance of joblib.Memory for memory caching,\n",
      " |              and TPOT does NOT clean the caching directory up upon shutdown.\n",
      " |          None:\n",
      " |              TPOT does not use memory caching.\n",
      " |      use_dask: boolean, default False\n",
      " |          Whether to use Dask-ML's pipeline optimiziations. This avoid re-fitting\n",
      " |          the same estimator on the same split of data multiple times. It\n",
      " |          will also provide more detailed diagnostics when using Dask's\n",
      " |          distributed scheduler.\n",
      " |      \n",
      " |          See `avoid repeated work <https://dask-ml.readthedocs.io/en/latest/hyper-parameter-search.html#avoid-repeated-work>`__\n",
      " |          for more details.\n",
      " |      periodic_checkpoint_folder: path string, optional (default: None)\n",
      " |          If supplied, a folder in which tpot will periodically save pipelines in pareto front so far while optimizing.\n",
      " |          Currently once per generation but not more often than once per 30 seconds.\n",
      " |          Useful in multiple cases:\n",
      " |              Sudden death before tpot could save optimized pipeline\n",
      " |              Track its progress\n",
      " |              Grab pipelines while it's still optimizing\n",
      " |      early_stop: int or None (default: None)\n",
      " |          How many generations TPOT checks whether there is no improvement in optimization process.\n",
      " |          End optimization process if there is no improvement in the set number of generations.\n",
      " |      verbosity: int, optional (default: 0)\n",
      " |          How much information TPOT communicates while it's running.\n",
      " |          0 = none, 1 = minimal, 2 = high, 3 = all.\n",
      " |          A setting of 2 or higher will add a progress bar during the optimization procedure.\n",
      " |      disable_update_check: bool, optional (default: False)\n",
      " |          Flag indicating whether the TPOT version checker should be disabled.\n",
      " |      \n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      None\n",
      " |  \n",
      " |  clean_pipeline_string(self, individual)\n",
      " |      Provide a string of the individual without the parameter prefixes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      individual: individual\n",
      " |          Individual which should be represented by a pretty string\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      A string like str(individual), but with parameter prefixes removed.\n",
      " |  \n",
      " |  export(self, output_file_name, data_file_path='')\n",
      " |      Export the optimized pipeline as Python code.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      output_file_name: string\n",
      " |          String containing the path and file name of the desired output file\n",
      " |      data_file_path: string (default: '')\n",
      " |          By default, the path of input dataset is 'PATH/TO/DATA/FILE' by default.\n",
      " |          If data_file_path is another string, the path will be replaced.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      False if it skipped writing the pipeline to file\n",
      " |      True if the pipeline was actually written\n",
      " |  \n",
      " |  fit(self, features, target, sample_weight=None, groups=None)\n",
      " |      Fit an optimized machine learning pipeline.\n",
      " |      \n",
      " |      Uses genetic programming to optimize a machine learning pipeline that\n",
      " |      maximizes score on the provided features and target. Performs internal\n",
      " |      k-fold cross-validaton to avoid overfitting on the provided data. The\n",
      " |      best pipeline is then trained on the entire set of provided samples.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      features: array-like {n_samples, n_features}\n",
      " |          Feature matrix\n",
      " |      \n",
      " |          TPOT and all scikit-learn algorithms assume that the features will be numerical\n",
      " |          and there will be no missing values. As such, when a feature matrix is provided\n",
      " |          to TPOT, all missing values will automatically be replaced (i.e., imputed) using\n",
      " |          median value imputation.\n",
      " |      \n",
      " |          If you wish to use a different imputation strategy than median imputation, please\n",
      " |          make sure to apply imputation to your feature set prior to passing it to TPOT.\n",
      " |      target: array-like {n_samples}\n",
      " |          List of class labels for prediction\n",
      " |      sample_weight: array-like {n_samples}, optional\n",
      " |          Per-sample weights. Higher weights indicate more importance. If specified,\n",
      " |          sample_weight will be passed to any pipeline element whose fit() function accepts\n",
      " |          a sample_weight argument. By default, using sample_weight does not affect tpot's\n",
      " |          scoring functions, which determine preferences between pipelines.\n",
      " |      groups: array-like, with shape {n_samples, }, optional\n",
      " |          Group labels for the samples used when performing cross-validation.\n",
      " |          This parameter should only be used in conjunction with sklearn's Group cross-validation\n",
      " |          functions, such as sklearn.model_selection.GroupKFold\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self: object\n",
      " |          Returns a copy of the fitted TPOT object\n",
      " |  \n",
      " |  fit_predict(self, features, target, sample_weight=None, groups=None)\n",
      " |      Call fit and predict in sequence.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      features: array-like {n_samples, n_features}\n",
      " |          Feature matrix\n",
      " |      target: array-like {n_samples}\n",
      " |          List of class labels for prediction\n",
      " |      sample_weight: array-like {n_samples}, optional\n",
      " |          Per-sample weights. Higher weights force TPOT to put more emphasis on those points\n",
      " |      groups: array-like, with shape {n_samples, }, optional\n",
      " |          Group labels for the samples used when performing cross-validation.\n",
      " |          This parameter should only be used in conjunction with sklearn's Group cross-validation\n",
      " |          functions, such as sklearn.model_selection.GroupKFold\n",
      " |      \n",
      " |      Returns\n",
      " |      ----------\n",
      " |      array-like: {n_samples}\n",
      " |          Predicted target for the provided features\n",
      " |  \n",
      " |  predict(self, features)\n",
      " |      Use the optimized pipeline to predict the target for a feature set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      features: array-like {n_samples, n_features}\n",
      " |          Feature matrix\n",
      " |      \n",
      " |      Returns\n",
      " |      ----------\n",
      " |      array-like: {n_samples}\n",
      " |          Predicted target for the samples in the feature matrix\n",
      " |  \n",
      " |  predict_proba(self, features)\n",
      " |      Use the optimized pipeline to estimate the class probabilities for a feature set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      features: array-like {n_samples, n_features}\n",
      " |          Feature matrix of the testing set\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array-like: {n_samples, n_target}\n",
      " |          The class probabilities of the input samples\n",
      " |  \n",
      " |  score(self, testing_features, testing_target)\n",
      " |      Return the score on the given testing data using the user-specified scoring function.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      testing_features: array-like {n_samples, n_features}\n",
      " |          Feature matrix of the testing set\n",
      " |      testing_target: array-like {n_samples}\n",
      " |          List of class labels for prediction in the testing set\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      accuracy_score: float\n",
      " |          The estimated test set accuracy\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = [0.736,\n",
    "0.74,\n",
    "0.76,\n",
    "0.76,\n",
    "0.764,\n",
    "0.764,\n",
    "0.764,\n",
    "0.764,\n",
    "0.764,\n",
    "0.768,\n",
    "0.768,\n",
    "0.768,\n",
    "0.768,\n",
    "0.768,\n",
    "0.768,\n",
    "0.764,\n",
    "0.764,\n",
    "0.764,\n",
    "0.764,\n",
    "0.784,\n",
    "0.784,\n",
    "0.784,\n",
    "0.784,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "0.792,\n",
    "]\n",
    "\n",
    "generations = [i for i in range(1,51)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (0.9.0)\r\n",
      "Requirement already satisfied: numpy>=1.9.3 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from seaborn) (1.17.2)\r\n",
      "Requirement already satisfied: matplotlib>=1.4.3 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from seaborn) (3.1.1)\r\n",
      "Requirement already satisfied: pandas>=0.15.2 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from seaborn) (0.25.1)\r\n",
      "Requirement already satisfied: scipy>=0.14.0 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from seaborn) (1.2.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from matplotlib>=1.4.3->seaborn) (1.1.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from matplotlib>=1.4.3->seaborn) (2.8.0)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from matplotlib>=1.4.3->seaborn) (2.4.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from matplotlib>=1.4.3->seaborn) (0.10.0)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from pandas>=0.15.2->seaborn) (2019.2)\r\n",
      "Requirement already satisfied: setuptools in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.3->seaborn) (41.0.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /home/midas/anaconda3/envs/automl-3.7/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib>=1.4.3->seaborn) (1.12.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.lineplot(x=generations, y=train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on TPOTClassifier in module tpot.tpot object:\n",
      "\n",
      "class TPOTClassifier(tpot.base.TPOTBase)\n",
      " |  TPOTClassifier(generations=100, population_size=100, offspring_size=None, mutation_rate=0.9, crossover_rate=0.1, scoring=None, cv=5, subsample=1.0, n_jobs=1, max_time_mins=None, max_eval_time_mins=5, random_state=None, config_dict=None, template=None, warm_start=False, memory=None, use_dask=False, periodic_checkpoint_folder=None, early_stop=None, verbosity=0, disable_update_check=False)\n",
      " |  \n",
      " |  TPOT estimator for classification problems.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TPOTClassifier\n",
      " |      tpot.base.TPOTBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  classification = True\n",
      " |  \n",
      " |  default_config_dict = {'sklearn.cluster.FeatureAgglomeration': {'affin...\n",
      " |  \n",
      " |  regression = False\n",
      " |  \n",
      " |  scoring_function = 'accuracy'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tpot.base.TPOTBase:\n",
      " |  \n",
      " |  __init__(self, generations=100, population_size=100, offspring_size=None, mutation_rate=0.9, crossover_rate=0.1, scoring=None, cv=5, subsample=1.0, n_jobs=1, max_time_mins=None, max_eval_time_mins=5, random_state=None, config_dict=None, template=None, warm_start=False, memory=None, use_dask=False, periodic_checkpoint_folder=None, early_stop=None, verbosity=0, disable_update_check=False)\n",
      " |      Set up the genetic programming algorithm for pipeline optimization.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      generations: int, optional (default: 100)\n",
      " |          Number of iterations to the run pipeline optimization process.\n",
      " |          Generally, TPOT will work better when you give it more generations (and\n",
      " |          therefore time) to optimize the pipeline. TPOT will evaluate\n",
      " |          POPULATION_SIZE + GENERATIONS x OFFSPRING_SIZE pipelines in total.\n",
      " |      population_size: int, optional (default: 100)\n",
      " |          Number of individuals to retain in the GP population every generation.\n",
      " |          Generally, TPOT will work better when you give it more individuals\n",
      " |          (and therefore time) to optimize the pipeline. TPOT will evaluate\n",
      " |          POPULATION_SIZE + GENERATIONS x OFFSPRING_SIZE pipelines in total.\n",
      " |      offspring_size: int, optional (default: None)\n",
      " |          Number of offspring to produce in each GP generation.\n",
      " |          By default, offspring_size = population_size.\n",
      " |      mutation_rate: float, optional (default: 0.9)\n",
      " |          Mutation rate for the genetic programming algorithm in the range [0.0, 1.0].\n",
      " |          This parameter tells the GP algorithm how many pipelines to apply random\n",
      " |          changes to every generation. We recommend using the default parameter unless\n",
      " |          you understand how the mutation rate affects GP algorithms.\n",
      " |      crossover_rate: float, optional (default: 0.1)\n",
      " |          Crossover rate for the genetic programming algorithm in the range [0.0, 1.0].\n",
      " |          This parameter tells the genetic programming algorithm how many pipelines to\n",
      " |          \"breed\" every generation. We recommend using the default parameter unless you\n",
      " |          understand how the mutation rate affects GP algorithms.\n",
      " |      scoring: string or callable, optional\n",
      " |          Function used to evaluate the quality of a given pipeline for the\n",
      " |          problem. By default, accuracy is used for classification problems and\n",
      " |          mean squared error (MSE) for regression problems.\n",
      " |      \n",
      " |          Offers the same options as sklearn.model_selection.cross_val_score as well as\n",
      " |          a built-in score 'balanced_accuracy'. Classification metrics:\n",
      " |      \n",
      " |          ['accuracy', 'adjusted_rand_score', 'average_precision', 'balanced_accuracy',\n",
      " |          'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted',\n",
      " |          'precision', 'precision_macro', 'precision_micro', 'precision_samples',\n",
      " |          'precision_weighted', 'recall', 'recall_macro', 'recall_micro',\n",
      " |          'recall_samples', 'recall_weighted', 'roc_auc']\n",
      " |      \n",
      " |          Regression metrics:\n",
      " |      \n",
      " |          ['neg_median_absolute_error', 'neg_mean_absolute_error',\n",
      " |          'neg_mean_squared_error', 'r2']\n",
      " |      \n",
      " |          If you would like to use a custom scoring function, you can pass a callable\n",
      " |          function to this parameter with the signature scorer(y_true, y_pred).\n",
      " |          See the section on scoring functions in the documentation for more details.\n",
      " |      \n",
      " |          TPOT assumes that any custom scoring function with \"error\" or \"loss\" in the\n",
      " |          name is meant to be minimized, whereas any other functions will be maximized.\n",
      " |      cv: int or cross-validation generator, optional (default: 5)\n",
      " |          If CV is a number, then it is the number of folds to evaluate each\n",
      " |          pipeline over in k-fold cross-validation during the TPOT optimization\n",
      " |           process. If it is an object then it is an object to be used as a\n",
      " |           cross-validation generator.\n",
      " |      subsample: float, optional (default: 1.0)\n",
      " |          Subsample ratio of the training instance. Setting it to 0.5 means that TPOT\n",
      " |          randomly collects half of training samples for pipeline optimization process.\n",
      " |      n_jobs: int, optional (default: 1)\n",
      " |          Number of CPUs for evaluating pipelines in parallel during the TPOT\n",
      " |          optimization process. Assigning this to -1 will use as many cores as available\n",
      " |          on the computer. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used.\n",
      " |          Thus for n_jobs = -2, all CPUs but one are used.\n",
      " |      max_time_mins: int, optional (default: None)\n",
      " |          How many minutes TPOT has to optimize the pipeline.\n",
      " |          If provided, this setting will override the \"generations\" parameter and allow\n",
      " |          TPOT to run until it runs out of time.\n",
      " |      max_eval_time_mins: float, optional (default: 5)\n",
      " |          How many minutes TPOT has to optimize a single pipeline.\n",
      " |          Setting this parameter to higher values will allow TPOT to explore more\n",
      " |          complex pipelines, but will also allow TPOT to run longer.\n",
      " |      random_state: int, optional (default: None)\n",
      " |          Random number generator seed for TPOT. Use this parameter to make sure\n",
      " |          that TPOT will give you the same results each time you run it against the\n",
      " |          same data set with that seed.\n",
      " |      config_dict: a Python dictionary or string, optional (default: None)\n",
      " |          Python dictionary:\n",
      " |              A dictionary customizing the operators and parameters that\n",
      " |              TPOT uses in the optimization process.\n",
      " |              For examples, see config_regressor.py and config_classifier.py\n",
      " |          Path for configuration file:\n",
      " |              A path to a configuration file for customizing the operators and parameters that\n",
      " |              TPOT uses in the optimization process.\n",
      " |              For examples, see config_regressor.py and config_classifier.py\n",
      " |          String 'TPOT light':\n",
      " |              TPOT uses a light version of operator configuration dictionary instead of\n",
      " |              the default one.\n",
      " |          String 'TPOT MDR':\n",
      " |              TPOT uses a list of TPOT-MDR operator configuration dictionary instead of\n",
      " |              the default one.\n",
      " |          String 'TPOT sparse':\n",
      " |              TPOT uses a configuration dictionary with a one-hot-encoder and the\n",
      " |              operators normally included in TPOT that also support sparse matrices.\n",
      " |      template: string (default: None)\n",
      " |          Template of predefined pipeline structure. The option is for specifying a desired structure\n",
      " |          for the machine learning pipeline evaluated in TPOT. So far this option only supports\n",
      " |          linear pipeline structure. Each step in the pipeline should be a main class of operators\n",
      " |          (Selector, Transformer, Classifier or Regressor) or a specific operator\n",
      " |          (e.g. SelectPercentile) defined in TPOT operator configuration. If one step is a main class,\n",
      " |          TPOT will randomly assign all subclass operators (subclasses of SelectorMixin,\n",
      " |          TransformerMixin, ClassifierMixin or RegressorMixin in scikit-learn) to that step.\n",
      " |          Steps in the template are delimited by \"-\", e.g. \"SelectPercentile-Transformer-Classifier\".\n",
      " |          By default value of template is None, TPOT generates tree-based pipeline randomly.\n",
      " |      warm_start: bool, optional (default: False)\n",
      " |          Flag indicating whether the TPOT instance will reuse the population from\n",
      " |          previous calls to fit().\n",
      " |      memory: a Memory object or string, optional (default: None)\n",
      " |          If supplied, pipeline will cache each transformer after calling fit. This feature\n",
      " |          is used to avoid computing the fit transformers within a pipeline if the parameters\n",
      " |          and input data are identical with another fitted pipeline during optimization process.\n",
      " |          String 'auto':\n",
      " |              TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n",
      " |          String path of a caching directory\n",
      " |              TPOT uses memory caching with the provided directory and TPOT does NOT clean\n",
      " |              the caching directory up upon shutdown. If the directory does not exist, TPOT will\n",
      " |              create it.\n",
      " |          Memory object:\n",
      " |              TPOT uses the instance of joblib.Memory for memory caching,\n",
      " |              and TPOT does NOT clean the caching directory up upon shutdown.\n",
      " |          None:\n",
      " |              TPOT does not use memory caching.\n",
      " |      use_dask: boolean, default False\n",
      " |          Whether to use Dask-ML's pipeline optimiziations. This avoid re-fitting\n",
      " |          the same estimator on the same split of data multiple times. It\n",
      " |          will also provide more detailed diagnostics when using Dask's\n",
      " |          distributed scheduler.\n",
      " |      \n",
      " |          See `avoid repeated work <https://dask-ml.readthedocs.io/en/latest/hyper-parameter-search.html#avoid-repeated-work>`__\n",
      " |          for more details.\n",
      " |      periodic_checkpoint_folder: path string, optional (default: None)\n",
      " |          If supplied, a folder in which tpot will periodically save pipelines in pareto front so far while optimizing.\n",
      " |          Currently once per generation but not more often than once per 30 seconds.\n",
      " |          Useful in multiple cases:\n",
      " |              Sudden death before tpot could save optimized pipeline\n",
      " |              Track its progress\n",
      " |              Grab pipelines while it's still optimizing\n",
      " |      early_stop: int or None (default: None)\n",
      " |          How many generations TPOT checks whether there is no improvement in optimization process.\n",
      " |          End optimization process if there is no improvement in the set number of generations.\n",
      " |      verbosity: int, optional (default: 0)\n",
      " |          How much information TPOT communicates while it's running.\n",
      " |          0 = none, 1 = minimal, 2 = high, 3 = all.\n",
      " |          A setting of 2 or higher will add a progress bar during the optimization procedure.\n",
      " |      disable_update_check: bool, optional (default: False)\n",
      " |          Flag indicating whether the TPOT version checker should be disabled.\n",
      " |      \n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      None\n",
      " |  \n",
      " |  clean_pipeline_string(self, individual)\n",
      " |      Provide a string of the individual without the parameter prefixes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      individual: individual\n",
      " |          Individual which should be represented by a pretty string\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      A string like str(individual), but with parameter prefixes removed.\n",
      " |  \n",
      " |  export(self, output_file_name, data_file_path='')\n",
      " |      Export the optimized pipeline as Python code.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      output_file_name: string\n",
      " |          String containing the path and file name of the desired output file\n",
      " |      data_file_path: string (default: '')\n",
      " |          By default, the path of input dataset is 'PATH/TO/DATA/FILE' by default.\n",
      " |          If data_file_path is another string, the path will be replaced.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      False if it skipped writing the pipeline to file\n",
      " |      True if the pipeline was actually written\n",
      " |  \n",
      " |  fit(self, features, target, sample_weight=None, groups=None)\n",
      " |      Fit an optimized machine learning pipeline.\n",
      " |      \n",
      " |      Uses genetic programming to optimize a machine learning pipeline that\n",
      " |      maximizes score on the provided features and target. Performs internal\n",
      " |      k-fold cross-validaton to avoid overfitting on the provided data. The\n",
      " |      best pipeline is then trained on the entire set of provided samples.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      features: array-like {n_samples, n_features}\n",
      " |          Feature matrix\n",
      " |      \n",
      " |          TPOT and all scikit-learn algorithms assume that the features will be numerical\n",
      " |          and there will be no missing values. As such, when a feature matrix is provided\n",
      " |          to TPOT, all missing values will automatically be replaced (i.e., imputed) using\n",
      " |          median value imputation.\n",
      " |      \n",
      " |          If you wish to use a different imputation strategy than median imputation, please\n",
      " |          make sure to apply imputation to your feature set prior to passing it to TPOT.\n",
      " |      target: array-like {n_samples}\n",
      " |          List of class labels for prediction\n",
      " |      sample_weight: array-like {n_samples}, optional\n",
      " |          Per-sample weights. Higher weights indicate more importance. If specified,\n",
      " |          sample_weight will be passed to any pipeline element whose fit() function accepts\n",
      " |          a sample_weight argument. By default, using sample_weight does not affect tpot's\n",
      " |          scoring functions, which determine preferences between pipelines.\n",
      " |      groups: array-like, with shape {n_samples, }, optional\n",
      " |          Group labels for the samples used when performing cross-validation.\n",
      " |          This parameter should only be used in conjunction with sklearn's Group cross-validation\n",
      " |          functions, such as sklearn.model_selection.GroupKFold\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self: object\n",
      " |          Returns a copy of the fitted TPOT object\n",
      " |  \n",
      " |  fit_predict(self, features, target, sample_weight=None, groups=None)\n",
      " |      Call fit and predict in sequence.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      features: array-like {n_samples, n_features}\n",
      " |          Feature matrix\n",
      " |      target: array-like {n_samples}\n",
      " |          List of class labels for prediction\n",
      " |      sample_weight: array-like {n_samples}, optional\n",
      " |          Per-sample weights. Higher weights force TPOT to put more emphasis on those points\n",
      " |      groups: array-like, with shape {n_samples, }, optional\n",
      " |          Group labels for the samples used when performing cross-validation.\n",
      " |          This parameter should only be used in conjunction with sklearn's Group cross-validation\n",
      " |          functions, such as sklearn.model_selection.GroupKFold\n",
      " |      \n",
      " |      Returns\n",
      " |      ----------\n",
      " |      array-like: {n_samples}\n",
      " |          Predicted target for the provided features\n",
      " |  \n",
      " |  predict(self, features)\n",
      " |      Use the optimized pipeline to predict the target for a feature set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      features: array-like {n_samples, n_features}\n",
      " |          Feature matrix\n",
      " |      \n",
      " |      Returns\n",
      " |      ----------\n",
      " |      array-like: {n_samples}\n",
      " |          Predicted target for the samples in the feature matrix\n",
      " |  \n",
      " |  predict_proba(self, features)\n",
      " |      Use the optimized pipeline to estimate the class probabilities for a feature set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      features: array-like {n_samples, n_features}\n",
      " |          Feature matrix of the testing set\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array-like: {n_samples, n_target}\n",
      " |          The class probabilities of the input samples\n",
      " |  \n",
      " |  score(self, testing_features, testing_target)\n",
      " |      Return the score on the given testing data using the user-specified scoring function.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      testing_features: array-like {n_samples, n_features}\n",
      " |          Feature matrix of the testing set\n",
      " |      testing_target: array-like {n_samples}\n",
      " |          List of class labels for prediction in the testing set\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      accuracy_score: float\n",
      " |          The estimated test set accuracy\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cce753c0614d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m submission = pd.DataFrame({\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mid_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;34m\"target\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     })\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"id\": id_test,\n",
    "        \"target\": preds\n",
    "    })\n",
    "\n",
    "submission.to_csv(\"../submit_files/dont_overfit_tpot_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
